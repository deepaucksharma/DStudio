#!/usr/bin/env python3
"""
Basic ETL Pipeline - Mumbai Dabbawala System
============================================

Mumbai ‡§ï‡•á ‡§¶‡§¨‡•ç‡§¨‡§æ‡§µ‡§æ‡§≤‡•á ‡§ú‡•à‡§∏‡•á Data ‡§ï‡•ã ‡§è‡§ï ‡§ú‡§ó‡§π ‡§∏‡•á ‡§â‡§†‡§æ‡§ï‡§∞ ‡§¶‡•Ç‡§∏‡§∞‡•Ä ‡§ú‡§ó‡§π ‡§™‡§π‡•Å‡§Ç‡§ö‡§æ‡§®‡§æ‡•§
Extract ‡§ï‡§∞‡•ã, Transform ‡§ï‡§∞‡•ã, Load ‡§ï‡§∞‡•ã - ‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤ Lunch delivery ‡§ï‡•Ä ‡§§‡§∞‡§π‡•§

Real-world use case: E-commerce product catalog sync between systems

Author: DStudio Engineering Team
Episode: 11 - ETL & Data Integration Patterns
"""

import mysql.connector
import psycopg2
import pandas as pd
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional
import json
from dataclasses import dataclass
from contextlib import contextmanager

# ‡§°‡•á‡§ü‡§æ ‡§π‡•á‡§≤‡•ç‡§• ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è metrics
@dataclass
class ETLMetrics:
    """ETL pipeline ‡§ï‡•á performance metrics - ‡§ú‡•à‡§∏‡•á dabbawala ‡§ï‡•Ä delivery report"""
    start_time: datetime
    end_time: Optional[datetime] = None
    records_extracted: int = 0
    records_transformed: int = 0
    records_loaded: int = 0
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []
    
    def duration_minutes(self) -> float:
        """‡§ï‡§ø‡§§‡§®‡•Ä ‡§¶‡•á‡§∞ ‡§≤‡§ó‡•Ä delivery ‡§Æ‡•á‡§Ç - minutes ‡§Æ‡•á‡§Ç"""
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds() / 60
        return 0.0
    
    def success_rate(self) -> float:
        """‡§ï‡§ø‡§§‡§®‡§æ successful delivery ‡§π‡•Å‡§Ü - percentage ‡§Æ‡•á‡§Ç"""
        if self.records_extracted == 0:
            return 0.0
        return (self.records_loaded / self.records_extracted) * 100

class MumbaiDabbawalaETL:
    """
    Mumbai Dabbawala Style ETL Pipeline
    ===================================
    
    ‡§ú‡•à‡§∏‡•á ‡§¶‡§¨‡•ç‡§¨‡§æ‡§µ‡§æ‡§≤‡•á:
    1. ‡§ò‡§∞ ‡§∏‡•á ‡§ñ‡§æ‡§®‡§æ ‡§â‡§†‡§æ‡§§‡•á ‡§π‡•à‡§Ç (Extract)
    2. Station ‡§™‡•á sort ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç (Transform) 
    3. Office ‡§Æ‡•á‡§Ç deliver ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç (Load)
    
    ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§π‡§Æ data ‡§ï‡•ã process ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§
    """
    
    def __init__(self, mysql_config: Dict, postgres_config: Dict):
        self.mysql_config = mysql_config
        self.postgres_config = postgres_config
        self.logger = self._setup_logging()
        self.metrics = None
        
    def _setup_logging(self):
        """Log setup - ‡§π‡§∞ step ‡§ï‡§æ record ‡§∞‡§ñ‡§®‡§æ ‡§ú‡§∞‡•Ç‡§∞‡•Ä ‡§π‡•à"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('etl_pipeline.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger('DabbawalaETL')
    
    @contextmanager
    def mysql_connection(self):
        """MySQL connection manager - ‡§ò‡§∞ ‡§ï‡§æ connection"""
        connection = None
        try:
            connection = mysql.connector.connect(**self.mysql_config)
            self.logger.info("üè† MySQL ‡§∏‡•á connection establish ‡§π‡•Å‡§Ü - ‡§ò‡§∞ connected!")
            yield connection
        except Exception as e:
            self.logger.error(f"‚ùå MySQL connection failed: {str(e)}")
            raise
        finally:
            if connection:
                connection.close()
                self.logger.info("üîê MySQL connection closed")
    
    @contextmanager 
    def postgres_connection(self):
        """PostgreSQL connection manager - office ‡§ï‡§æ connection"""
        connection = None
        try:
            connection = psycopg2.connect(**self.postgres_config)
            self.logger.info("üè¢ PostgreSQL ‡§∏‡•á connection establish ‡§π‡•Å‡§Ü - office connected!")
            yield connection
        except Exception as e:
            self.logger.error(f"‚ùå PostgreSQL connection failed: {str(e)}")
            raise
        finally:
            if connection:
                connection.close()
                self.logger.info("üîê PostgreSQL connection closed")
    
    def extract_flipkart_products(self) -> pd.DataFrame:
        """
        Extract Phase - ‡§ò‡§∞ ‡§∏‡•á ‡§°‡•á‡§ü‡§æ ‡§â‡§†‡§æ‡§®‡§æ
        ==============================
        
        Flipkart ‡§ï‡•á products ‡§ï‡•ã MySQL ‡§∏‡•á ‡§®‡§ø‡§ï‡§æ‡§≤‡§§‡•á ‡§π‡•à‡§Ç‡•§
        ‡§Ø‡§π ‡§µ‡•ã phase ‡§π‡•à ‡§ú‡§¨ dabbawala ‡§ò‡§∞-‡§ò‡§∞ ‡§ú‡§æ‡§ï‡§∞ ‡§ñ‡§æ‡§®‡§æ collect ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
        """
        self.logger.info("üöö ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à EXTRACT phase - data collection")
        
        query = """
        SELECT 
            product_id,
            product_name,
            category_id,
            price,
            stock_quantity,
            seller_id,
            rating,
            reviews_count,
            created_at,
            updated_at
        FROM flipkart_products 
        WHERE updated_at > DATE_SUB(NOW(), INTERVAL 1 DAY)
        AND is_active = 1
        ORDER BY updated_at DESC
        """
        
        try:
            with self.mysql_connection() as conn:
                # Pandas ‡§ï‡§æ use ‡§ï‡§∞‡§ï‡•á efficiently data load ‡§ï‡§∞‡§®‡§æ
                df = pd.read_sql(query, conn)
                
                self.metrics.records_extracted = len(df)
                self.logger.info(f"‚úÖ {len(df)} products extract ‡§π‡•Å‡§è - dabbawala ‡§®‡•á {len(df)} dabba collect ‡§ï‡§ø‡§è!")
                
                if len(df) == 0:
                    self.logger.warning("‚ö†Ô∏è ‡§ï‡•ã‡§à ‡§®‡§Ø‡§æ data ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ - empty dabba!")
                    
                return df
                
        except Exception as e:
            error_msg = f"Extract phase ‡§Æ‡•á‡§Ç error: {str(e)}"
            self.logger.error(f"‚ùå {error_msg}")
            self.metrics.errors.append(error_msg)
            raise

    def transform_product_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Transform Phase - ‡§°‡•á‡§ü‡§æ ‡§ï‡•ã ‡§∏‡§æ‡§´‡§º ‡§ï‡§∞‡§®‡§æ ‡§î‡§∞ format ‡§ï‡§∞‡§®‡§æ  
        ==================================================
        
        ‡§ú‡•à‡§∏‡•á dabbawala railway station ‡§™‡•á ‡§∏‡§≠‡•Ä dabba ‡§ï‡•ã sort ‡§ï‡§∞‡§§‡§æ ‡§π‡•à,
        ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§π‡§Æ data ‡§ï‡•ã clean ‡§î‡§∞ standardize ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§
        """
        self.logger.info("üîÑ ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à TRANSFORM phase - data cleaning ‡§î‡§∞ formatting")
        
        if df.empty:
            self.logger.info("üì≠ ‡§ï‡•ã‡§à data ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à transform ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è")
            return df
        
        original_count = len(df)
        
        try:
            # 1. ‡§°‡•Å‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§ü ‡§π‡§ü‡§æ‡§®‡§æ - same product multiple times ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è
            df = df.drop_duplicates(subset=['product_id'])
            self.logger.info(f"üßπ Duplicates removed: {original_count - len(df)} duplicate entries")
            
            # 2. Price validation - negative ‡§Ø‡§æ zero price valid ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à
            df = df[df['price'] > 0]
            self.logger.info(f"üí∞ Invalid prices filtered out")
            
            # 3. Category enrichment - category name add ‡§ï‡§∞‡§®‡§æ
            category_mapping = {
                1: 'Electronics', 2: 'Clothing', 3: 'Books', 
                4: 'Home & Kitchen', 5: 'Sports', 6: 'Beauty'
            }
            df['category_name'] = df['category_id'].map(category_mapping)
            
            # 4. Price bands - pricing categories ‡§¨‡§®‡§æ‡§®‡§æ (Indian context)
            def get_price_band(price):
                """Price ‡§ï‡•ã Indian market ‡§ï‡•á according categorize ‡§ï‡§∞‡§®‡§æ"""
                if price < 500:
                    return 'Budget'  # Affordable for everyone
                elif price < 2000:
                    return 'Mid-Range'  # Middle class friendly
                elif price < 10000:
                    return 'Premium'  # Upper middle class
                else:
                    return 'Luxury'  # High-end segment
            
            df['price_band'] = df['price'].apply(get_price_band)
            
            # 5. Seller performance score - rating ‡§ï‡•á basis ‡§™‡§∞
            def calculate_seller_score(rating, reviews_count):
                """Seller ‡§ï‡•Ä performance calculate ‡§ï‡§∞‡§®‡§æ"""
                if reviews_count < 10:
                    return 'New Seller'  # Kam reviews
                elif rating >= 4.5:
                    return 'Excellent'  # Top performer
                elif rating >= 4.0:
                    return 'Good'      # Decent seller
                elif rating >= 3.5:
                    return 'Average'   # Okayish
                else:
                    return 'Poor'      # Improvement needed
            
            df['seller_performance'] = df.apply(
                lambda x: calculate_seller_score(x['rating'], x['reviews_count']), 
                axis=1
            )
            
            # 6. Stock status - inventory levels
            def get_stock_status(quantity):
                """Stock level ‡§ï‡•á according status"""
                if quantity == 0:
                    return 'Out of Stock'
                elif quantity < 10:
                    return 'Low Stock'  # Urgent restocking needed
                elif quantity < 50:
                    return 'Medium Stock'
                else:
                    return 'High Stock'  # Well stocked
            
            df['stock_status'] = df['stock_quantity'].apply(get_stock_status)
            
            # 7. Data types ‡§ï‡•ã properly set ‡§ï‡§∞‡§®‡§æ
            df['product_id'] = df['product_id'].astype(str)
            df['price'] = df['price'].round(2)  # Paisa ‡§ï‡•á ‡§≤‡§ø‡§è 2 decimal places
            df['rating'] = df['rating'].round(1)
            
            # 8. Null values handling - missing data ‡§ï‡•ã handle ‡§ï‡§∞‡§®‡§æ
            df['product_name'].fillna('Unknown Product', inplace=True)
            df['category_name'].fillna('Uncategorized', inplace=True)
            
            # 9. Business rules validation
            # Review count ‡§î‡§∞ rating consistent ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è
            df.loc[df['reviews_count'] == 0, 'rating'] = 0
            
            # 10. Timestamp formatting for PostgreSQL
            df['created_at'] = pd.to_datetime(df['created_at'])
            df['updated_at'] = pd.to_datetime(df['updated_at'])
            df['processed_at'] = datetime.now()  # Processing timestamp add ‡§ï‡§∞‡§®‡§æ
            
            self.metrics.records_transformed = len(df)
            transform_success_rate = (len(df) / original_count) * 100
            
            self.logger.info(f"‚úÖ Transform complete! {len(df)}/{original_count} records processed ({transform_success_rate:.1f}% success rate)")
            self.logger.info(f"üéØ Added columns: category_name, price_band, seller_performance, stock_status, processed_at")
            
            return df
            
        except Exception as e:
            error_msg = f"Transform phase ‡§Æ‡•á‡§Ç error: {str(e)}"
            self.logger.error(f"‚ùå {error_msg}")
            self.metrics.errors.append(error_msg)
            raise

    def load_to_postgres(self, df: pd.DataFrame) -> bool:
        """
        Load Phase - Cleaned data ‡§ï‡•ã PostgreSQL ‡§Æ‡•á‡§Ç store ‡§ï‡§∞‡§®‡§æ
        ====================================================
        
        ‡§Ø‡§π final delivery phase ‡§π‡•à - dabbawala office ‡§Æ‡•á‡§Ç dabba ‡§™‡§π‡•Å‡§Ç‡§ö‡§æ‡§§‡§æ ‡§π‡•à‡•§
        """
        self.logger.info("üì¶ ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à LOAD phase - final destination ‡§Æ‡•á‡§Ç data delivery")
        
        if df.empty:
            self.logger.info("üì≠ ‡§ï‡•ã‡§à data ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à load ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è")
            return True
        
        try:
            with self.postgres_connection() as conn:
                cursor = conn.cursor()
                
                # Table create ‡§ï‡§∞‡§®‡§æ ‡§Ö‡§ó‡§∞ exist ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡•Ä
                create_table_query = """
                CREATE TABLE IF NOT EXISTS flipkart_products_processed (
                    product_id VARCHAR(50) PRIMARY KEY,
                    product_name VARCHAR(255) NOT NULL,
                    category_id INTEGER,
                    category_name VARCHAR(100),
                    price DECIMAL(10,2),
                    price_band VARCHAR(20),
                    stock_quantity INTEGER,
                    stock_status VARCHAR(20),
                    seller_id VARCHAR(50),
                    seller_performance VARCHAR(20),
                    rating DECIMAL(3,1),
                    reviews_count INTEGER,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                """
                cursor.execute(create_table_query)
                self.logger.info("üèóÔ∏è Table structure ready - delivery location prepared!")
                
                # Bulk insert ‡§ï‡•á ‡§≤‡§ø‡§è data prepare ‡§ï‡§∞‡§®‡§æ
                records = []
                for _, row in df.iterrows():
                    record = (
                        row['product_id'], row['product_name'], row['category_id'],
                        row['category_name'], row['price'], row['price_band'],
                        row['stock_quantity'], row['stock_status'], row['seller_id'],
                        row['seller_performance'], row['rating'], row['reviews_count'],
                        row['created_at'], row['updated_at'], row['processed_at']
                    )
                    records.append(record)
                
                # Upsert operation - existing records ‡§ï‡•ã update ‡§ï‡§∞‡§®‡§æ, ‡§®‡§à ‡§ï‡•ã insert ‡§ï‡§∞‡§®‡§æ
                upsert_query = """
                INSERT INTO flipkart_products_processed (
                    product_id, product_name, category_id, category_name,
                    price, price_band, stock_quantity, stock_status,
                    seller_id, seller_performance, rating, reviews_count,
                    created_at, updated_at, processed_at
                ) VALUES %s
                ON CONFLICT (product_id) 
                DO UPDATE SET
                    product_name = EXCLUDED.product_name,
                    category_id = EXCLUDED.category_id,
                    category_name = EXCLUDED.category_name,
                    price = EXCLUDED.price,
                    price_band = EXCLUDED.price_band,
                    stock_quantity = EXCLUDED.stock_quantity,
                    stock_status = EXCLUDED.stock_status,
                    seller_performance = EXCLUDED.seller_performance,
                    rating = EXCLUDED.rating,
                    reviews_count = EXCLUDED.reviews_count,
                    updated_at = EXCLUDED.updated_at,
                    processed_at = EXCLUDED.processed_at
                """
                
                # Psycopg2 ‡§ï‡•á execute_values ‡§ï‡§æ use ‡§ï‡§∞‡§ï‡•á bulk insert
                from psycopg2.extras import execute_values
                execute_values(cursor, upsert_query, records, template=None)
                
                # Transaction commit ‡§ï‡§∞‡§®‡§æ
                conn.commit()
                
                self.metrics.records_loaded = len(records)
                self.logger.info(f"‚úÖ {len(records)} records successfully loaded to PostgreSQL")
                self.logger.info(f"üìä Load complete - all dabbas delivered successfully!")
                
                # Data quality verification
                cursor.execute("SELECT COUNT(*) FROM flipkart_products_processed WHERE processed_at::date = CURRENT_DATE")
                today_count = cursor.fetchone()[0]
                self.logger.info(f"üìà Today's total processed records: {today_count}")
                
                return True
                
        except Exception as e:
            error_msg = f"Load phase ‡§Æ‡•á‡§Ç error: {str(e)}"
            self.logger.error(f"‚ùå {error_msg}")
            self.metrics.errors.append(error_msg)
            return False

    def run_etl_pipeline(self) -> Dict:
        """
        Complete ETL Pipeline Execution
        ==============================
        
        Mumbai dabbawala ‡§ï‡•Ä ‡§§‡§∞‡§π complete delivery cycle ‡§ö‡§≤‡§æ‡§®‡§æ‡•§
        """
        self.logger.info("üöÄ Starting Mumbai Dabbawala ETL Pipeline...")
        self.metrics = ETLMetrics(start_time=datetime.now())
        
        pipeline_success = False
        
        try:
            # Step 1: Extract - ‡§ò‡§∞ ‡§∏‡•á data ‡§â‡§†‡§æ‡§®‡§æ
            self.logger.info("üìç Step 1/3: EXTRACT phase starting...")
            extracted_data = self.extract_flipkart_products()
            
            # Step 2: Transform - data ‡§ï‡•ã process ‡§ï‡§∞‡§®‡§æ
            self.logger.info("üìç Step 2/3: TRANSFORM phase starting...")  
            transformed_data = self.transform_product_data(extracted_data)
            
            # Step 3: Load - final destination ‡§Æ‡•á‡§Ç deliver ‡§ï‡§∞‡§®‡§æ
            self.logger.info("üìç Step 3/3: LOAD phase starting...")
            load_success = self.load_to_postgres(transformed_data)
            
            pipeline_success = load_success
            
        except Exception as e:
            self.logger.error(f"üí• Pipeline failed: {str(e)}")
            self.metrics.errors.append(f"Pipeline failure: {str(e)}")
            
        finally:
            # Pipeline completion
            self.metrics.end_time = datetime.now()
            
            # Final report generation
            report = self._generate_pipeline_report(pipeline_success)
            self.logger.info("üìã Pipeline execution completed!")
            
            return report
    
    def _generate_pipeline_report(self, success: bool) -> Dict:
        """Pipeline ‡§ï‡•Ä detailed report generate ‡§ï‡§∞‡§®‡§æ"""
        
        report = {
            "pipeline_name": "Mumbai Dabbawala ETL",
            "execution_date": self.metrics.start_time.strftime("%Y-%m-%d %H:%M:%S"),
            "status": "SUCCESS" if success else "FAILED",
            "duration_minutes": self.metrics.duration_minutes(),
            "performance_metrics": {
                "records_extracted": self.metrics.records_extracted,
                "records_transformed": self.metrics.records_transformed, 
                "records_loaded": self.metrics.records_loaded,
                "success_rate_percent": self.metrics.success_rate(),
                "processing_speed_records_per_minute": (
                    self.metrics.records_loaded / self.metrics.duration_minutes() 
                    if self.metrics.duration_minutes() > 0 else 0
                )
            },
            "errors": self.metrics.errors,
            "recommendations": []
        }
        
        # Performance recommendations
        if self.metrics.success_rate() < 95:
            report["recommendations"].append("‚ö†Ô∏è Success rate is below 95%. Check data quality issues.")
        
        if self.metrics.duration_minutes() > 30:
            report["recommendations"].append("üêå Pipeline is taking more than 30 minutes. Consider optimization.")
        
        if len(self.metrics.errors) == 0:
            report["recommendations"].append("‚úÖ Pipeline ran without errors. Great job!")
        
        return report

def main():
    """
    Main execution function - Production ready example
    ================================================
    
    Real Mumbai dabbawala ETL pipeline ‡§ï‡•ã run ‡§ï‡§∞‡§®‡§æ‡•§
    """
    
    print("üöÄ Mumbai Dabbawala ETL Pipeline Starting...")
    print("=" * 60)
    
    # Database configurations - Production settings
    mysql_config = {
        'host': 'mysql-master.flipkart.com',  # Master DB for reads
        'user': 'etl_user',
        'password': 'secure_password_123',
        'database': 'flipkart_products',
        'port': 3306,
        'charset': 'utf8mb4',
        'autocommit': False,
        'connect_timeout': 30
    }
    
    postgres_config = {
        'host': 'postgres-warehouse.analytics.com',  # Data warehouse
        'user': 'warehouse_etl',
        'password': 'warehouse_secure_pass',
        'database': 'analytics_db',
        'port': 5432,
        'connect_timeout': 30
    }
    
    # ETL pipeline initialization
    etl_pipeline = MumbaiDabbawalaETL(mysql_config, postgres_config)
    
    try:
        # Execute complete pipeline
        result = etl_pipeline.run_etl_pipeline()
        
        # Print final report
        print("\nüìä Pipeline Execution Report:")
        print("=" * 40)
        print(json.dumps(result, indent=2, default=str))
        
        # Success/Failure indication
        if result["status"] == "SUCCESS":
            print(f"\n‚úÖ Pipeline completed successfully!")
            print(f"‚è±Ô∏è Duration: {result['duration_minutes']:.2f} minutes")
            print(f"üì¶ Processed: {result['performance_metrics']['records_loaded']} records")
            print(f"üéØ Success Rate: {result['performance_metrics']['success_rate_percent']:.1f}%")
        else:
            print(f"\n‚ùå Pipeline failed!")
            print(f"üí• Errors: {len(result['errors'])} issues found")
    
    except Exception as e:
        print(f"\nüí• Critical Error: {str(e)}")
        return False
    
    return True

if __name__ == "__main__":
    """
    Script execution entry point
    ===========================
    
    Production environment ‡§Æ‡•á‡§Ç ‡§á‡§∏‡•á cron job ‡§Ø‡§æ Airflow task ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç run ‡§ï‡§∞‡§®‡§æ‡•§
    """
    
    success = main()
    exit(0 if success else 1)


# Performance Tips for Production:
"""
üîß Production Optimization Tips:

1. **Connection Pooling**: 
   - MySQL ‡§î‡§∞ PostgreSQL ‡§ï‡•á ‡§≤‡§ø‡§è connection pooling use ‡§ï‡§∞‡•á‡§Ç
   - SQLAlchemy ‡§Ø‡§æ psycopg2 pool ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç

2. **Batch Processing**:
   - Large datasets ‡§ï‡•á ‡§≤‡§ø‡§è chunks ‡§Æ‡•á‡§Ç process ‡§ï‡§∞‡•á‡§Ç  
   - Memory usage control ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è

3. **Parallel Processing**:
   - Multiple threads/processes ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
   - concurrent.futures ‡§ï‡§æ use ‡§ï‡§∞‡•á‡§Ç

4. **Monitoring**:
   - Prometheus metrics add ‡§ï‡§∞‡•á‡§Ç
   - Grafana dashboards ‡§¨‡§®‡§æ‡§è‡§Ç
   - Error alerting setup ‡§ï‡§∞‡•á‡§Ç

5. **Error Recovery**:
   - Dead letter queues implement ‡§ï‡§∞‡•á‡§Ç
   - Exponential backoff retry ‡§ï‡§∞‡•á‡§Ç
   - Circuit breaker pattern use ‡§ï‡§∞‡•á‡§Ç

6. **Data Quality**:
   - Great Expectations library use ‡§ï‡§∞‡•á‡§Ç
   - Data validation rules implement ‡§ï‡§∞‡•á‡§Ç
   - Schema evolution handling ‡§ï‡§∞‡•á‡§Ç

7. **Cost Optimization**:
   - Read replicas ‡§ï‡§æ use ‡§ï‡§∞‡•á‡§Ç source ‡§ï‡•á ‡§≤‡§ø‡§è
   - Compression enable ‡§ï‡§∞‡•á‡§Ç
   - Archive old data ‡§ï‡•ã cheaper storage ‡§™‡§∞ move ‡§ï‡§∞‡•á‡§Ç

Mumbai ke dabbawale ‡§ï‡•Ä ‡§§‡§∞‡§π - Time pe delivery, Quality guaranteed! ü•òüì¶
"""