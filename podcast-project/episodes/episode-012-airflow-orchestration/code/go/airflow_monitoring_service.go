/*
Airflow Monitoring Service in Go
Episode 12: Airflow Orchestration - Production Monitoring

‡§Ø‡§π Go service Airflow ‡§ï‡•á monitoring ‡§î‡§∞ alerting ‡§ï‡•ã handle ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§
High-performance monitoring ‡§ï‡•á ‡§≤‡§ø‡§è Go ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§π‡•Å‡§è‡•§

Author: Code Developer Agent
Language: Go with Hindi Comments
Context: Production Airflow monitoring for Indian infrastructure
*/

package main

import (
	"bytes"
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"os"
	"strconv"
	"sync"
	"time"

	"github.com/gorilla/mux"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	_ "github.com/lib/pq"
	"github.com/go-redis/redis/v8"
)

// =============================================================================
// Data Structures - Indian Context ‡§ï‡•á ‡§≤‡§ø‡§è structures
// =============================================================================

// AirflowMetrics - Airflow ‡§ï‡•Ä performance metrics
type AirflowMetrics struct {
	Timestamp        time.Time `json:"timestamp"`
	ActiveDAGs       int       `json:"active_dags"`
	RunningTasks     int       `json:"running_tasks"`
	FailedTasks      int       `json:"failed_tasks"`
	QueuedTasks      int       `json:"queued_tasks"`
	SuccessRate      float64   `json:"success_rate"`
	AverageTaskTime  float64   `json:"avg_task_time_seconds"`
	DatabaseHealth   bool      `json:"database_healthy"`
	SchedulerHealth  bool      `json:"scheduler_healthy"`
	WebserverHealth  bool      `json:"webserver_healthy"`
	CeleryWorkers    int       `json:"celery_workers"`
}

// DAGStatus - Individual DAG ‡§ï‡•Ä status
type DAGStatus struct {
	DAGID           string    `json:"dag_id"`
	LastRunStatus   string    `json:"last_run_status"`
	LastRunTime     time.Time `json:"last_run_time"`
	NextRunTime     *time.Time `json:"next_run_time,omitempty"`
	SuccessRate24h  float64   `json:"success_rate_24h"`
	AverageRunTime  float64   `json:"avg_run_time_minutes"`
	IsActive        bool      `json:"is_active"`
	TaskFailures    int       `json:"task_failures_24h"`
}

// IndianBusinessMetrics - Indian business context ‡§ï‡•á specific metrics
type IndianBusinessMetrics struct {
	FestivalSeasonActive bool    `json:"festival_season_active"`
	MonsoonSeason       bool    `json:"monsoon_season"`
	BusinessHours       bool    `json:"business_hours"`
	PaymentGatewayLoad  float64 `json:"payment_gateway_load"`
	EcommerceTraffic    float64 `json:"ecommerce_traffic_multiplier"`
	BankingHours        bool    `json:"banking_hours"`
	RegionalLoadFactor  float64 `json:"regional_load_factor"`
}

// Alert - Alert configuration ‡§î‡§∞ status
type Alert struct {
	ID          string                 `json:"id"`
	Name        string                 `json:"name"`
	Condition   string                 `json:"condition"`
	Threshold   float64                `json:"threshold"`
	Severity    string                 `json:"severity"`
	Channel     []string               `json:"channels"`
	Enabled     bool                   `json:"enabled"`
	LastFired   *time.Time             `json:"last_fired,omitempty"`
	Metadata    map[string]interface{} `json:"metadata"`
}

// =============================================================================
// Service Configuration
// =============================================================================

type Config struct {
	Port              string
	DatabaseURL       string
	RedisURL          string
	AirflowWebURL     string
	SlackWebhookURL   string
	PagerDutyKey      string
	CheckInterval     time.Duration
	MetricsRetention  time.Duration
}

// =============================================================================
// Monitoring Service
// =============================================================================

type AirflowMonitoringService struct {
	config     *Config
	db         *sql.DB
	redis      *redis.Client
	alerts     []Alert
	metrics    *AirflowMetrics
	dagStatus  map[string]*DAGStatus
	mutex      sync.RWMutex
	
	// Prometheus metrics
	activeDAGsGauge       prometheus.Gauge
	runningTasksGauge     prometheus.Gauge
	failedTasksGauge      prometheus.Gauge
	successRateGauge      prometheus.Gauge
	taskDurationHistogram prometheus.Histogram
	alertsTotal           prometheus.Counter
}

// NewAirflowMonitoringService - ‡§®‡§à monitoring service create ‡§ï‡§∞‡§®‡§æ
func NewAirflowMonitoringService(config *Config) (*AirflowMonitoringService, error) {
	// PostgreSQL connection for Airflow metadata
	db, err := sql.Open("postgres", config.DatabaseURL)
	if err != nil {
		return nil, fmt.Errorf("database connection failed: %v", err)
	}

	// Redis connection for caching ‡§î‡§∞ real-time data
	opt, err := redis.ParseURL(config.RedisURL)
	if err != nil {
		return nil, fmt.Errorf("redis connection failed: %v", err)
	}
	rdb := redis.NewClient(opt)

	service := &AirflowMonitoringService{
		config:    config,
		db:        db,
		redis:     rdb,
		dagStatus: make(map[string]*DAGStatus),
		alerts:    loadDefaultAlerts(),
	}

	// Prometheus metrics initialize ‡§ï‡§∞‡§®‡§æ
	service.initPrometheusMetrics()

	return service, nil
}

// initPrometheusMetrics - Prometheus metrics setup
func (s *AirflowMonitoringService) initPrometheusMetrics() {
	// Airflow specific metrics
	s.activeDAGsGauge = prometheus.NewGauge(prometheus.GaugeOpts{
		Name: "airflow_active_dags_total",
		Help: "Number of active DAGs in Airflow",
	})

	s.runningTasksGauge = prometheus.NewGauge(prometheus.GaugeOpts{
		Name: "airflow_running_tasks_total", 
		Help: "Number of currently running tasks",
	})

	s.failedTasksGauge = prometheus.NewGauge(prometheus.GaugeOpts{
		Name: "airflow_failed_tasks_total",
		Help: "Number of failed tasks in last 24 hours",
	})

	s.successRateGauge = prometheus.NewGauge(prometheus.GaugeOpts{
		Name: "airflow_success_rate_percent",
		Help: "Overall task success rate percentage",
	})

	s.taskDurationHistogram = prometheus.NewHistogram(prometheus.HistogramOpts{
		Name: "airflow_task_duration_seconds",
		Help: "Task execution duration in seconds",
		Buckets: prometheus.ExponentialBuckets(1, 2, 10), // 1s to 512s
	})

	s.alertsTotal = prometheus.NewCounter(prometheus.CounterOpts{
		Name: "airflow_alerts_total",
		Help: "Total number of alerts fired",
	})

	// Register metrics with Prometheus
	prometheus.MustRegister(
		s.activeDAGsGauge,
		s.runningTasksGauge, 
		s.failedTasksGauge,
		s.successRateGauge,
		s.taskDurationHistogram,
		s.alertsTotal,
	)
}

// Start - Monitoring service ‡§ï‡•ã start ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) Start() error {
	log.Printf("üöÄ Starting Airflow Monitoring Service on port %s", s.config.Port)
	
	// Background monitoring goroutine
	go s.monitoringLoop()

	// Background alert processing
	go s.alertProcessingLoop()

	// Background cleanup
	go s.cleanupLoop()

	// HTTP server setup
	router := mux.NewRouter()
	s.setupRoutes(router)

	log.Printf("‚úÖ Monitoring service started successfully")
	return http.ListenAndServe(":"+s.config.Port, router)
}

// setupRoutes - HTTP routes setup ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) setupRoutes(router *mux.Router) {
	// Health check endpoint
	router.HandleFunc("/health", s.healthCheckHandler).Methods("GET")
	
	// Metrics endpoint
	router.HandleFunc("/metrics/airflow", s.airflowMetricsHandler).Methods("GET")
	router.HandleFunc("/metrics/dags", s.dagMetricsHandler).Methods("GET")
	router.HandleFunc("/metrics/indian-business", s.indianBusinessMetricsHandler).Methods("GET")
	
	// Alert management
	router.HandleFunc("/alerts", s.listAlertsHandler).Methods("GET")
	router.HandleFunc("/alerts", s.createAlertHandler).Methods("POST")
	router.HandleFunc("/alerts/{id}", s.updateAlertHandler).Methods("PUT")
	router.HandleFunc("/alerts/{id}", s.deleteAlertHandler).Methods("DELETE")
	
	// Dashboard data
	router.HandleFunc("/dashboard/overview", s.dashboardOverviewHandler).Methods("GET")
	router.HandleFunc("/dashboard/dag/{dag_id}", s.dagDetailHandler).Methods("GET")
	
	// Prometheus metrics endpoint
	router.Handle("/prometheus", promhttp.Handler())
	
	// Static files for dashboard
	router.PathPrefix("/").Handler(http.FileServer(http.Dir("./static/")))
}

// =============================================================================
// Monitoring Logic
// =============================================================================

// monitoringLoop - Main monitoring loop
func (s *AirflowMonitoringService) monitoringLoop() {
	ticker := time.NewTicker(s.config.CheckInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// Airflow metrics collect ‡§ï‡§∞‡§®‡§æ
			if err := s.collectAirflowMetrics(); err != nil {
				log.Printf("‚ùå Error collecting Airflow metrics: %v", err)
			}

			// DAG status update ‡§ï‡§∞‡§®‡§æ
			if err := s.updateDAGStatus(); err != nil {
				log.Printf("‚ùå Error updating DAG status: %v", err)
			}

			// Indian business metrics calculate ‡§ï‡§∞‡§®‡§æ
			if err := s.calculateIndianBusinessMetrics(); err != nil {
				log.Printf("‚ùå Error calculating Indian business metrics: %v", err)
			}

			// Prometheus metrics update ‡§ï‡§∞‡§®‡§æ
			s.updatePrometheusMetrics()

			log.Printf("üìä Monitoring cycle completed at %s", time.Now().Format("15:04:05"))
		}
	}
}

// collectAirflowMetrics - Airflow metrics ‡§ï‡•ã collect ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) collectAirflowMetrics() error {
	ctx := context.Background()
	
	metrics := &AirflowMetrics{
		Timestamp: time.Now(),
	}

	// Database ‡§∏‡•á task statistics ‡§®‡§ø‡§ï‡§æ‡§≤‡§®‡§æ
	query := `
	SELECT 
		COUNT(DISTINCT dag_id) as active_dags,
		COUNT(CASE WHEN state = 'running' THEN 1 END) as running_tasks,
		COUNT(CASE WHEN state = 'failed' AND start_date > NOW() - INTERVAL '24 hours' THEN 1 END) as failed_tasks_24h,
		COUNT(CASE WHEN state = 'queued' THEN 1 END) as queued_tasks,
		AVG(EXTRACT(EPOCH FROM (end_date - start_date))) as avg_task_duration
	FROM task_instance 
	WHERE start_date > NOW() - INTERVAL '24 hours'
	`
	
	row := s.db.QueryRow(query)
	var avgDuration sql.NullFloat64
	
	err := row.Scan(
		&metrics.ActiveDAGs,
		&metrics.RunningTasks, 
		&metrics.FailedTasks,
		&metrics.QueuedTasks,
		&avgDuration,
	)
	
	if err != nil {
		return fmt.Errorf("failed to query task metrics: %v", err)
	}

	if avgDuration.Valid {
		metrics.AverageTaskTime = avgDuration.Float64
	}

	// Success rate calculate ‡§ï‡§∞‡§®‡§æ
	totalTasks := metrics.RunningTasks + metrics.FailedTasks + metrics.QueuedTasks
	if totalTasks > 0 {
		successfulTasks := totalTasks - metrics.FailedTasks
		metrics.SuccessRate = float64(successfulTasks) / float64(totalTasks) * 100
	}

	// Component health checks
	metrics.DatabaseHealth = s.checkDatabaseHealth()
	metrics.SchedulerHealth = s.checkSchedulerHealth()
	metrics.WebserverHealth = s.checkWebserverHealth()
	metrics.CeleryWorkers = s.getCeleryWorkerCount()

	// Store metrics in Redis for fast access
	metricsJSON, _ := json.Marshal(metrics)
	s.redis.Set(ctx, "airflow:metrics:latest", metricsJSON, time.Hour).Err()

	// Thread-safe update
	s.mutex.Lock()
	s.metrics = metrics
	s.mutex.Unlock()

	return nil
}

// updateDAGStatus - ‡§∏‡§≠‡•Ä DAGs ‡§ï‡•Ä status update ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) updateDAGStatus() error {
	query := `
	SELECT 
		dag_id,
		state,
		execution_date,
		start_date,
		end_date
	FROM dag_run 
	WHERE execution_date > NOW() - INTERVAL '24 hours'
	ORDER BY dag_id, execution_date DESC
	`
	
	rows, err := s.db.Query(query)
	if err != nil {
		return fmt.Errorf("failed to query DAG runs: %v", err)
	}
	defer rows.Close()

	dagStats := make(map[string]*DAGStatus)
	
	for rows.Next() {
		var dagID, state string
		var execDate, startDate time.Time
		var endDate sql.NullTime
		
		err := rows.Scan(&dagID, &state, &execDate, &startDate, &endDate)
		if err != nil {
			log.Printf("‚ö†Ô∏è Error scanning DAG run: %v", err)
			continue
		}

		// ‡§™‡§π‡§≤‡•Ä entry ‡§π‡§∞ DAG ‡§ï‡•á ‡§≤‡§ø‡§è latest run ‡§π‡•ã‡§ó‡•Ä
		if _, exists := dagStats[dagID]; !exists {
			dagStatus := &DAGStatus{
				DAGID:         dagID,
				LastRunStatus: state,
				LastRunTime:   startDate,
				IsActive:      true,
			}

			// Run time calculate ‡§ï‡§∞‡§®‡§æ ‡§Ö‡§ó‡§∞ completed ‡§π‡•à
			if endDate.Valid {
				duration := endDate.Time.Sub(startDate)
				dagStatus.AverageRunTime = duration.Minutes()
			}

			dagStats[dagID] = dagStatus
		}
	}

	// Calculate success rates ‡§î‡§∞ task failures
	for dagID := range dagStats {
		successRate := s.calculateDAGSuccessRate(dagID)
		taskFailures := s.getDAGTaskFailures(dagID)
		
		dagStats[dagID].SuccessRate24h = successRate
		dagStats[dagID].TaskFailures = taskFailures
	}

	// Thread-safe update
	s.mutex.Lock()
	s.dagStatus = dagStats
	s.mutex.Unlock()

	return nil
}

// calculateIndianBusinessMetrics - Indian business context ‡§ï‡•á metrics
func (s *AirflowMonitoringService) calculateIndianBusinessMetrics() error {
	now := time.Now()
	
	// IST ‡§Æ‡•á‡§Ç convert ‡§ï‡§∞‡§®‡§æ
	ist, _ := time.LoadLocation("Asia/Kolkata")
	nowIST := now.In(ist)
	
	metrics := IndianBusinessMetrics{
		FestivalSeasonActive: s.isFestivalSeason(nowIST),
		MonsoonSeason:       s.isMonsoonSeason(nowIST),
		BusinessHours:       s.isBusinessHours(nowIST),
		BankingHours:        s.isBankingHours(nowIST),
		RegionalLoadFactor:  s.calculateRegionalLoadFactor(nowIST),
	}

	// Payment gateway load check ‡§ï‡§∞‡§®‡§æ
	pgLoad, err := s.getPaymentGatewayLoad()
	if err != nil {
		log.Printf("‚ö†Ô∏è Failed to get payment gateway load: %v", err)
		metrics.PaymentGatewayLoad = -1 // Indicate error
	} else {
		metrics.PaymentGatewayLoad = pgLoad
	}

	// E-commerce traffic multiplier
	metrics.EcommerceTraffic = s.calculateEcommerceMultiplier(nowIST, metrics.FestivalSeasonActive)

	// Store in Redis
	ctx := context.Background()
	metricsJSON, _ := json.Marshal(metrics)
	s.redis.Set(ctx, "airflow:indian_metrics:latest", metricsJSON, time.Hour).Err()

	return nil
}

// =============================================================================
// Indian Business Logic
// =============================================================================

// isFestivalSeason - Festival season check ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) isFestivalSeason(t time.Time) bool {
	month := t.Month()
	// October, November, December
	return month >= 10 && month <= 12
}

// isMonsoonSeason - Monsoon season check ‡§ï‡§∞‡§®‡§æ  
func (s *AirflowMonitoringService) isMonsoonSeason(t time.Time) bool {
	month := t.Month()
	// June to September
	return month >= 6 && month <= 9
}

// isBusinessHours - Indian business hours check ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) isBusinessHours(t time.Time) bool {
	hour := t.Hour()
	// 9 AM to 6 PM IST
	return hour >= 9 && hour <= 18
}

// isBankingHours - Indian banking hours check ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) isBankingHours(t time.Time) bool {
	hour := t.Hour()
	weekday := t.Weekday()
	
	// Monday to Friday, 9 AM to 5 PM
	return weekday >= 1 && weekday <= 5 && hour >= 9 && hour <= 17
}

// calculateRegionalLoadFactor - Regional load factor calculate ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) calculateRegionalLoadFactor(t time.Time) float64 {
	hour := t.Hour()
	
	// Peak hours ‡§Æ‡•á‡§Ç higher load factor
	if hour >= 19 && hour <= 22 { // Evening peak
		return 1.5
	} else if hour >= 12 && hour <= 14 { // Lunch time
		return 1.3
	} else if hour >= 9 && hour <= 11 { // Morning peak
		return 1.2
	}
	
	return 1.0 // Normal load
}

// calculateEcommerceMultiplier - E-commerce traffic multiplier
func (s *AirflowMonitoringService) calculateEcommerceMultiplier(t time.Time, festivalSeason bool) float64 {
	baseMultiplier := 1.0
	
	// Festival season boost
	if festivalSeason {
		baseMultiplier = 2.5
	}
	
	// Weekend boost
	weekday := t.Weekday()
	if weekday == 0 || weekday == 6 { // Sunday ‡§Ø‡§æ Saturday
		baseMultiplier *= 1.3
	}
	
	// Evening hours boost
	hour := t.Hour()
	if hour >= 19 && hour <= 23 {
		baseMultiplier *= 1.2
	}
	
	return baseMultiplier
}

// =============================================================================
// Health Check Functions
// =============================================================================

// checkDatabaseHealth - Database health check
func (s *AirflowMonitoringService) checkDatabaseHealth() bool {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()
	
	err := s.db.PingContext(ctx)
	return err == nil
}

// checkSchedulerHealth - Scheduler health check
func (s *AirflowMonitoringService) checkSchedulerHealth() bool {
	// Airflow scheduler heartbeat check ‡§ï‡§∞‡§®‡§æ
	query := `
	SELECT COUNT(*) 
	FROM job 
	WHERE job_type = 'SchedulerJob' 
	AND latest_heartbeat > NOW() - INTERVAL '1 minute'
	`
	
	var count int
	err := s.db.QueryRow(query).Scan(&count)
	return err == nil && count > 0
}

// checkWebserverHealth - Webserver health check
func (s *AirflowMonitoringService) checkWebserverHealth() bool {
	client := &http.Client{Timeout: 10 * time.Second}
	resp, err := client.Get(s.config.AirflowWebURL + "/health")
	
	if err != nil {
		return false
	}
	defer resp.Body.Close()
	
	return resp.StatusCode == 200
}

// getCeleryWorkerCount - Celery worker count ‡§®‡§ø‡§ï‡§æ‡§≤‡§®‡§æ
func (s *AirflowMonitoringService) getCeleryWorkerCount() int {
	// Redis ‡§∏‡•á active workers count ‡§ï‡§∞‡§®‡§æ
	ctx := context.Background()
	keys := s.redis.Keys(ctx, "celery_worker:*").Val()
	
	activeWorkers := 0
	for _, key := range keys {
		// Worker heartbeat check ‡§ï‡§∞‡§®‡§æ
		lastSeen := s.redis.Get(ctx, key).Val()
		if lastSeen != "" {
			if timestamp, err := strconv.ParseInt(lastSeen, 10, 64); err == nil {
				lastSeenTime := time.Unix(timestamp, 0)
				if time.Since(lastSeenTime) < 2*time.Minute {
					activeWorkers++
				}
			}
		}
	}
	
	return activeWorkers
}

// getPaymentGatewayLoad - Payment gateway load ‡§®‡§ø‡§ï‡§æ‡§≤‡§®‡§æ
func (s *AirflowMonitoringService) getPaymentGatewayLoad() (float64, error) {
	ctx := context.Background()
	
	// Redis ‡§∏‡•á payment gateway metrics ‡§®‡§ø‡§ï‡§æ‡§≤‡§®‡§æ
	pgMetrics := s.redis.Get(ctx, "payment_gateway:load").Val()
	if pgMetrics == "" {
		return 0.0, nil // No load data available
	}
	
	load, err := strconv.ParseFloat(pgMetrics, 64)
	if err != nil {
		return 0.0, err
	}
	
	return load, nil
}

// =============================================================================
// Alert Processing
// =============================================================================

// alertProcessingLoop - Alert processing ‡§ï‡§æ main loop
func (s *AirflowMonitoringService) alertProcessingLoop() {
	ticker := time.NewTicker(30 * time.Second) // ‡§π‡§∞ 30 seconds ‡§Æ‡•á‡§Ç alerts check ‡§ï‡§∞‡§®‡§æ
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			s.processAlerts()
		}
	}
}

// processAlerts - ‡§∏‡§≠‡•Ä alerts ‡§ï‡•ã process ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) processAlerts() {
	s.mutex.RLock()
	metrics := s.metrics
	s.mutex.RUnlock()
	
	if metrics == nil {
		return
	}

	for _, alert := range s.alerts {
		if !alert.Enabled {
			continue
		}

		shouldFire := s.evaluateAlertCondition(alert, metrics)
		
		if shouldFire {
			// Rate limiting - same alert ‡§ï‡•ã ‡§¨‡§æ‡§∞-‡§¨‡§æ‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§≠‡•á‡§ú‡§®‡§æ
			if alert.LastFired != nil && time.Since(*alert.LastFired) < 10*time.Minute {
				continue
			}

			if err := s.fireAlert(alert, metrics); err != nil {
				log.Printf("‚ùå Failed to fire alert %s: %v", alert.Name, err)
			} else {
				now := time.Now()
				alert.LastFired = &now
				s.alertsTotal.Inc()
				log.Printf("üö® Alert fired: %s", alert.Name)
			}
		}
	}
}

// evaluateAlertCondition - Alert condition ‡§ï‡•ã evaluate ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) evaluateAlertCondition(alert Alert, metrics *AirflowMetrics) bool {
	switch alert.Condition {
	case "success_rate_below":
		return metrics.SuccessRate < alert.Threshold
		
	case "failed_tasks_above":
		return float64(metrics.FailedTasks) > alert.Threshold
		
	case "running_tasks_above":
		return float64(metrics.RunningTasks) > alert.Threshold
		
	case "avg_task_time_above":
		return metrics.AverageTaskTime > alert.Threshold
		
	case "database_unhealthy":
		return !metrics.DatabaseHealth
		
	case "scheduler_unhealthy":
		return !metrics.SchedulerHealth
		
	case "webserver_unhealthy":
		return !metrics.WebserverHealth
		
	case "celery_workers_below":
		return float64(metrics.CeleryWorkers) < alert.Threshold
		
	default:
		return false
	}
}

// fireAlert - Alert ‡§ï‡•ã fire ‡§ï‡§∞‡§®‡§æ (Slack, email, PagerDuty etc.)
func (s *AirflowMonitoringService) fireAlert(alert Alert, metrics *AirflowMetrics) error {
	alertMessage := s.formatAlertMessage(alert, metrics)
	
	for _, channel := range alert.Channel {
		switch channel {
		case "slack":
			if err := s.sendSlackAlert(alertMessage, alert.Severity); err != nil {
				return fmt.Errorf("slack alert failed: %v", err)
			}
			
		case "pagerduty":
			if alert.Severity == "critical" || alert.Severity == "high" {
				if err := s.sendPagerDutyAlert(alert, alertMessage); err != nil {
					return fmt.Errorf("pagerduty alert failed: %v", err)
				}
			}
			
		case "email":
			// Email alert implementation
			log.Printf("üìß Email alert would be sent: %s", alertMessage)
		}
	}
	
	return nil
}

// sendSlackAlert - Slack ‡§Æ‡•á‡§Ç alert ‡§≠‡•á‡§ú‡§®‡§æ
func (s *AirflowMonitoringService) sendSlackAlert(message, severity string) error {
	if s.config.SlackWebhookURL == "" {
		return fmt.Errorf("slack webhook URL not configured")
	}
	
	// Severity ‡§ï‡•á according emoji choose ‡§ï‡§∞‡§®‡§æ
	emoji := "‚ö†Ô∏è"
	color := "warning"
	
	switch severity {
	case "critical":
		emoji = "üö®"
		color = "danger"
	case "high":
		emoji = "üî•" 
		color = "danger"
	case "medium":
		emoji = "‚ö†Ô∏è"
		color = "warning"
	case "low":
		emoji = "üì¢"
		color = "good"
	}

	payload := map[string]interface{}{
		"text": fmt.Sprintf("%s Airflow Alert", emoji),
		"attachments": []map[string]interface{}{
			{
				"color":      color,
				"text":       message,
				"footer":     "Airflow Monitoring Service",
				"ts":         time.Now().Unix(),
				"mrkdwn_in":  []string{"text"},
			},
		},
	}

	payloadBytes, _ := json.Marshal(payload)
	
	resp, err := http.Post(s.config.SlackWebhookURL, "application/json", bytes.NewBuffer(payloadBytes))
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	
	if resp.StatusCode != 200 {
		return fmt.Errorf("slack webhook returned status: %d", resp.StatusCode)
	}
	
	return nil
}

// sendPagerDutyAlert - PagerDuty alert ‡§≠‡•á‡§ú‡§®‡§æ
func (s *AirflowMonitoringService) sendPagerDutyAlert(alert Alert, message string) error {
	// PagerDuty integration implementation
	log.Printf("üìü PagerDuty alert would be sent: %s", message)
	return nil
}

// formatAlertMessage - Alert message ‡§ï‡•ã format ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) formatAlertMessage(alert Alert, metrics *AirflowMetrics) string {
	ist, _ := time.LoadLocation("Asia/Kolkata")
	
	msg := fmt.Sprintf("*%s*\n", alert.Name)
	msg += fmt.Sprintf("Time: %s IST\n", time.Now().In(ist).Format("2006-01-02 15:04:05"))
	msg += fmt.Sprintf("Condition: %s\n", alert.Condition)
	msg += fmt.Sprintf("Threshold: %.2f\n", alert.Threshold)
	msg += fmt.Sprintf("Current Metrics:\n")
	msg += fmt.Sprintf("‚Ä¢ Success Rate: %.2f%%\n", metrics.SuccessRate)
	msg += fmt.Sprintf("‚Ä¢ Failed Tasks: %d\n", metrics.FailedTasks)
	msg += fmt.Sprintf("‚Ä¢ Running Tasks: %d\n", metrics.RunningTasks)
	msg += fmt.Sprintf("‚Ä¢ Avg Task Time: %.2fs\n", metrics.AverageTaskTime)
	msg += fmt.Sprintf("‚Ä¢ Celery Workers: %d\n", metrics.CeleryWorkers)
	
	return msg
}

// =============================================================================
// HTTP Handlers
// =============================================================================

// healthCheckHandler - Service health check
func (s *AirflowMonitoringService) healthCheckHandler(w http.ResponseWriter, r *http.Request) {
	health := map[string]interface{}{
		"status": "healthy",
		"timestamp": time.Now().Format(time.RFC3339),
		"version": "1.0.0",
		"components": map[string]bool{
			"database": s.checkDatabaseHealth(),
			"redis": s.redis.Ping(context.Background()).Err() == nil,
		},
	}
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(health)
}

// airflowMetricsHandler - Airflow metrics API
func (s *AirflowMonitoringService) airflowMetricsHandler(w http.ResponseWriter, r *http.Request) {
	s.mutex.RLock()
	metrics := s.metrics
	s.mutex.RUnlock()
	
	if metrics == nil {
		http.Error(w, "Metrics not available", http.StatusServiceUnavailable)
		return
	}
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(metrics)
}

// dagMetricsHandler - DAG metrics API
func (s *AirflowMonitoringService) dagMetricsHandler(w http.ResponseWriter, r *http.Request) {
	s.mutex.RLock()
	dagStatus := s.dagStatus
	s.mutex.RUnlock()
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(dagStatus)
}

// indianBusinessMetricsHandler - Indian business metrics API
func (s *AirflowMonitoringService) indianBusinessMetricsHandler(w http.ResponseWriter, r *http.Request) {
	ctx := context.Background()
	
	metricsJSON := s.redis.Get(ctx, "airflow:indian_metrics:latest").Val()
	if metricsJSON == "" {
		http.Error(w, "Indian business metrics not available", http.StatusServiceUnavailable)
		return
	}
	
	var metrics IndianBusinessMetrics
	if err := json.Unmarshal([]byte(metricsJSON), &metrics); err != nil {
		http.Error(w, "Failed to parse metrics", http.StatusInternalServerError)
		return
	}
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(metrics)
}

// =============================================================================
// Utility Functions
// =============================================================================

// updatePrometheusMetrics - Prometheus metrics update ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) updatePrometheusMetrics() {
	s.mutex.RLock()
	metrics := s.metrics
	s.mutex.RUnlock()
	
	if metrics == nil {
		return
	}
	
	s.activeDAGsGauge.Set(float64(metrics.ActiveDAGs))
	s.runningTasksGauge.Set(float64(metrics.RunningTasks))
	s.failedTasksGauge.Set(float64(metrics.FailedTasks))
	s.successRateGauge.Set(metrics.SuccessRate)
	s.taskDurationHistogram.Observe(metrics.AverageTaskTime)
}

// calculateDAGSuccessRate - DAG ‡§ï‡•Ä success rate calculate ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) calculateDAGSuccessRate(dagID string) float64 {
	query := `
	SELECT 
		COUNT(CASE WHEN state = 'success' THEN 1 END)::float / COUNT(*)::float * 100 as success_rate
	FROM dag_run 
	WHERE dag_id = $1 
	AND execution_date > NOW() - INTERVAL '24 hours'
	`
	
	var successRate sql.NullFloat64
	err := s.db.QueryRow(query, dagID).Scan(&successRate)
	
	if err != nil || !successRate.Valid {
		return 0.0
	}
	
	return successRate.Float64
}

// getDAGTaskFailures - DAG ‡§ï‡•á task failures count ‡§ï‡§∞‡§®‡§æ
func (s *AirflowMonitoringService) getDAGTaskFailures(dagID string) int {
	query := `
	SELECT COUNT(*)
	FROM task_instance ti
	JOIN dag_run dr ON ti.dag_id = dr.dag_id AND ti.execution_date = dr.execution_date
	WHERE ti.dag_id = $1 
	AND ti.state = 'failed' 
	AND ti.start_date > NOW() - INTERVAL '24 hours'
	`
	
	var failureCount int
	err := s.db.QueryRow(query, dagID).Scan(&failureCount)
	
	if err != nil {
		return 0
	}
	
	return failureCount
}

// cleanupLoop - Cleanup ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ background loop
func (s *AirflowMonitoringService) cleanupLoop() {
	ticker := time.NewTicker(1 * time.Hour)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// Old metrics ‡§ï‡•ã Redis ‡§∏‡•á clean ‡§ï‡§∞‡§®‡§æ
			ctx := context.Background()
			
			// 24 hours ‡§∏‡•á ‡§™‡•Å‡§∞‡§æ‡§®‡•á metrics delete ‡§ï‡§∞‡§®‡§æ
			cutoffTime := time.Now().Add(-24 * time.Hour)
			pattern := fmt.Sprintf("airflow:metrics:%s*", cutoffTime.Format("2006-01-02"))
			
			keys := s.redis.Keys(ctx, pattern).Val()
			if len(keys) > 0 {
				s.redis.Del(ctx, keys...).Err()
				log.Printf("üßπ Cleaned up %d old metric entries", len(keys))
			}
		}
	}
}

// loadDefaultAlerts - Default alerts load ‡§ï‡§∞‡§®‡§æ
func loadDefaultAlerts() []Alert {
	return []Alert{
		{
			ID:        "success-rate-low",
			Name:      "Low Success Rate",
			Condition: "success_rate_below",
			Threshold: 95.0,
			Severity:  "high",
			Channel:   []string{"slack", "pagerduty"},
			Enabled:   true,
		},
		{
			ID:        "failed-tasks-high",
			Name:      "High Failed Task Count",
			Condition: "failed_tasks_above", 
			Threshold: 50.0,
			Severity:  "medium",
			Channel:   []string{"slack"},
			Enabled:   true,
		},
		{
			ID:        "scheduler-down",
			Name:      "Scheduler Unhealthy",
			Condition: "scheduler_unhealthy",
			Threshold: 0,
			Severity:  "critical",
			Channel:   []string{"slack", "pagerduty", "email"},
			Enabled:   true,
		},
		{
			ID:        "database-down",
			Name:      "Database Unhealthy", 
			Condition: "database_unhealthy",
			Threshold: 0,
			Severity:  "critical",
			Channel:   []string{"slack", "pagerduty", "email"},
			Enabled:   true,
		},
		{
			ID:        "workers-low",
			Name:      "Low Celery Worker Count",
			Condition: "celery_workers_below",
			Threshold: 2.0,
			Severity:  "medium", 
			Channel:   []string{"slack"},
			Enabled:   true,
		},
	}
}

// =============================================================================
// Main Function
// =============================================================================

func main() {
	// Configuration load ‡§ï‡§∞‡§®‡§æ environment variables ‡§∏‡•á
	config := &Config{
		Port:              getEnv("PORT", "8080"),
		DatabaseURL:       getEnv("DATABASE_URL", "postgres://airflow:airflow@localhost/airflow?sslmode=disable"),
		RedisURL:          getEnv("REDIS_URL", "redis://localhost:6379/0"),
		AirflowWebURL:     getEnv("AIRFLOW_WEB_URL", "http://localhost:8080"),
		SlackWebhookURL:   getEnv("SLACK_WEBHOOK_URL", ""),
		PagerDutyKey:      getEnv("PAGERDUTY_KEY", ""),
		CheckInterval:     30 * time.Second,
		MetricsRetention:  24 * time.Hour,
	}

	// Monitoring service create ‡§ï‡§∞‡§®‡§æ
	service, err := NewAirflowMonitoringService(config)
	if err != nil {
		log.Fatalf("‚ùå Failed to create monitoring service: %v", err)
	}

	// Service start ‡§ï‡§∞‡§®‡§æ
	log.Printf("üáÆüá≥ Starting Airflow Monitoring Service for Indian Infrastructure...")
	if err := service.Start(); err != nil {
		log.Fatalf("‚ùå Service failed to start: %v", err)
	}
}

// getEnv - Environment variable ‡§ï‡•ã default ‡§ï‡•á ‡§∏‡§æ‡§• get ‡§ï‡§∞‡§®‡§æ
func getEnv(key, defaultValue string) string {
	if value := os.Getenv(key); value != "" {
		return value
	}
	return defaultValue
}

/*
Airflow Monitoring Service Summary
=================================

‡§Ø‡§π comprehensive Go service Airflow monitoring ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à:

### Key Features:
1. **Real-time Monitoring**: DAG status, task metrics, system health
2. **Indian Business Context**: Festival seasons, monsoon, business hours
3. **Multi-channel Alerting**: Slack, PagerDuty, Email
4. **Prometheus Integration**: Metrics export for Grafana dashboards  
5. **High Performance**: Go-based for low latency and high throughput
6. **Production Ready**: Error handling, rate limiting, cleanup

### Monitoring Capabilities:
- Airflow component health (Scheduler, Webserver, Database)
- Task execution metrics and success rates
- DAG-level performance tracking
- Celery worker monitoring
- Payment gateway load tracking
- Regional load factor calculations

### Indian Infrastructure Optimizations:
- IST timezone handling
- Festival season traffic patterns
- Monsoon season considerations
- Banking hours awareness
- Regional load distribution

### Production Deployment:
```bash
# Build ‡§ï‡§∞‡§®‡§æ
go build -o airflow-monitor airflow_monitoring_service.go

# Environment variables set ‡§ï‡§∞‡§®‡§æ
export DATABASE_URL="postgres://user:pass@host/airflow"
export REDIS_URL="redis://host:6379/0"
export SLACK_WEBHOOK_URL="https://hooks.slack.com/..."

# Service start ‡§ï‡§∞‡§®‡§æ
./airflow-monitor
```

### API Endpoints:
- `/health` - Service health check
- `/metrics/airflow` - Airflow metrics
- `/metrics/indian-business` - Indian business context metrics
- `/prometheus` - Prometheus metrics
- `/dashboard/overview` - Dashboard data

‡§Ø‡§π service Indian tech companies ‡§ï‡•Ä unique requirements ‡§ï‡•ã ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§§‡•á ‡§π‡•Å‡§è
enterprise-grade Airflow monitoring ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§
*/