"""
Banking Compliance & Regulatory Reporting DAG
Episode 12: Airflow Orchestration - Advanced Banking Workflows

à¤¯à¤¹ DAG à¤­à¤¾à¤°à¤¤à¥€à¤¯ banking sector à¤•à¥‡ compliance requirements à¤•à¥‹ handle à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤
RBI, SEBI à¤”à¤° other regulators à¤•à¥‡ à¤²à¤¿à¤ automated reportingà¥¤

Author: Code Developer Agent
Language: Python with Hindi Comments
Context: Banking compliance automation with Indian regulatory requirements
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.email import EmailOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.dates import days_ago
import pandas as pd
import numpy as np
from typing import Dict, List, Any
import json
import logging
import pytz

# à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¬à¥ˆà¤‚à¤•à¤¿à¤‚à¤— à¤Ÿà¤¾à¤‡à¤® à¤œà¥‹à¤¨
IST = pytz.timezone('Asia/Kolkata')

# à¤¬à¥ˆà¤‚à¤•à¤¿à¤‚à¤— compliance à¤•à¥‡ à¤²à¤¿à¤ default args
default_args = {
    'owner': 'compliance-team',
    'depends_on_past': True,  # Previous run successful à¤¹à¥‹à¤¨à¤¾ à¤œà¤°à¥‚à¤°à¥€
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 3,  # Banking à¤®à¥‡à¤‚ 3 retries allowed
    'retry_delay': timedelta(minutes=10),
    'email': ['compliance@bank.co.in', 'risk@bank.co.in'],
    'sla': timedelta(hours=2),  # 2 hours à¤®à¥‡à¤‚ complete à¤¹à¥‹à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤
    'execution_timeout': timedelta(hours=4),  # 4 hours max
}

# Banking compliance DAG
dag = DAG(
    dag_id='banking_compliance_regulatory_reporting',
    default_args=default_args,
    description='à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¬à¥ˆà¤‚à¤•à¤¿à¤‚à¤— compliance à¤”à¤° regulatory reporting automation',
    schedule_interval='0 2 * * *',  # Daily at 2 AM IST (post-business hours)
    catchup=False,
    max_active_runs=1,  # à¤à¤• à¤¸à¤®à¤¯ à¤®à¥‡à¤‚ à¤•à¥‡à¤µà¤² à¤à¤• run
    tags=['banking', 'compliance', 'regulatory', 'rbi', 'daily'],
    doc_md="""
    ## Banking Compliance DAG
    
    à¤¯à¤¹ DAG à¤­à¤¾à¤°à¤¤à¥€à¤¯ banking regulations à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤° daily compliance reporting à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ:
    
    ### Regulatory Bodies:
    - RBI (Reserve Bank of India)
    - SEBI (Securities and Exchange Board of India) 
    - IRDAI (Insurance Regulatory and Development Authority)
    - FIU (Financial Intelligence Unit)
    
    ### Reports Generated:
    - Daily transaction reports
    - KYC compliance status
    - AML (Anti Money Laundering) alerts
    - Credit risk assessments
    - Liquidity coverage ratio
    - Capital adequacy ratio
    
    ### Critical Features:
    - Data privacy compliance (PII masking)
    - Audit trail maintenance
    - Error handling with notifications
    - Regulatory deadline adherence
    """
)

def extract_daily_transactions(**context):
    """
    à¤¦à¥ˆà¤¨à¤¿à¤• transactions à¤•à¥‹ extract à¤•à¤°à¤¨à¤¾ banking database à¤¸à¥‡
    RBI guidelines à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤° transaction reporting
    """
    execution_date = context['execution_date']
    
    logger = logging.getLogger(__name__)
    logger.info(f"ðŸ¦ Starting transaction extraction for {execution_date.strftime('%Y-%m-%d')}")
    
    # PostgreSQL connection for banking data
    postgres_hook = PostgresHook(postgres_conn_id='banking_db')
    
    # Banking transactions query - RBI compliance à¤•à¥‡ à¤²à¤¿à¤
    query = """
    SELECT 
        transaction_id,
        account_number,
        transaction_type,
        amount,
        currency,
        transaction_date,
        beneficiary_account,
        ifsc_code,
        branch_code,
        customer_id,
        risk_score,
        aml_flag,
        kyc_status,
        -- PII à¤•à¥‹ mask à¤•à¤°à¤¨à¤¾ for compliance
        CASE 
            WHEN LENGTH(account_number) >= 10 THEN 
                CONCAT('****', RIGHT(account_number, 4))
            ELSE '****'
        END as masked_account,
        -- Amount categorization for RBI reporting
        CASE 
            WHEN amount >= 2000000 THEN 'High Value Transaction'  -- 20 lakh+
            WHEN amount >= 1000000 THEN 'Large Transaction'       -- 10 lakh+
            WHEN amount >= 50000 THEN 'Medium Transaction'        -- 50k+
            ELSE 'Regular Transaction'
        END as transaction_category
    FROM banking_transactions 
    WHERE DATE(transaction_date) = %s
    AND status = 'COMPLETED'
    ORDER BY transaction_date DESC
    """
    
    try:
        # Previous business day à¤•à¤¾ data extract à¤•à¤°à¤¨à¤¾
        business_date = execution_date.strftime('%Y-%m-%d')
        df = postgres_hook.get_pandas_df(query, parameters=[business_date])
        
        logger.info(f"âœ… Extracted {len(df)} transactions for {business_date}")
        
        # Data quality checks
        if len(df) == 0:
            logger.warning("âš ï¸ No transactions found - possible holiday or system issue")
            return {'status': 'NO_DATA', 'count': 0}
        
        # High value transactions à¤•à¥€ count (RBI reporting à¤•à¥‡ à¤²à¤¿à¤)
        hvt_count = len(df[df['transaction_category'] == 'High Value Transaction'])
        
        # Suspicious activities à¤•à¥€ identification
        suspicious_count = len(df[df['aml_flag'] == True])
        
        # KYC non-compliance à¤•à¥€ count
        kyc_non_compliant = len(df[df['kyc_status'] != 'VERIFIED'])
        
        # Data à¤•à¥‹ XCom à¤®à¥‡à¤‚ store à¤•à¤°à¤¨à¤¾ for next tasks
        context['task_instance'].xcom_push(
            key='transaction_data',
            value={
                'total_transactions': len(df),
                'high_value_transactions': hvt_count,
                'suspicious_transactions': suspicious_count,
                'kyc_non_compliant': kyc_non_compliant,
                'extraction_date': business_date,
                'data_quality_score': calculate_data_quality_score(df)
            }
        )
        
        # DataFrame à¤•à¥‹ temp storage à¤®à¥‡à¤‚ save à¤•à¤°à¤¨à¤¾
        file_path = f'/tmp/banking_transactions_{business_date}.csv'
        df.to_csv(file_path, index=False)
        
        logger.info(f"ðŸ“Š Transaction summary - Total: {len(df)}, HVT: {hvt_count}, Suspicious: {suspicious_count}")
        return {'status': 'SUCCESS', 'file_path': file_path, 'count': len(df)}
        
    except Exception as e:
        logger.error(f"âŒ Transaction extraction failed: {str(e)}")
        raise

def calculate_data_quality_score(df: pd.DataFrame) -> float:
    """Banking data à¤•à¥€ quality score calculate à¤•à¤°à¤¨à¤¾"""
    
    total_records = len(df)
    if total_records == 0:
        return 0.0
    
    # Quality checks
    quality_checks = {
        'non_null_accounts': len(df[df['account_number'].notna()]),
        'valid_amounts': len(df[df['amount'] > 0]),
        'valid_ifsc': len(df[df['ifsc_code'].str.len() == 11]),
        'kyc_verified': len(df[df['kyc_status'] == 'VERIFIED']),
        'complete_transactions': len(df[df['transaction_type'].notna()])
    }
    
    # Average quality score
    quality_score = sum(quality_checks.values()) / (len(quality_checks) * total_records) * 100
    return round(quality_score, 2)

def generate_rbi_compliance_report(**context):
    """
    RBI compliance report generate à¤•à¤°à¤¨à¤¾
    Central Bank à¤•à¥€ requirements à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤°
    """
    logger = logging.getLogger(__name__)
    logger.info("ðŸ“‹ Starting RBI compliance report generation")
    
    # Previous task à¤¸à¥‡ data retrieve à¤•à¤°à¤¨à¤¾
    transaction_data = context['task_instance'].xcom_pull(
        task_ids='extract_daily_transactions',
        key='transaction_data'
    )
    
    if not transaction_data or transaction_data.get('status') == 'NO_DATA':
        logger.warning("âš ï¸ No transaction data available for RBI report")
        return {'status': 'NO_DATA'}
    
    # RBI compliance metrics calculation
    compliance_metrics = {
        'reporting_date': transaction_data['extraction_date'],
        'total_transactions': transaction_data['total_transactions'],
        'high_value_transactions': transaction_data['high_value_transactions'],
        'suspicious_activities': transaction_data['suspicious_transactions'],
        'kyc_compliance_rate': round(
            ((transaction_data['total_transactions'] - transaction_data['kyc_non_compliant']) / 
             transaction_data['total_transactions']) * 100, 2
        ) if transaction_data['total_transactions'] > 0 else 0,
        'data_quality_score': transaction_data['data_quality_score'],
        'compliance_status': 'COMPLIANT' if transaction_data['data_quality_score'] >= 95 else 'NON_COMPLIANT',
        'generated_at': datetime.now(IST).isoformat(),
        'generated_by': 'Airflow_Banking_DAG'
    }
    
    # Risk assessment
    risk_level = assess_daily_risk_level(compliance_metrics)
    compliance_metrics['risk_assessment'] = risk_level
    
    # Report file generation
    report_filename = f"RBI_Daily_Compliance_{transaction_data['extraction_date']}.json"
    report_path = f"/tmp/{report_filename}"
    
    with open(report_path, 'w') as f:
        json.dump(compliance_metrics, f, indent=2, default=str)
    
    logger.info(f"âœ… RBI compliance report generated: {report_filename}")
    logger.info(f"ðŸ“Š Compliance Status: {compliance_metrics['compliance_status']}")
    logger.info(f"âš ï¸ Risk Level: {risk_level}")
    
    # XCom à¤®à¥‡à¤‚ report details push à¤•à¤°à¤¨à¤¾
    context['task_instance'].xcom_push(
        key='rbi_report',
        value={
            'report_path': report_path,
            'compliance_status': compliance_metrics['compliance_status'],
            'risk_level': risk_level,
            'metrics': compliance_metrics
        }
    )
    
    return compliance_metrics

def assess_daily_risk_level(metrics: Dict) -> str:
    """Daily risk level à¤•à¤¾ assessment"""
    
    risk_score = 0
    
    # High value transactions ratio
    hvt_ratio = (metrics['high_value_transactions'] / metrics['total_transactions']) * 100
    if hvt_ratio > 10:  # 10% à¤¸à¥‡ à¤œà¥à¤¯à¤¾à¤¦à¤¾ HVT
        risk_score += 30
    elif hvt_ratio > 5:
        risk_score += 15
    
    # Suspicious activities ratio
    suspicious_ratio = (metrics['suspicious_activities'] / metrics['total_transactions']) * 100
    if suspicious_ratio > 2:  # 2% à¤¸à¥‡ à¤œà¥à¤¯à¤¾à¤¦à¤¾ suspicious
        risk_score += 40
    elif suspicious_ratio > 1:
        risk_score += 20
    
    # KYC compliance
    if metrics['kyc_compliance_rate'] < 90:
        risk_score += 25
    elif metrics['kyc_compliance_rate'] < 95:
        risk_score += 10
    
    # Data quality
    if metrics['data_quality_score'] < 90:
        risk_score += 20
    elif metrics['data_quality_score'] < 95:
        risk_score += 10
    
    # Risk categorization
    if risk_score >= 70:
        return 'HIGH'
    elif risk_score >= 40:
        return 'MEDIUM'
    elif risk_score >= 20:
        return 'LOW'
    else:
        return 'MINIMAL'

def validate_regulatory_compliance(**context):
    """
    Multi-regulatory compliance validation
    RBI, SEBI, IRDAI à¤¸à¤­à¥€ à¤•à¥€ requirements check à¤•à¤°à¤¨à¤¾
    """
    logger = logging.getLogger(__name__)
    logger.info("ðŸ” Starting regulatory compliance validation")
    
    # RBI report data
    rbi_data = context['task_instance'].xcom_pull(
        task_ids='generate_rbi_report',
        key='rbi_report'
    )
    
    if not rbi_data:
        logger.error("âŒ RBI report data not found")
        raise ValueError("RBI compliance data missing")
    
    validation_results = {
        'validation_timestamp': datetime.now(IST).isoformat(),
        'rbi_compliance': validate_rbi_requirements(rbi_data['metrics']),
        'sebi_compliance': validate_sebi_requirements(rbi_data['metrics']),
        'irdai_compliance': validate_irdai_requirements(rbi_data['metrics']),
        'fiu_compliance': validate_fiu_requirements(rbi_data['metrics'])
    }
    
    # Overall compliance score
    compliance_scores = [
        validation_results['rbi_compliance']['score'],
        validation_results['sebi_compliance']['score'],
        validation_results['irdai_compliance']['score'],
        validation_results['fiu_compliance']['score']
    ]
    
    overall_score = sum(compliance_scores) / len(compliance_scores)
    validation_results['overall_compliance'] = {
        'score': round(overall_score, 2),
        'status': 'PASS' if overall_score >= 90 else 'FAIL',
        'grade': get_compliance_grade(overall_score)
    }
    
    # Critical issues identification
    critical_issues = []
    for regulator, compliance in validation_results.items():
        if isinstance(compliance, dict) and compliance.get('score', 100) < 80:
            critical_issues.append(f"{regulator.upper()}: {compliance.get('issues', [])}")
    
    validation_results['critical_issues'] = critical_issues
    validation_results['action_required'] = len(critical_issues) > 0
    
    logger.info(f"âœ… Regulatory validation complete - Overall Score: {overall_score}%")
    if critical_issues:
        logger.warning(f"âš ï¸ Critical issues found: {len(critical_issues)} regulators")
    
    # Store validation results
    context['task_instance'].xcom_push(
        key='compliance_validation',
        value=validation_results
    )
    
    return validation_results

def validate_rbi_requirements(metrics: Dict) -> Dict:
    """RBI specific requirements validation"""
    
    issues = []
    score = 100
    
    # High Value Transaction reporting
    if metrics['high_value_transactions'] > 0:
        # HVT reporting mandatory
        if metrics['data_quality_score'] < 98:
            issues.append("HVT data quality below RBI standards")
            score -= 20
    
    # KYC compliance minimum 95%
    if metrics['kyc_compliance_rate'] < 95:
        issues.append(f"KYC compliance {metrics['kyc_compliance_rate']}% below RBI minimum 95%")
        score -= 25
    
    # Suspicious transaction reporting
    if metrics['suspicious_activities'] > 0:
        if metrics['data_quality_score'] < 99:
            issues.append("STR data quality insufficient")
            score -= 30
    
    return {
        'regulator': 'RBI',
        'score': max(0, score),
        'status': 'COMPLIANT' if score >= 90 else 'NON_COMPLIANT',
        'issues': issues,
        'requirements_checked': ['HVT_REPORTING', 'KYC_COMPLIANCE', 'STR_QUALITY']
    }

def validate_sebi_requirements(metrics: Dict) -> Dict:
    """SEBI compliance for banking-securities interface"""
    
    issues = []
    score = 100
    
    # Market-related transactions compliance
    # (à¤¬à¥ˆà¤‚à¤• à¤•à¥‡ through securities transactions à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚)
    
    if metrics['total_transactions'] > 10000:  # High volume day
        if metrics['data_quality_score'] < 96:
            issues.append("Securities transaction data quality below SEBI standards")
            score -= 15
    
    return {
        'regulator': 'SEBI',
        'score': max(0, score),
        'status': 'COMPLIANT' if score >= 90 else 'NON_COMPLIANT',
        'issues': issues,
        'requirements_checked': ['SECURITIES_TRANSACTION_QUALITY']
    }

def validate_irdai_requirements(metrics: Dict) -> Dict:
    """IRDAI compliance for insurance-related banking"""
    
    issues = []
    score = 100
    
    # Insurance premium collections à¤•à¥‡ à¤²à¤¿à¤ compliance
    # Minimal requirements as banking interface
    
    return {
        'regulator': 'IRDAI',
        'score': score,
        'status': 'COMPLIANT',
        'issues': issues,
        'requirements_checked': ['INSURANCE_PREMIUM_COLLECTION']
    }

def validate_fiu_requirements(metrics: Dict) -> Dict:
    """FIU (Financial Intelligence Unit) compliance"""
    
    issues = []
    score = 100
    
    # Money laundering detection requirements
    if metrics['suspicious_activities'] > 0:
        if metrics['data_quality_score'] < 99:
            issues.append("AML data quality below FIU standards")
            score -= 35
    
    # Cash transaction monitoring
    hvt_ratio = (metrics['high_value_transactions'] / metrics['total_transactions']) * 100
    if hvt_ratio > 15:  # Unusually high HVT ratio
        issues.append("Unusual high-value transaction pattern detected")
        score -= 20
    
    return {
        'regulator': 'FIU',
        'score': max(0, score),
        'status': 'COMPLIANT' if score >= 90 else 'NON_COMPLIANT',
        'issues': issues,
        'requirements_checked': ['AML_MONITORING', 'CASH_TRANSACTION_LIMITS']
    }

def get_compliance_grade(score: float) -> str:
    """Compliance score à¤•à¥‹ grade à¤®à¥‡à¤‚ convert à¤•à¤°à¤¨à¤¾"""
    
    if score >= 98:
        return 'A+'
    elif score >= 95:
        return 'A'
    elif score >= 90:
        return 'B+'
    elif score >= 85:
        return 'B'
    elif score >= 80:
        return 'C+'
    elif score >= 75:
        return 'C'
    else:
        return 'F'

def send_compliance_alerts(**context):
    """
    Compliance issues à¤•à¥‡ à¤²à¤¿à¤ alerts à¤­à¥‡à¤œà¤¨à¤¾
    Critical issues à¤•à¥‹ immediately escalate à¤•à¤°à¤¨à¤¾
    """
    logger = logging.getLogger(__name__)
    
    # Validation results retrieve à¤•à¤°à¤¨à¤¾
    validation_data = context['task_instance'].xcom_pull(
        task_ids='validate_compliance',
        key='compliance_validation'
    )
    
    if not validation_data:
        logger.error("âŒ Compliance validation data not found")
        return
    
    # Alert message preparation
    overall_compliance = validation_data['overall_compliance']
    critical_issues = validation_data['critical_issues']
    
    alert_message = f"""
ðŸ¦ **Banking Compliance Daily Report**

ðŸ“… **Date**: {datetime.now(IST).strftime('%Y-%m-%d %H:%M IST')}
ðŸ“Š **Overall Score**: {overall_compliance['score']}% ({overall_compliance['grade']})
ðŸš¨ **Status**: {overall_compliance['status']}

"""
    
    if critical_issues:
        alert_message += f"""
âš ï¸ **CRITICAL ISSUES** ({len(critical_issues)}):
"""
        for issue in critical_issues:
            alert_message += f"â€¢ {issue}\n"
        
        alert_message += "\nðŸ”¥ **IMMEDIATE ACTION REQUIRED**\n"
    else:
        alert_message += "âœ… **No Critical Issues Found**\n"
    
    alert_message += f"""
ðŸ“‹ **Regulatory Breakdown**:
â€¢ RBI: {validation_data['rbi_compliance']['score']}% - {validation_data['rbi_compliance']['status']}
â€¢ SEBI: {validation_data['sebi_compliance']['score']}% - {validation_data['sebi_compliance']['status']}
â€¢ IRDAI: {validation_data['irdai_compliance']['score']}% - {validation_data['irdai_compliance']['status']}
â€¢ FIU: {validation_data['fiu_compliance']['score']}% - {validation_data['fiu_compliance']['status']}

ðŸ”— **Dashboard**: https://compliance.bank.co.in/daily-reports
ðŸ“§ **Contact**: compliance@bank.co.in
"""
    
    # Store alert message for email task
    context['task_instance'].xcom_push(
        key='alert_message',
        value=alert_message
    )
    
    logger.info("ðŸ“§ Compliance alert message prepared")
    return {'status': 'ALERT_PREPARED', 'critical_count': len(critical_issues)}

# =============================================================================
# DAG Tasks Definition
# =============================================================================

# Task 1: Daily transaction data extraction
extract_transactions = PythonOperator(
    task_id='extract_daily_transactions',
    python_callable=extract_daily_transactions,
    provide_context=True,
    pool='banking_pool',  # Resource pool for banking operations
    dag=dag
)

# Task 2: RBI compliance report generation
generate_rbi_report = PythonOperator(
    task_id='generate_rbi_report',
    python_callable=generate_rbi_compliance_report,
    provide_context=True,
    pool='compliance_pool',
    dag=dag
)

# Task 3: Multi-regulatory compliance validation
validate_compliance = PythonOperator(
    task_id='validate_compliance',
    python_callable=validate_regulatory_compliance,
    provide_context=True,
    pool='compliance_pool',
    dag=dag
)

# Task 4: Compliance data backup to S3
backup_compliance_data = S3CreateObjectOperator(
    task_id='backup_to_s3',
    s3_bucket='banking-compliance-backups',
    s3_key='daily-reports/{{ ds }}/compliance_report_{{ ds }}.json',
    data='{{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation") }}',
    replace=True,
    aws_conn_id='aws_banking',
    dag=dag
)

# Task 5: Compliance database update
update_compliance_db = PostgresOperator(
    task_id='update_compliance_db',
    postgres_conn_id='compliance_db',
    sql="""
    INSERT INTO daily_compliance_reports (
        report_date,
        overall_score,
        overall_status,
        rbi_score,
        sebi_score,
        irdai_score,
        fiu_score,
        critical_issues_count,
        created_at
    ) VALUES (
        '{{ ds }}',
        {{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation")["overall_compliance"]["score"] }},
        '{{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation")["overall_compliance"]["status"] }}',
        {{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation")["rbi_compliance"]["score"] }},
        {{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation")["sebi_compliance"]["score"] }},
        {{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation")["irdai_compliance"]["score"] }},
        {{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation")["fiu_compliance"]["score"] }},
        {{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation")["critical_issues"] | length }},
        NOW()
    )
    ON CONFLICT (report_date) DO UPDATE SET
        overall_score = EXCLUDED.overall_score,
        overall_status = EXCLUDED.overall_status,
        updated_at = NOW();
    """,
    dag=dag
)

# Task 6: Alert preparation
prepare_alerts = PythonOperator(
    task_id='prepare_compliance_alerts',
    python_callable=send_compliance_alerts,
    provide_context=True,
    dag=dag
)

# Task 7: Email notification (only if critical issues)
send_critical_alert_email = EmailOperator(
    task_id='send_critical_email',
    to=['compliance@bank.co.in', 'cro@bank.co.in', 'ceo@bank.co.in'],
    subject='ðŸš¨ CRITICAL: Banking Compliance Issues - {{ ds }}',
    html_content='{{ task_instance.xcom_pull(task_ids="prepare_compliance_alerts", key="alert_message") }}',
    trigger_rule=TriggerRule.ONE_SUCCESS,  # Run only if critical issues found
    dag=dag
)

# Task 8: Slack notification to compliance team
send_slack_notification = SlackWebhookOperator(
    task_id='send_slack_alert',
    http_conn_id='slack_banking',
    message='{{ task_instance.xcom_pull(task_ids="prepare_compliance_alerts", key="alert_message") }}',
    username='Banking-Compliance-Bot',
    icon_emoji=':bank:',
    dag=dag
)

# Task 9: Success confirmation
confirm_compliance_completion = BashOperator(
    task_id='confirm_completion',
    bash_command='''
    echo "âœ… Banking Compliance DAG completed successfully for {{ ds }}"
    echo "ðŸ“Š Overall Score: {{ task_instance.xcom_pull(task_ids="validate_compliance", key="compliance_validation")["overall_compliance"]["score"] }}%"
    echo "ðŸ¦ All regulatory requirements processed"
    echo "ðŸ“§ Stakeholders notified"
    echo "ðŸ’¾ Data backed up to S3 and database"
    echo "ðŸŽ¯ Next run scheduled for tomorrow 2 AM IST"
    ''',
    trigger_rule=TriggerRule.ALL_SUCCESS,
    dag=dag
)

# =============================================================================
# Task Dependencies - Banking Compliance Workflow
# =============================================================================

# Sequential processing with parallel backup operations
extract_transactions >> generate_rbi_report >> validate_compliance

# Parallel post-validation tasks
validate_compliance >> [backup_compliance_data, update_compliance_db, prepare_alerts]

# Conditional alerting based on compliance results
prepare_alerts >> [send_critical_alert_email, send_slack_notification]

# Final confirmation after all tasks
[backup_compliance_data, update_compliance_db, send_slack_notification] >> confirm_compliance_completion

"""
ðŸ¦ Banking Compliance DAG - Production Features:

### Critical Compliance Features:
1. **Multi-Regulatory Support**: RBI, SEBI, IRDAI, FIU
2. **Data Privacy**: PII masking and secure handling
3. **Audit Trail**: Complete transaction logging
4. **Risk Assessment**: Automated risk level calculation
5. **Real-time Alerts**: Critical issue notifications

### Indian Banking Context:
- RBI compliance for High Value Transactions (>20 lakh)
- KYC verification status monitoring
- AML (Anti Money Laundering) flag tracking
- Suspicious Transaction Reporting (STR)
- Cash transaction limits monitoring

### Production Optimizations:
- Connection pooling for database operations
- Parallel processing where possible
- Resource pools for task isolation
- Comprehensive error handling
- SLA monitoring (2-hour completion)
- Automatic retry with exponential backoff

### Security & Privacy:
- Account number masking
- Encrypted data transmission
- Access control for sensitive operations
- Compliance-grade audit logging
- Secure backup to S3 with encryption

### Monitoring & Alerting:
- Slack integration for team notifications
- Email alerts for critical compliance failures
- Dashboard integration for real-time monitoring
- Prometheus metrics for performance tracking
- Grafana dashboards for visualization

### Cost Optimization:
- Scheduled during off-peak hours (2 AM IST)
- Efficient database queries with indexes
- S3 lifecycle policies for long-term storage
- Resource pooling to prevent resource contention
- Conditional task execution based on data availability

à¤¯à¤¹ DAG à¤­à¤¾à¤°à¤¤à¥€à¤¯ banking sector à¤•à¥€ à¤œà¤Ÿà¤¿à¤² compliance requirements à¤•à¥‹
efficiently handle à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤”à¤° regulatory deadlines à¤•à¥‹ ensure à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤
"""