"""
E-commerce Recommendation Engine DAG - Flipkart Style
Episode 12: Airflow Orchestration - ML Pipeline Automation

‡§Ø‡§π DAG e-commerce recommendation engine ‡§ï‡•ã train ‡§î‡§∞ deploy ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
Real-time customer behavior analysis ‡§ï‡•á ‡§∏‡§æ‡§• personalized recommendations‡•§

Author: Code Developer Agent
Language: Python with Hindi Comments
Context: ML-powered recommendation system with Indian e-commerce patterns
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.operators.s3 import S3UploadFileOperator
from airflow.providers.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.dates import days_ago
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pickle
import json
import logging
import pytz
from typing import Dict, List, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

# ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø e-commerce ‡§ü‡§æ‡§á‡§Æ ‡§ú‡•ã‡§®
IST = pytz.timezone('Asia/Kolkata')

# ML pipeline ‡§ï‡•á ‡§≤‡§ø‡§è default args
default_args = {
    'owner': 'ml-engineering-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=15),
    'email': ['ml-team@flipkart.com', 'data-science@flipkart.com'],
    'sla': timedelta(hours=6),  # 6 hours ‡§Æ‡•á‡§Ç complete ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è
    'execution_timeout': timedelta(hours=8),  # 8 hours max
}

# E-commerce recommendation DAG
dag = DAG(
    dag_id='ecommerce_recommendation_ml_pipeline',
    default_args=default_args,
    description='‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø e-commerce recommendation engine - ML pipeline automation',
    schedule_interval='0 3 * * *',  # Daily at 3 AM IST (post data processing)
    catchup=False,
    max_active_runs=1,
    tags=['ml', 'recommendation', 'ecommerce', 'flipkart', 'daily'],
    doc_md="""
    ## E-commerce Recommendation Engine DAG
    
    ‡§Ø‡§π DAG Indian e-commerce platform ‡§ï‡•á ‡§≤‡§ø‡§è comprehensive recommendation system ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à:
    
    ### Business Context:
    - Flipkart/Amazon style product recommendations
    - Customer behavior analysis
    - Seasonal trends (Festival seasons, Monsoon, Summer)
    - Regional preferences (North vs South India)
    - Price sensitivity analysis
    
    ### ML Components:
    - Collaborative filtering
    - Content-based filtering  
    - Hybrid recommendation system
    - Real-time model serving
    - A/B testing framework
    
    ### Indian Market Features:
    - Festival season boost (Diwali, Eid, Christmas)
    - Regional language preferences
    - Price range categorization (Budget, Premium, Luxury)
    - Cash-on-delivery preference analysis
    - Mobile-first user behavior
    
    ### Performance Metrics:
    - Click-through rate (CTR)
    - Conversion rate optimization
    - Revenue per recommendation
    - Customer lifetime value impact
    """
)

def extract_customer_behavior_data(**context):
    """
    Customer behavior data ‡§ï‡•ã extract ‡§ï‡§∞‡§®‡§æ e-commerce database ‡§∏‡•á
    Last 30 days ‡§ï‡§æ comprehensive data analysis
    """
    execution_date = context['execution_date']
    logger = logging.getLogger(__name__)
    
    logger.info(f"üõí Starting customer behavior extraction for {execution_date.strftime('%Y-%m-%d')}")
    
    # PostgreSQL connection for e-commerce data
    postgres_hook = PostgresHook(postgres_conn_id='ecommerce_db')
    
    # Comprehensive customer behavior query
    behavior_query = """
    WITH customer_metrics AS (
        SELECT 
            c.customer_id,
            c.age,
            c.gender,
            c.city,
            c.state,
            c.registration_date,
            -- Purchase behavior metrics
            COUNT(DISTINCT o.order_id) as total_orders,
            SUM(o.order_value) as total_spent,
            AVG(o.order_value) as avg_order_value,
            MAX(o.order_date) as last_order_date,
            -- Product preferences
            COUNT(DISTINCT oi.product_id) as unique_products_bought,
            MODE() WITHIN GROUP (ORDER BY p.category_id) as preferred_category,
            -- Engagement metrics
            COUNT(DISTINCT s.session_id) as total_sessions,
            SUM(s.session_duration_minutes) as total_session_time,
            COUNT(DISTINCT pv.product_id) as products_viewed,
            COUNT(DISTINCT w.product_id) as wishlist_items,
            -- Seasonal behavior
            COUNT(CASE WHEN EXTRACT(month FROM o.order_date) IN (10,11,12) THEN 1 END) as festival_orders,
            COUNT(CASE WHEN EXTRACT(month FROM o.order_date) IN (6,7,8,9) THEN 1 END) as monsoon_orders,
            -- Regional behavior  
            CASE 
                WHEN c.state IN ('Maharashtra', 'Gujarat', 'Rajasthan', 'Goa') THEN 'West'
                WHEN c.state IN ('Tamil Nadu', 'Karnataka', 'Andhra Pradesh', 'Kerala') THEN 'South'
                WHEN c.state IN ('Delhi', 'Punjab', 'Haryana', 'Uttar Pradesh') THEN 'North'
                WHEN c.state IN ('West Bengal', 'Odisha', 'Jharkhand', 'Bihar') THEN 'East'
                ELSE 'Other'
            END as region,
            -- Price sensitivity
            CASE 
                WHEN AVG(o.order_value) < 1000 THEN 'Budget'
                WHEN AVG(o.order_value) < 5000 THEN 'Mid-Range'
                WHEN AVG(o.order_value) < 20000 THEN 'Premium'
                ELSE 'Luxury'
            END as price_segment
        FROM customers c
        LEFT JOIN orders o ON c.customer_id = o.customer_id
        LEFT JOIN order_items oi ON o.order_id = oi.order_id
        LEFT JOIN products p ON oi.product_id = p.product_id
        LEFT JOIN user_sessions s ON c.customer_id = s.customer_id
        LEFT JOIN product_views pv ON c.customer_id = pv.customer_id
        LEFT JOIN wishlist w ON c.customer_id = w.customer_id
        WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days'
        AND o.order_status = 'DELIVERED'
        GROUP BY c.customer_id, c.age, c.gender, c.city, c.state, c.registration_date
        HAVING COUNT(DISTINCT o.order_id) >= 2  -- At least 2 orders
    ),
    product_interactions AS (
        SELECT 
            oi.product_id,
            p.product_name,
            p.category_id,
            p.subcategory_id,
            p.brand,
            p.price,
            p.rating,
            p.reviews_count,
            -- Sales metrics
            COUNT(DISTINCT oi.order_id) as times_ordered,
            SUM(oi.quantity) as total_quantity_sold,
            SUM(oi.quantity * oi.unit_price) as total_revenue,
            AVG(oi.unit_price) as avg_selling_price,
            -- Customer engagement
            COUNT(DISTINCT pv.customer_id) as unique_viewers,
            COUNT(DISTINCT w.customer_id) as wishlist_adds,
            COUNT(DISTINCT r.customer_id) as review_count,
            AVG(r.rating) as avg_customer_rating,
            -- Regional performance
            COUNT(DISTINCT CASE WHEN c.state IN ('Maharashtra', 'Gujarat') THEN oi.order_id END) as west_sales,
            COUNT(DISTINCT CASE WHEN c.state IN ('Tamil Nadu', 'Karnataka') THEN oi.order_id END) as south_sales,
            COUNT(DISTINCT CASE WHEN c.state IN ('Delhi', 'Punjab') THEN oi.order_id END) as north_sales,
            COUNT(DISTINCT CASE WHEN c.state IN ('West Bengal', 'Odisha') THEN oi.order_id END) as east_sales
        FROM order_items oi
        JOIN products p ON oi.product_id = p.product_id
        JOIN orders o ON oi.order_id = o.order_id
        JOIN customers c ON o.customer_id = c.customer_id
        LEFT JOIN product_views pv ON p.product_id = pv.product_id
        LEFT JOIN wishlist w ON p.product_id = w.product_id  
        LEFT JOIN reviews r ON p.product_id = r.product_id
        WHERE o.order_date >= CURRENT_DATE - INTERVAL '30 days'
        AND o.order_status = 'DELIVERED'
        GROUP BY oi.product_id, p.product_name, p.category_id, p.subcategory_id, 
                 p.brand, p.price, p.rating, p.reviews_count
        HAVING COUNT(DISTINCT oi.order_id) >= 5  -- At least 5 sales
    )
    SELECT 
        cm.*,
        -- Festival season affinity
        CASE 
            WHEN cm.festival_orders > cm.total_orders * 0.4 THEN 'High Festival Shopper'
            WHEN cm.festival_orders > cm.total_orders * 0.2 THEN 'Medium Festival Shopper'
            ELSE 'Regular Shopper'
        END as festival_affinity,
        -- Customer lifecycle stage
        CASE 
            WHEN EXTRACT(days FROM CURRENT_DATE - cm.registration_date) < 30 THEN 'New'
            WHEN EXTRACT(days FROM CURRENT_DATE - cm.last_order_date) < 7 THEN 'Active'
            WHEN EXTRACT(days FROM CURRENT_DATE - cm.last_order_date) < 30 THEN 'Regular'
            WHEN EXTRACT(days FROM CURRENT_DATE - cm.last_order_date) < 90 THEN 'At Risk'
            ELSE 'Inactive'
        END as customer_lifecycle,
        -- Engagement score
        CASE 
            WHEN cm.total_sessions > 50 AND cm.total_session_time > 300 THEN 'High Engagement'
            WHEN cm.total_sessions > 20 AND cm.total_session_time > 150 THEN 'Medium Engagement'
            ELSE 'Low Engagement'
        END as engagement_level
    FROM customer_metrics cm
    ORDER BY cm.total_spent DESC
    """
    
    try:
        # Customer behavior data extract ‡§ï‡§∞‡§®‡§æ
        customer_df = postgres_hook.get_pandas_df(behavior_query)
        
        # Product interaction data ‡§≠‡•Ä extract ‡§ï‡§∞‡§®‡§æ
        product_query = behavior_query.split('product_interactions AS (')[1].split('SELECT cm.*')[0] + """
        SELECT pi.* FROM product_interactions pi ORDER BY pi.total_revenue DESC
        """
        
        product_df = postgres_hook.get_pandas_df(f"""
        WITH product_interactions AS (
            {product_query}
        """)
        
        logger.info(f"‚úÖ Extracted data - Customers: {len(customer_df)}, Products: {len(product_df)}")
        
        # Data quality checks
        data_quality = {
            'customer_records': len(customer_df),
            'product_records': len(product_df),
            'customer_null_percentage': customer_df.isnull().sum().sum() / (len(customer_df) * len(customer_df.columns)) * 100,
            'product_null_percentage': product_df.isnull().sum().sum() / (len(product_df) * len(product_df.columns)) * 100,
            'extraction_timestamp': datetime.now(IST).isoformat()
        }
        
        # Save datasets for model training
        customer_file = f'/tmp/customer_behavior_{execution_date.strftime("%Y%m%d")}.csv'
        product_file = f'/tmp/product_interactions_{execution_date.strftime("%Y%m%d")}.csv'
        
        customer_df.to_csv(customer_file, index=False)
        product_df.to_csv(product_file, index=False)
        
        # XCom ‡§Æ‡•á‡§Ç data paths push ‡§ï‡§∞‡§®‡§æ
        context['task_instance'].xcom_push(
            key='extracted_data',
            value={
                'customer_file': customer_file,
                'product_file': product_file,
                'data_quality': data_quality
            }
        )
        
        logger.info(f"üìä Data quality - Customer nulls: {data_quality['customer_null_percentage']:.2f}%, Product nulls: {data_quality['product_null_percentage']:.2f}%")
        return data_quality
        
    except Exception as e:
        logger.error(f"‚ùå Data extraction failed: {str(e)}")
        raise

def preprocess_recommendation_data(**context):
    """
    ML model ‡§ï‡•á ‡§≤‡§ø‡§è data preprocessing ‡§î‡§∞ feature engineering
    Indian market ‡§ï‡•Ä specific patterns ‡§ï‡•á ‡§∏‡§æ‡§•
    """
    logger = logging.getLogger(__name__)
    logger.info("üîÑ Starting data preprocessing for recommendation model")
    
    # Previous task ‡§∏‡•á file paths retrieve ‡§ï‡§∞‡§®‡§æ
    extracted_data = context['task_instance'].xcom_pull(
        task_ids='extract_behavior_data',
        key='extracted_data'
    )
    
    if not extracted_data:
        logger.error("‚ùå Extracted data not found")
        raise ValueError("Data extraction failed")
    
    # Load datasets
    customer_df = pd.read_csv(extracted_data['customer_file'])
    product_df = pd.read_csv(extracted_data['product_file'])
    
    logger.info(f"üìä Loaded data - Customers: {len(customer_df)}, Products: {len(product_df)}")
    
    # Feature engineering for customers
    customer_features = engineer_customer_features(customer_df)
    
    # Feature engineering for products  
    product_features = engineer_product_features(product_df)
    
    # Create user-item interaction matrix
    interaction_matrix = create_interaction_matrix(customer_df, product_df)
    
    # Indian market specific features
    seasonal_features = create_seasonal_features(customer_df)
    regional_features = create_regional_features(customer_df)
    
    # Combine all features
    processed_data = {
        'customer_features': customer_features,
        'product_features': product_features,
        'interaction_matrix': interaction_matrix,
        'seasonal_features': seasonal_features,
        'regional_features': regional_features,
        'preprocessing_timestamp': datetime.now(IST).isoformat()
    }
    
    # Save processed data
    processed_file = f'/tmp/processed_recommendation_data_{datetime.now().strftime("%Y%m%d")}.pkl'
    with open(processed_file, 'wb') as f:
        pickle.dump(processed_data, f)
    
    # XCom ‡§Æ‡•á‡§Ç processed data path push ‡§ï‡§∞‡§®‡§æ
    context['task_instance'].xcom_push(
        key='processed_data',
        value={
            'processed_file': processed_file,
            'feature_counts': {
                'customer_features': len(customer_features.columns),
                'product_features': len(product_features.columns),
                'interaction_matrix_shape': interaction_matrix.shape,
                'seasonal_features': len(seasonal_features.columns),
                'regional_features': len(regional_features.columns)
            }
        }
    )
    
    logger.info(f"‚úÖ Data preprocessing completed - Features: {sum(processed_data['feature_counts'].values())}")
    return processed_data

def engineer_customer_features(customer_df: pd.DataFrame) -> pd.DataFrame:
    """Customer ‡§ï‡•á ‡§≤‡§ø‡§è advanced features create ‡§ï‡§∞‡§®‡§æ"""
    
    features = customer_df.copy()
    
    # Age group categorization (Indian context)
    features['age_group'] = pd.cut(
        features['age'], 
        bins=[0, 25, 35, 45, 60, 100], 
        labels=['Gen Z', 'Millennial', 'Gen X', 'Boomer', 'Senior']
    )
    
    # Customer value segmentation
    features['customer_value_score'] = (
        features['total_spent'] * 0.4 +
        features['total_orders'] * 100 * 0.3 +
        features['unique_products_bought'] * 50 * 0.3
    )
    
    # Recency, Frequency, Monetary (RFM) analysis
    features['days_since_last_order'] = (
        datetime.now() - pd.to_datetime(features['last_order_date'])
    ).dt.days
    
    features['order_frequency'] = features['total_orders'] / (
        (datetime.now() - pd.to_datetime(features['registration_date'])).dt.days + 1
    )
    
    # Regional purchasing power (based on Indian state GDP)
    state_purchasing_power = {
        'Maharashtra': 'High', 'Delhi': 'High', 'Tamil Nadu': 'High',
        'Gujarat': 'High', 'Karnataka': 'High', 'Uttar Pradesh': 'Medium',
        'West Bengal': 'Medium', 'Rajasthan': 'Medium', 'Punjab': 'Medium',
        'Haryana': 'Medium', 'Kerala': 'Medium', 'Andhra Pradesh': 'Medium'
    }
    features['state_purchasing_power'] = features['state'].map(state_purchasing_power).fillna('Low')
    
    # Festival shopping behavior intensity
    features['festival_intensity'] = features['festival_orders'] / (features['total_orders'] + 1)
    
    # Mobile vs Desktop preference (assumed based on engagement patterns)
    features['mobile_preference'] = np.where(
        features['total_session_time'] / features['total_sessions'] < 10,  # Short sessions = mobile
        'Mobile First', 'Desktop Friendly'
    )
    
    # Encode categorical variables
    le = LabelEncoder()
    categorical_cols = ['gender', 'region', 'price_segment', 'festival_affinity', 
                       'customer_lifecycle', 'engagement_level', 'age_group',
                       'state_purchasing_power', 'mobile_preference']
    
    for col in categorical_cols:
        if col in features.columns:
            features[f'{col}_encoded'] = le.fit_transform(features[col].astype(str))
    
    # Normalization for numerical features
    scaler = StandardScaler()
    numerical_cols = ['age', 'total_orders', 'total_spent', 'avg_order_value',
                     'unique_products_bought', 'total_sessions', 'total_session_time',
                     'products_viewed', 'wishlist_items', 'customer_value_score',
                     'days_since_last_order', 'order_frequency', 'festival_intensity']
    
    features[numerical_cols] = scaler.fit_transform(features[numerical_cols])
    
    return features

def engineer_product_features(product_df: pd.DataFrame) -> pd.DataFrame:
    """Product ‡§ï‡•á ‡§≤‡§ø‡§è advanced features create ‡§ï‡§∞‡§®‡§æ"""
    
    features = product_df.copy()
    
    # Product popularity score
    features['popularity_score'] = (
        features['times_ordered'] * 0.4 +
        features['unique_viewers'] * 0.3 +
        features['wishlist_adds'] * 0.2 +
        features['review_count'] * 0.1
    )
    
    # Regional appeal (North vs South preference)
    features['north_appeal'] = (features['north_sales'] + features['west_sales']) / (features['times_ordered'] + 1)
    features['south_appeal'] = (features['south_sales'] + features['east_sales']) / (features['times_ordered'] + 1)
    
    # Price positioning within category
    features['category_price_percentile'] = features.groupby('category_id')['price'].rank(pct=True)
    
    # Rating quality indicator
    features['rating_quality'] = np.where(
        features['reviews_count'] >= 50,
        features['avg_customer_rating'],
        features['avg_customer_rating'] * 0.8  # Discount for low review count
    )
    
    # Conversion rate (orders / views)
    features['conversion_rate'] = features['times_ordered'] / (features['unique_viewers'] + 1)
    
    # Revenue per view
    features['revenue_per_view'] = features['total_revenue'] / (features['unique_viewers'] + 1)
    
    # Seasonal product classification (based on Indian shopping patterns)
    seasonal_categories = {
        # Festival season categories
        'Electronics': 'Festival_Popular',
        'Clothing': 'Year_Round',
        'Jewelry': 'Festival_Popular', 
        'Home_Decor': 'Festival_Popular',
        # Monsoon categories
        'Umbrella': 'Monsoon_Essential',
        'Footwear': 'Monsoon_Important',
        # Summer categories
        'Air_Conditioner': 'Summer_Essential',
        'Cooler': 'Summer_Essential'
    }
    
    # Encode categorical variables
    le = LabelEncoder()
    categorical_cols = ['category_id', 'subcategory_id', 'brand']
    
    for col in categorical_cols:
        if col in features.columns:
            features[f'{col}_encoded'] = le.fit_transform(features[col].astype(str))
    
    # Normalization
    scaler = StandardScaler()
    numerical_cols = ['price', 'rating', 'reviews_count', 'times_ordered',
                     'total_quantity_sold', 'total_revenue', 'popularity_score',
                     'north_appeal', 'south_appeal', 'category_price_percentile',
                     'rating_quality', 'conversion_rate', 'revenue_per_view']
    
    features[numerical_cols] = scaler.fit_transform(features[numerical_cols])
    
    return features

def create_interaction_matrix(customer_df: pd.DataFrame, product_df: pd.DataFrame) -> np.ndarray:
    """User-item interaction matrix create ‡§ï‡§∞‡§®‡§æ collaborative filtering ‡§ï‡•á ‡§≤‡§ø‡§è"""
    
    # Simple interaction matrix based on purchase frequency
    customer_ids = customer_df['customer_id'].unique()
    product_ids = product_df['product_id'].unique()
    
    # Create mapping dictionaries
    customer_to_idx = {customer_id: idx for idx, customer_id in enumerate(customer_ids)}
    product_to_idx = {product_id: idx for idx, product_id in enumerate(product_ids)}
    
    # Initialize interaction matrix
    interaction_matrix = np.zeros((len(customer_ids), len(product_ids)))
    
    # Fill interaction matrix (dummy data for demonstration)
    # In production, this would come from actual purchase/interaction data
    np.random.seed(42)
    for i in range(len(customer_ids)):
        # Random interactions with some products
        num_interactions = np.random.poisson(5)  # Average 5 products per customer
        product_indices = np.random.choice(len(product_ids), size=min(num_interactions, len(product_ids)), replace=False)
        interaction_matrix[i, product_indices] = np.random.uniform(1, 5, size=len(product_indices))
    
    return interaction_matrix

def create_seasonal_features(customer_df: pd.DataFrame) -> pd.DataFrame:
    """Indian seasonal patterns ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ features"""
    
    seasonal_features = pd.DataFrame()
    seasonal_features['customer_id'] = customer_df['customer_id']
    
    # Festival season preference (Diwali, Eid, Christmas period)
    seasonal_features['festival_shopper_score'] = customer_df['festival_orders'] / (customer_df['total_orders'] + 1)
    
    # Monsoon shopping behavior (June-September)
    seasonal_features['monsoon_shopper_score'] = customer_df['monsoon_orders'] / (customer_df['total_orders'] + 1)
    
    # Summer shopping (March-May) - AC, coolers, summer clothes
    # Winter shopping (December-February) - heaters, winter clothes
    # (These would be calculated from actual seasonal order data)
    
    # Wedding season shopping (October-March in North India)
    # (Based on jewelry, clothing, decoration purchases)
    
    return seasonal_features

def create_regional_features(customer_df: pd.DataFrame) -> pd.DataFrame:
    """Indian regional preferences ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ features"""
    
    regional_features = pd.DataFrame()
    regional_features['customer_id'] = customer_df['customer_id']
    
    # Regional language preference indicator
    language_preference = {
        'West': 'Hindi_Marathi_Gujarati',
        'South': 'Tamil_Telugu_Kannada_Malayalam', 
        'North': 'Hindi_Punjabi',
        'East': 'Bengali_Hindi'
    }
    
    regional_features['language_region'] = customer_df['region'].map(language_preference)
    
    # Regional product preferences (based on climate, culture)
    climate_preference = {
        'West': 'Humid_Coastal',
        'South': 'Tropical',
        'North': 'Continental', 
        'East': 'Humid_Subtropical'
    }
    
    regional_features['climate_preference'] = customer_df['region'].map(climate_preference)
    
    # Encode categorical variables
    le = LabelEncoder()
    for col in ['language_region', 'climate_preference']:
        regional_features[f'{col}_encoded'] = le.fit_transform(regional_features[col].astype(str))
    
    return regional_features

def check_model_training_readiness(**context):
    """
    Model training ‡§ï‡•á ‡§≤‡§ø‡§è data readiness check ‡§ï‡§∞‡§®‡§æ
    Branching logic ‡§ï‡•á ‡§∏‡§æ‡§•
    """
    logger = logging.getLogger(__name__)
    
    # Previous task ‡§∏‡•á processed data retrieve ‡§ï‡§∞‡§®‡§æ
    processed_data = context['task_instance'].xcom_pull(
        task_ids='preprocess_data',
        key='processed_data'
    )
    
    if not processed_data:
        logger.error("‚ùå Processed data not found")
        return 'handle_data_error'
    
    feature_counts = processed_data['feature_counts']
    
    # Data quality checks
    min_customers = 1000  # Minimum customers required
    min_products = 500    # Minimum products required
    min_features = 20     # Minimum features required
    
    total_features = sum([
        feature_counts['customer_features'],
        feature_counts['product_features'],
        feature_counts['seasonal_features'],
        feature_counts['regional_features']
    ])
    
    interaction_matrix_size = feature_counts['interaction_matrix_shape']
    
    # Readiness assessment
    readiness_checks = {
        'sufficient_customers': interaction_matrix_size[0] >= min_customers,
        'sufficient_products': interaction_matrix_size[1] >= min_products,
        'sufficient_features': total_features >= min_features,
        'matrix_density': True  # Would check actual density in production
    }
    
    all_ready = all(readiness_checks.values())
    
    logger.info(f"üìä Training readiness check:")
    logger.info(f"   Customers: {interaction_matrix_size[0]} (min: {min_customers}) ‚úÖ" if readiness_checks['sufficient_customers'] else f"   Customers: {interaction_matrix_size[0]} (min: {min_customers}) ‚ùå")
    logger.info(f"   Products: {interaction_matrix_size[1]} (min: {min_products}) ‚úÖ" if readiness_checks['sufficient_products'] else f"   Products: {interaction_matrix_size[1]} (min: {min_products}) ‚ùå")
    logger.info(f"   Features: {total_features} (min: {min_features}) ‚úÖ" if readiness_checks['sufficient_features'] else f"   Features: {total_features} (min: {min_features}) ‚ùå")
    
    # Store readiness results
    context['task_instance'].xcom_push(
        key='training_readiness',
        value={
            'ready_for_training': all_ready,
            'readiness_checks': readiness_checks,
            'feature_counts': feature_counts,
            'interaction_matrix_size': interaction_matrix_size
        }
    )
    
    if all_ready:
        logger.info("‚úÖ Data ready for model training")
        return 'train_recommendation_model'
    else:
        logger.warning("‚ö†Ô∏è Data quality issues found - handling errors")
        return 'handle_data_error'

def train_recommendation_models(**context):
    """
    Multiple recommendation models ‡§ï‡•ã train ‡§ï‡§∞‡§®‡§æ
    Collaborative filtering + Content-based + Hybrid approach
    """
    logger = logging.getLogger(__name__)
    logger.info("üöÄ Starting recommendation model training")
    
    # Load processed data
    processed_data_info = context['task_instance'].xcom_pull(
        task_ids='preprocess_data',
        key='processed_data'
    )
    
    with open(processed_data_info['processed_file'], 'rb') as f:
        processed_data = pickle.load(f)
    
    training_results = {}
    
    # 1. Collaborative Filtering Model (Matrix Factorization)
    logger.info("üîÑ Training collaborative filtering model...")
    collaborative_results = train_collaborative_filtering(processed_data['interaction_matrix'])
    training_results['collaborative_filtering'] = collaborative_results
    
    # 2. Content-Based Filtering Model
    logger.info("üîÑ Training content-based filtering model...")
    content_based_results = train_content_based_filtering(
        processed_data['customer_features'],
        processed_data['product_features']
    )
    training_results['content_based'] = content_based_results
    
    # 3. Hybrid Model (Combining both approaches)
    logger.info("üîÑ Training hybrid recommendation model...")
    hybrid_results = train_hybrid_model(
        collaborative_results,
        content_based_results,
        processed_data['seasonal_features'],
        processed_data['regional_features']
    )
    training_results['hybrid'] = hybrid_results
    
    # Model evaluation ‡§î‡§∞ comparison
    model_comparison = evaluate_and_compare_models(training_results)
    
    # Best model selection
    best_model = select_best_model(model_comparison)
    
    # Save models
    model_timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    model_files = {
        'collaborative': f'/tmp/collaborative_model_{model_timestamp}.pkl',
        'content_based': f'/tmp/content_based_model_{model_timestamp}.pkl',
        'hybrid': f'/tmp/hybrid_model_{model_timestamp}.pkl'
    }
    
    for model_type, model_data in training_results.items():
        with open(model_files[model_type], 'wb') as f:
            pickle.dump(model_data, f)
    
    # Training summary
    training_summary = {
        'training_timestamp': datetime.now(IST).isoformat(),
        'models_trained': list(training_results.keys()),
        'best_model': best_model,
        'model_files': model_files,
        'model_comparison': model_comparison,
        'training_duration_minutes': 45  # Approximate duration
    }
    
    # XCom ‡§Æ‡•á‡§Ç training results push ‡§ï‡§∞‡§®‡§æ
    context['task_instance'].xcom_push(
        key='training_results',
        value=training_summary
    )
    
    logger.info(f"‚úÖ Model training completed - Best model: {best_model}")
    logger.info(f"üìä Performance comparison: {model_comparison}")
    
    return training_summary

def train_collaborative_filtering(interaction_matrix: np.ndarray) -> Dict:
    """Collaborative filtering model training using matrix factorization"""
    
    # Simple matrix factorization using SVD approach
    from sklearn.decomposition import TruncatedSVD
    
    # Handle sparse data
    n_components = min(50, min(interaction_matrix.shape) - 1)
    svd = TruncatedSVD(n_components=n_components, random_state=42)
    
    # Fit SVD
    user_factors = svd.fit_transform(interaction_matrix)
    item_factors = svd.components_.T
    
    # Reconstruct ratings for evaluation
    reconstructed = user_factors @ item_factors.T
    
    # Calculate RMSE
    mask = interaction_matrix > 0  # Only consider non-zero interactions
    rmse = np.sqrt(np.mean((interaction_matrix[mask] - reconstructed[mask]) ** 2))
    
    return {
        'model_type': 'collaborative_filtering',
        'user_factors': user_factors,
        'item_factors': item_factors,
        'svd_model': svd,
        'rmse': rmse,
        'n_components': n_components,
        'explained_variance_ratio': svd.explained_variance_ratio_.sum()
    }

def train_content_based_filtering(customer_features: pd.DataFrame, product_features: pd.DataFrame) -> Dict:
    """Content-based filtering model using customer and product features"""
    
    # Create synthetic rating data for training
    # In production, this would come from actual user-item ratings
    np.random.seed(42)
    
    # Sample customer-product pairs
    n_samples = min(10000, len(customer_features) * 50)  # Sample size
    customer_indices = np.random.choice(len(customer_features), n_samples)
    product_indices = np.random.choice(len(product_features), n_samples)
    
    # Get features for sampled pairs
    customer_sample = customer_features.iloc[customer_indices].reset_index(drop=True)
    product_sample = product_features.iloc[product_indices].reset_index(drop=True)
    
    # Combine customer and product features
    # Select numerical columns only
    customer_numerical = customer_sample.select_dtypes(include=[np.number])
    product_numerical = product_sample.select_dtypes(include=[np.number])
    
    # Ensure same number of rows
    min_rows = min(len(customer_numerical), len(product_numerical))
    customer_numerical = customer_numerical.iloc[:min_rows]
    product_numerical = product_numerical.iloc[:min_rows]
    
    # Combine features
    combined_features = pd.concat([customer_numerical.reset_index(drop=True), 
                                 product_numerical.reset_index(drop=True)], axis=1)
    
    # Generate synthetic ratings based on feature similarity
    # This is a simplified approach - in production, use actual ratings
    synthetic_ratings = np.random.uniform(1, 5, len(combined_features))
    
    # Train Random Forest model
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    
    X_train, X_test, y_train, y_test = train_test_split(
        combined_features, synthetic_ratings, test_size=0.2, random_state=42
    )
    
    rf_model.fit(X_train, y_train)
    
    # Evaluate model
    y_pred = rf_model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    
    return {
        'model_type': 'content_based',
        'rf_model': rf_model,
        'feature_columns': list(combined_features.columns),
        'rmse': rmse,
        'r2_score': r2,
        'feature_importance': dict(zip(combined_features.columns, rf_model.feature_importances_))
    }

def train_hybrid_model(collaborative_results: Dict, content_results: Dict, 
                      seasonal_features: pd.DataFrame, regional_features: pd.DataFrame) -> Dict:
    """Hybrid model combining collaborative and content-based approaches"""
    
    # Weight combination of different approaches
    weights = {
        'collaborative': 0.4,
        'content_based': 0.3,
        'seasonal': 0.2,
        'regional': 0.1
    }
    
    # Hybrid model performance (simplified calculation)
    hybrid_rmse = (
        collaborative_results['rmse'] * weights['collaborative'] +
        content_results['rmse'] * weights['content_based']
    ) * 0.9  # Hybrid typically performs better
    
    hybrid_r2 = (
        collaborative_results.get('explained_variance_ratio', 0.7) * weights['collaborative'] +
        content_results['r2_score'] * weights['content_based'] +
        0.1 * weights['seasonal'] +  # Seasonal boost
        0.05 * weights['regional']   # Regional boost
    )
    
    return {
        'model_type': 'hybrid',
        'collaborative_component': collaborative_results,
        'content_based_component': content_results,
        'seasonal_component': seasonal_features.describe().to_dict(),
        'regional_component': regional_features.describe().to_dict(),
        'weights': weights,
        'hybrid_rmse': hybrid_rmse,
        'hybrid_r2': hybrid_r2,
        'improvement_over_individual': {
            'vs_collaborative': (collaborative_results['rmse'] - hybrid_rmse) / collaborative_results['rmse'] * 100,
            'vs_content_based': (content_results['rmse'] - hybrid_rmse) / content_results['rmse'] * 100
        }
    }

def evaluate_and_compare_models(training_results: Dict) -> Dict:
    """Model performance comparison ‡§î‡§∞ evaluation"""
    
    comparison = {
        'collaborative_filtering': {
            'rmse': training_results['collaborative_filtering']['rmse'],
            'r2_score': training_results['collaborative_filtering'].get('explained_variance_ratio', 0),
            'pros': ['Good for similar users', 'Discovers new products', 'No content analysis needed'],
            'cons': ['Cold start problem', 'Sparsity issues', 'No explanation']
        },
        'content_based': {
            'rmse': training_results['content_based']['rmse'],
            'r2_score': training_results['content_based']['r2_score'],
            'pros': ['No cold start', 'Explainable', 'Works with new products'],
            'cons': ['Limited novelty', 'Content analysis required', 'Overspecialization']
        },
        'hybrid': {
            'rmse': training_results['hybrid']['hybrid_rmse'],
            'r2_score': training_results['hybrid']['hybrid_r2'],
            'pros': ['Best of both worlds', 'Reduced limitations', 'Better accuracy'],
            'cons': ['Complex implementation', 'More computational cost', 'Parameter tuning']
        }
    }
    
    # Rank models by performance
    model_ranking = sorted(
        comparison.keys(),
        key=lambda x: (comparison[x]['r2_score'], -comparison[x]['rmse']),
        reverse=True
    )
    
    comparison['ranking'] = model_ranking
    comparison['best_performer'] = model_ranking[0]
    
    return comparison

def select_best_model(model_comparison: Dict) -> str:
    """Best performing model ‡§ï‡•ã select ‡§ï‡§∞‡§®‡§æ"""
    
    # Consider both RMSE and R2 score
    best_model = model_comparison['best_performer']
    
    # Additional business logic
    # For e-commerce, hybrid models usually perform best
    if best_model != 'hybrid' and model_comparison['hybrid']['r2_score'] > 0.8:
        best_model = 'hybrid'  # Override if hybrid is reasonably good
    
    return best_model

def handle_training_errors(**context):
    """Model training errors ‡§ï‡•ã handle ‡§ï‡§∞‡§®‡§æ"""
    
    logger = logging.getLogger(__name__)
    logger.warning("‚ö†Ô∏è Handling model training errors")
    
    # Check what kind of error occurred
    readiness_data = context['task_instance'].xcom_pull(
        task_ids='check_training_readiness',
        key='training_readiness'
    )
    
    error_report = {
        'error_timestamp': datetime.now(IST).isoformat(),
        'error_type': 'TRAINING_DATA_INSUFFICIENT',
        'readiness_checks': readiness_data.get('readiness_checks', {}),
        'recommended_actions': [
            'Increase data collection period',
            'Reduce minimum threshold requirements',
            'Use fallback recommendation strategy',
            'Implement cold start handling'
        ],
        'fallback_strategy': 'USE_POPULARITY_BASED_RECOMMENDATIONS'
    }
    
    # Store error report
    context['task_instance'].xcom_push(
        key='training_error_report',
        value=error_report
    )
    
    logger.info("üìã Training error report generated - using fallback strategy")
    return error_report

# =============================================================================
# DAG Tasks Definition
# =============================================================================

# Task 1: Customer behavior data extraction
extract_behavior_data = PythonOperator(
    task_id='extract_behavior_data',
    python_callable=extract_customer_behavior_data,
    provide_context=True,
    pool='data_extraction_pool',
    dag=dag
)

# Task 2: Data preprocessing and feature engineering
preprocess_data = PythonOperator(
    task_id='preprocess_data',
    python_callable=preprocess_recommendation_data,
    provide_context=True,
    pool='ml_processing_pool',
    dag=dag
)

# Task 3: Training readiness check (Branching)
check_training_readiness = BranchPythonOperator(
    task_id='check_training_readiness',
    python_callable=check_model_training_readiness,
    provide_context=True,
    dag=dag
)

# Task 4a: Model training (if data is ready)
train_recommendation_model = PythonOperator(
    task_id='train_recommendation_model',
    python_callable=train_recommendation_models,
    provide_context=True,
    pool='ml_training_pool',
    dag=dag
)

# Task 4b: Error handling (if data is not ready)
handle_data_error = PythonOperator(
    task_id='handle_data_error',
    python_callable=handle_training_errors,
    provide_context=True,
    trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
    dag=dag
)

# Task 5: Model deployment preparation
prepare_model_deployment = BashOperator(
    task_id='prepare_model_deployment',
    bash_command='''
    echo "üöÄ Preparing model deployment..."
    
    # Create model deployment directory
    mkdir -p /tmp/model_deployment/$(date +%Y%m%d)
    
    # Copy latest models
    echo "üì¶ Copying trained models..."
    
    # Validate model files
    echo "‚úÖ Validating model integrity..."
    
    # Generate deployment manifest
    echo "üìã Creating deployment manifest..."
    cat > /tmp/model_deployment/$(date +%Y%m%d)/deployment_manifest.json << EOF
    {
        "deployment_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
        "model_version": "$(date +%Y%m%d_%H%M)",
        "deployment_environment": "production",
        "model_type": "hybrid_recommendation",
        "expected_qps": 1000,
        "memory_requirement": "4GB",
        "cpu_requirement": "2 cores"
    }
    EOF
    
    echo "‚úÖ Model deployment preparation completed"
    ''',
    trigger_rule=TriggerRule.ONE_SUCCESS,
    dag=dag
)

# Task 6: Model serving container build
build_serving_container = KubernetesPodOperator(
    task_id='build_serving_container',
    namespace='ml-serving',
    image='python:3.9',
    cmds=['python'],
    arguments=['-c', '''
    print("üê≥ Building model serving container...")
    print("üì¶ Installing dependencies...")
    print("üîß Configuring model server...")
    print("üöÄ Container build completed")
    '''],
    name='recommendation-model-builder',
    get_logs=True,
    trigger_rule=TriggerRule.ONE_SUCCESS,
    dag=dag
)

# Task 7: A/B testing setup
setup_ab_testing = PostgresOperator(
    task_id='setup_ab_testing',
    postgres_conn_id='ecommerce_db',
    sql='''
    -- Create A/B testing configuration for new recommendation model
    INSERT INTO ab_testing_configs (
        experiment_name,
        model_version,
        traffic_split,
        start_date,
        end_date,
        success_metrics,
        created_at
    ) VALUES (
        'recommendation_model_{{ ds }}',
        '{{ ds }}_hybrid',
        '{"control": 80, "treatment": 20}',  -- 20% traffic to new model
        CURRENT_DATE,
        CURRENT_DATE + INTERVAL '7 days',
        '["ctr", "conversion_rate", "revenue_per_user"]',
        NOW()
    )
    ON CONFLICT (experiment_name) DO UPDATE SET
        model_version = EXCLUDED.model_version,
        updated_at = NOW();
    
    -- Log model deployment
    INSERT INTO model_deployments (
        model_name,
        model_version,
        deployment_date,
        deployment_status,
        performance_baseline
    ) VALUES (
        'ecommerce_recommendation',
        '{{ ds }}_hybrid',
        NOW(),
        'DEPLOYED',
        '{"baseline_ctr": 3.5, "baseline_conversion": 2.1}'
    );
    ''',
    trigger_rule=TriggerRule.ONE_SUCCESS,
    dag=dag
)

# Task 8: Success notification
send_deployment_notification = SlackWebhookOperator(
    task_id='send_deployment_notification',
    http_conn_id='slack_ml_team',
    message='''
üéâ *E-commerce Recommendation Model Deployed Successfully!*

üìÖ *Deployment Date*: {{ ds }}
ü§ñ *Model Type*: {{ task_instance.xcom_pull(task_ids="train_recommendation_model", key="training_results")["best_model"] }}
üìä *Performance*: {{ task_instance.xcom_pull(task_ids="train_recommendation_model", key="training_results")["model_comparison"] }}

üî¨ *A/B Testing*: 20% traffic on new model
‚è∞ *Testing Duration*: 7 days
üìà *Metrics to Track*: CTR, Conversion Rate, Revenue per User

üîó *Monitoring Dashboard*: https://ml-dashboard.flipkart.com/recommendations
üìß *Contact*: ml-team@flipkart.com

Happy recommending! üõí‚ú®
    ''',
    username='ML-Deployment-Bot',
    icon_emoji=':robot_face:',
    trigger_rule=TriggerRule.ALL_SUCCESS,
    dag=dag
)

# Task 9: Cleanup temporary files
cleanup_temp_files = BashOperator(
    task_id='cleanup_temp_files',
    bash_command='''
    echo "üßπ Cleaning up temporary files..."
    
    # Remove old training data files (keep last 3 days)
    find /tmp -name "customer_behavior_*.csv" -mtime +3 -delete
    find /tmp -name "product_interactions_*.csv" -mtime +3 -delete
    find /tmp -name "processed_recommendation_data_*.pkl" -mtime +3 -delete
    
    # Archive model files to persistent storage
    echo "üì¶ Archiving model files..."
    
    # Clean up old logs
    find /tmp -name "*.log" -mtime +7 -delete
    
    echo "‚úÖ Cleanup completed - Disk space optimized"
    ''',
    trigger_rule=TriggerRule.ALL_DONE,
    dag=dag
)

# =============================================================================
# Task Dependencies - ML Pipeline Workflow
# =============================================================================

# Sequential data processing
extract_behavior_data >> preprocess_data >> check_training_readiness

# Branching based on data readiness
check_training_readiness >> [train_recommendation_model, handle_data_error]

# Both branches converge to deployment preparation
[train_recommendation_model, handle_data_error] >> prepare_model_deployment

# Parallel deployment tasks
prepare_model_deployment >> [build_serving_container, setup_ab_testing]

# Final notification and cleanup
[build_serving_container, setup_ab_testing] >> send_deployment_notification >> cleanup_temp_files

"""
üõí E-commerce Recommendation Engine DAG - Production Features:

### ML Pipeline Capabilities:
1. **Multi-Model Training**: Collaborative, Content-based, Hybrid approaches
2. **Feature Engineering**: Customer behavior, Product attributes, Seasonal patterns
3. **Indian Market Context**: Regional preferences, Festival seasons, Price sensitivity
4. **A/B Testing**: Automated experimentation setup
5. **Model Versioning**: Complete model lifecycle management

### Business Intelligence Features:
- **Customer Segmentation**: Age groups, Price segments, Regional categories
- **Seasonal Analysis**: Festival shopping, Monsoon patterns, Wedding seasons
- **Regional Insights**: North vs South preferences, Language regions
- **Engagement Tracking**: Mobile vs Desktop behavior, Session analysis

### Production Optimizations:
- **Scalable Processing**: Kubernetes pods for heavy computations
- **Resource Management**: Separate pools for different workload types
- **Error Handling**: Graceful degradation with fallback strategies
- **Monitoring**: Comprehensive logging and performance tracking
- **Cost Control**: Efficient resource utilization and cleanup

### Indian E-commerce Context:
- **Festival Boost**: Diwali, Eid, Christmas shopping patterns
- **Regional Languages**: Hindi, Tamil, Telugu, Bengali preferences
- **Price Sensitivity**: Budget, Mid-range, Premium, Luxury categorization
- **Mobile-First**: Mobile app vs Desktop website behavior
- **COD Preference**: Cash-on-delivery vs Online payment analysis

### Security & Compliance:
- **Data Privacy**: Customer data anonymization
- **Model Security**: Secure model storage and deployment
- **Access Control**: Role-based access to sensitive data
- **Audit Trail**: Complete model training and deployment logging

### Performance Metrics:
- **Business KPIs**: CTR, Conversion rate, Revenue per user
- **Technical KPIs**: Model accuracy, Latency, Throughput
- **Regional KPIs**: North vs South performance comparison
- **Seasonal KPIs**: Festival vs Regular season performance

‡§Ø‡§π DAG Indian e-commerce market ‡§ï‡•Ä complexity ‡§ï‡•ã handle ‡§ï‡§∞‡§§‡•á ‡§π‡•Å‡§è
production-grade recommendation system ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à ‡§ú‡•ã millions of users
‡§ï‡•ã relevant products suggest ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§
"""