#!/usr/bin/env python3
"""
Episode 13: CDC & Real-Time Data Pipelines
Example 3: PostgreSQL Logical Replication for CDC

‡§Ø‡§π example PostgreSQL ‡§ï‡•á logical replication ‡§ï‡§æ use ‡§ï‡§∞‡§ï‡•á CDC implement ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
Zerodha, Groww ‡§ú‡•à‡§∏‡•á financial platforms ‡§ï‡•á ‡§≤‡§ø‡§è real-time trading data‡•§

Author: Distributed Systems Podcast Team
Context: Indian fintech platforms - stock trading, mutual funds
"""

import asyncio
import asyncpg
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, AsyncGenerator
import uuid
from dataclasses import dataclass, asdict
from decimal import Decimal
import psycopg2
from psycopg2.extras import LogicalReplicationConnection, ReplicationMessage
import threading
import queue
from concurrent.futures import ThreadPoolExecutor
import time

# Hindi logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s [%(thread)d]',
    handlers=[
        logging.FileHandler('postgres_cdc.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class StockTrade:
    """Indian stock market trade representation"""
    trade_id: str
    symbol: str  # RELIANCE, TCS, INFY
    exchange: str  # NSE, BSE
    trade_type: str  # BUY, SELL
    quantity: int
    price: Decimal
    user_id: str
    broker_id: str
    timestamp: datetime
    settlement_date: datetime
    brokerage: Decimal
    taxes: Decimal
    
@dataclass
class MutualFundTransaction:
    """Mutual fund transaction - Zerodha Coin, Groww style"""
    transaction_id: str
    fund_code: str  # HDFC_TOP_100, SBI_BLUE_CHIP
    amc: str  # HDFC, SBI, ICICI
    transaction_type: str  # BUY, SELL, SIP
    units: Decimal
    nav: Decimal
    amount: Decimal
    user_id: str
    folio_number: str
    timestamp: datetime
    sip_id: Optional[str] = None

class PostgreSQLReplicationSlotManager:
    """
    PostgreSQL replication slot management for Indian fintech
    Mumbai stock exchange ‡§ï‡•á real-time data ‡§ï‡•á ‡§≤‡§ø‡§è optimized
    """
    
    def __init__(self, connection_params: Dict[str, str]):
        self.connection_params = connection_params
        self.connection = None
        self.slot_name = "zerodha_fintech_slot"
        self.publication_name = "zerodha_fintech_publication"
        
    async def create_publication_and_slot(self):
        """
        Publication ‡§î‡§∞ replication slot ‡§¨‡§®‡§æ‡§ì
        """
        logger.info("üè¶ Creating PostgreSQL publication and replication slot")
        
        try:
            # Regular connection for DDL operations
            conn = await asyncpg.connect(**self.connection_params)
            
            # Create publication for fintech tables
            publication_sql = f"""
            CREATE PUBLICATION IF NOT EXISTS {self.publication_name}
            FOR TABLE 
                stock_trades,
                mutual_fund_transactions,
                user_portfolios,
                sip_schedules;
            """
            await conn.execute(publication_sql)
            logger.info(f"‚úÖ Publication {self.publication_name} created")
            
            await conn.close()
            
            # Create replication slot using psycopg2 (asyncpg doesn't support replication)
            sync_conn = psycopg2.connect(
                host=self.connection_params['host'],
                port=self.connection_params['port'],
                user=self.connection_params['user'],
                password=self.connection_params['password'],
                database=self.connection_params['database'],
                connection_factory=LogicalReplicationConnection
            )
            
            cursor = sync_conn.cursor()
            
            try:
                # Create logical replication slot
                cursor.create_replication_slot(
                    self.slot_name,
                    output_plugin='pgoutput'
                )
                logger.info(f"‚úÖ Replication slot {self.slot_name} created")
            except psycopg2.errors.DuplicateObject:
                logger.info(f"‚ÑπÔ∏è Replication slot {self.slot_name} already exists")
            
            cursor.close()
            sync_conn.close()
            
        except Exception as e:
            logger.error(f"üí• Failed to create publication/slot: {str(e)}")
            raise
    
    async def drop_publication_and_slot(self):
        """
        Publication ‡§î‡§∞ slot ‡§ï‡•ã cleanup ‡§ï‡§∞‡•ã
        """
        logger.info("üßπ Cleaning up publication and replication slot")
        
        try:
            conn = await asyncpg.connect(**self.connection_params)
            await conn.execute(f"DROP PUBLICATION IF EXISTS {self.publication_name}")
            logger.info(f"‚úÖ Publication {self.publication_name} dropped")
            await conn.close()
            
            # Drop replication slot
            sync_conn = psycopg2.connect(
                host=self.connection_params['host'],
                port=self.connection_params['port'],
                user=self.connection_params['user'],
                password=self.connection_params['password'],
                database=self.connection_params['database'],
                connection_factory=LogicalReplicationConnection
            )
            
            cursor = sync_conn.cursor()
            
            try:
                cursor.drop_replication_slot(self.slot_name)
                logger.info(f"‚úÖ Replication slot {self.slot_name} dropped")
            except psycopg2.errors.InvalidName:
                logger.info(f"‚ÑπÔ∏è Replication slot {self.slot_name} doesn't exist")
            
            cursor.close()
            sync_conn.close()
            
        except Exception as e:
            logger.error(f"üí• Cleanup failed: {str(e)}")

class ZerodhaFintechCDCProcessor:
    """
    Zerodha style fintech CDC processor with real-time stock data
    """
    
    def __init__(self, connection_params: Dict[str, str]):
        self.connection_params = connection_params
        self.slot_manager = PostgreSQLReplicationSlotManager(connection_params)
        self.change_queue = queue.Queue()
        self.processors = []
        self.running = False
        
        # Indian stock market data
        self.nse_stocks = [
            "RELIANCE", "TCS", "HDFCBANK", "INFY", "HINDUNILVR",
            "ICICIBANK", "SBIN", "BHARTIARTL", "ITC", "LT",
            "KOTAKBANK", "AXISBANK", "WIPRO", "ASIANPAINT", "MARUTI"
        ]
        
        self.mutual_funds = [
            {"code": "HDFC_TOP_100", "amc": "HDFC", "category": "Large Cap"},
            {"code": "SBI_BLUE_CHIP", "amc": "SBI", "category": "Large Cap"},
            {"code": "ICICI_FOCUSED", "amc": "ICICI", "category": "Multi Cap"},
            {"code": "AXIS_MIDCAP", "amc": "Axis", "category": "Mid Cap"},
            {"code": "DSP_SMALL_CAP", "amc": "DSP", "category": "Small Cap"}
        ]
    
    async def setup_tables(self):
        """
        Fintech tables setup - Indian trading platform style
        """
        logger.info("üèóÔ∏è Setting up fintech database tables")
        
        conn = await asyncpg.connect(**self.connection_params)
        
        try:
            # Stock trades table - NSE/BSE trading
            stock_trades_sql = """
            CREATE TABLE IF NOT EXISTS stock_trades (
                trade_id VARCHAR(50) PRIMARY KEY,
                symbol VARCHAR(20) NOT NULL,
                exchange VARCHAR(10) NOT NULL CHECK (exchange IN ('NSE', 'BSE')),
                trade_type VARCHAR(10) NOT NULL CHECK (trade_type IN ('BUY', 'SELL')),
                quantity INTEGER NOT NULL CHECK (quantity > 0),
                price DECIMAL(12,4) NOT NULL CHECK (price > 0),
                user_id VARCHAR(50) NOT NULL,
                broker_id VARCHAR(20) NOT NULL,
                trade_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                settlement_date DATE NOT NULL,
                brokerage DECIMAL(10,2) DEFAULT 0,
                taxes DECIMAL(10,2) DEFAULT 0,
                status VARCHAR(20) DEFAULT 'PENDING' CHECK (status IN ('PENDING', 'EXECUTED', 'CANCELLED')),
                
                INDEX (symbol, exchange),
                INDEX (user_id, trade_timestamp),
                INDEX (trade_timestamp),
                INDEX (status)
            );
            """
            await conn.execute(stock_trades_sql)
            
            # Mutual fund transactions table
            mf_transactions_sql = """
            CREATE TABLE IF NOT EXISTS mutual_fund_transactions (
                transaction_id VARCHAR(50) PRIMARY KEY,
                fund_code VARCHAR(50) NOT NULL,
                amc VARCHAR(50) NOT NULL,
                transaction_type VARCHAR(10) NOT NULL CHECK (transaction_type IN ('BUY', 'SELL', 'SIP')),
                units DECIMAL(15,6) NOT NULL,
                nav DECIMAL(10,4) NOT NULL CHECK (nav > 0),
                amount DECIMAL(12,2) NOT NULL CHECK (amount > 0),
                user_id VARCHAR(50) NOT NULL,
                folio_number VARCHAR(50) NOT NULL,
                transaction_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                sip_id VARCHAR(50),
                status VARCHAR(20) DEFAULT 'PENDING' CHECK (status IN ('PENDING', 'CONFIRMED', 'REJECTED')),
                
                INDEX (user_id, transaction_timestamp),
                INDEX (fund_code, amc),
                INDEX (sip_id),
                INDEX (status)
            );
            """
            await conn.execute(mf_transactions_sql)
            
            # User portfolios table - real-time portfolio tracking
            portfolios_sql = """
            CREATE TABLE IF NOT EXISTS user_portfolios (
                portfolio_id VARCHAR(50) PRIMARY KEY,
                user_id VARCHAR(50) NOT NULL,
                symbol VARCHAR(20),
                fund_code VARCHAR(50),
                asset_type VARCHAR(10) NOT NULL CHECK (asset_type IN ('STOCK', 'MF')),
                quantity DECIMAL(15,6) NOT NULL,
                avg_price DECIMAL(12,4) NOT NULL,
                current_value DECIMAL(15,2),
                last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                
                UNIQUE(user_id, symbol, fund_code),
                INDEX (user_id, asset_type),
                INDEX (symbol),
                INDEX (fund_code)
            );
            """
            await conn.execute(portfolios_sql)
            
            # SIP schedules - systematic investment plans
            sip_schedules_sql = """
            CREATE TABLE IF NOT EXISTS sip_schedules (
                sip_id VARCHAR(50) PRIMARY KEY,
                user_id VARCHAR(50) NOT NULL,
                fund_code VARCHAR(50) NOT NULL,
                amount DECIMAL(10,2) NOT NULL CHECK (amount > 0),
                frequency VARCHAR(20) NOT NULL CHECK (frequency IN ('MONTHLY', 'WEEKLY')),
                start_date DATE NOT NULL,
                end_date DATE,
                next_execution_date DATE NOT NULL,
                status VARCHAR(20) DEFAULT 'ACTIVE' CHECK (status IN ('ACTIVE', 'PAUSED', 'STOPPED')),
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                
                INDEX (user_id),
                INDEX (next_execution_date),
                INDEX (status)
            );
            """
            await conn.execute(sip_schedules_sql)
            
            logger.info("‚úÖ All fintech tables created successfully")
            
        except Exception as e:
            logger.error(f"üí• Table creation failed: {str(e)}")
            raise
        finally:
            await conn.close()
    
    def start_replication_stream(self):
        """
        PostgreSQL logical replication stream ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•ã
        """
        logger.info("üì° Starting PostgreSQL logical replication stream")
        
        def replication_worker():
            try:
                conn = psycopg2.connect(
                    host=self.connection_params['host'],
                    port=self.connection_params['port'],
                    user=self.connection_params['user'],
                    password=self.connection_params['password'],
                    database=self.connection_params['database'],
                    connection_factory=LogicalReplicationConnection
                )
                
                cursor = conn.cursor()
                
                # Start consuming logical replication stream
                cursor.start_replication(
                    slot_name=self.slot_manager.slot_name,
                    options={
                        'publication_names': self.slot_manager.publication_name,
                        'proto_version': '1'
                    }
                )
                
                logger.info("‚úÖ Replication stream started successfully")
                
                # Process messages
                while self.running:
                    try:
                        msg = cursor.read_message()
                        if msg:
                            self.process_replication_message(msg)
                            msg.cursor.send_feedback(flush_lsn=msg.data_start)
                        else:
                            time.sleep(0.1)  # Brief pause if no messages
                            
                    except Exception as e:
                        logger.error(f"üí• Message processing error: {str(e)}")
                        time.sleep(1)
                
            except Exception as e:
                logger.error(f"üí• Replication stream error: {str(e)}")
            finally:
                if 'cursor' in locals():
                    cursor.close()
                if 'conn' in locals():
                    conn.close()
        
        # Start replication in background thread
        replication_thread = threading.Thread(target=replication_worker, daemon=True)
        replication_thread.start()
        logger.info("üöÄ Replication worker started")
        
        return replication_thread
    
    def process_replication_message(self, msg: ReplicationMessage):
        """
        Replication message ‡§ï‡•ã process ‡§ï‡§∞‡•ã - Mumbai trading floor ‡§ï‡•Ä speed ‡§Æ‡•á‡§Ç
        """
        try:
            # Parse WAL message (this is a simplified version)
            # In production, you'd use proper WAL parsing
            change_data = {
                'lsn': str(msg.data_start),
                'timestamp': datetime.now().isoformat(),
                'payload': msg.payload.decode('utf-8') if msg.payload else None
            }
            
            # Add to processing queue
            self.change_queue.put(change_data)
            
            # Log for monitoring
            if self.change_queue.qsize() % 100 == 0:
                logger.info(f"üìä Queue size: {self.change_queue.qsize()}")
                
        except Exception as e:
            logger.error(f"üí• Message parsing error: {str(e)}")
    
    def start_change_processors(self, num_processors: int = 3):
        """
        Multiple change processors ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•ã - parallel processing
        """
        logger.info(f"üîÑ Starting {num_processors} change processors")
        
        def change_processor(processor_id: int):
            logger.info(f"üèÉ Change processor {processor_id} started")
            
            while self.running:
                try:
                    # Get change from queue (with timeout)
                    try:
                        change = self.change_queue.get(timeout=1)
                    except queue.Empty:
                        continue
                    
                    # Process the change
                    self.process_change(change, processor_id)
                    self.change_queue.task_done()
                    
                except Exception as e:
                    logger.error(f"üí• Processor {processor_id} error: {str(e)}")
                    time.sleep(1)
            
            logger.info(f"üõë Change processor {processor_id} stopped")
        
        # Start processor threads
        for i in range(num_processors):
            processor = threading.Thread(target=change_processor, args=(i,), daemon=True)
            processor.start()
            self.processors.append(processor)
        
        logger.info(f"‚úÖ All {num_processors} processors started")
    
    def process_change(self, change: Dict[str, Any], processor_id: int):
        """
        Individual change ‡§ï‡•ã process ‡§ï‡§∞‡•ã
        """
        try:
            # ‡§Ø‡§π‡§æ‡§Å actual CDC logic ‡§π‡•ã‡§ó‡•Ä
            # For demo, we'll just log the change
            
            if change['payload']:
                # Parse change type (INSERT, UPDATE, DELETE)
                # Route to appropriate handler
                
                # Stock trade changes
                if 'stock_trades' in str(change['payload']):
                    self.handle_stock_trade_change(change, processor_id)
                
                # Mutual fund changes
                elif 'mutual_fund_transactions' in str(change['payload']):
                    self.handle_mf_transaction_change(change, processor_id)
                
                # Portfolio updates
                elif 'user_portfolios' in str(change['payload']):
                    self.handle_portfolio_change(change, processor_id)
                
                # SIP changes
                elif 'sip_schedules' in str(change['payload']):
                    self.handle_sip_change(change, processor_id)
            
        except Exception as e:
            logger.error(f"üí• Change processing error (P{processor_id}): {str(e)}")
    
    def handle_stock_trade_change(self, change: Dict[str, Any], processor_id: int):
        """
        Stock trade changes handle ‡§ï‡§∞‡•ã - NSE/BSE real-time processing
        """
        logger.info(f"üìà Processing stock trade change (P{processor_id})")
        
        # Real-time portfolio update
        # Risk management checks
        # Compliance reporting
        # Tax calculations
        
        # Send to downstream systems:
        # - Real-time P&L calculation
        # - Risk monitoring systems
        # - Compliance reporting
        # - Tax calculation engines
        
        self.send_to_risk_engine(change)
        self.update_real_time_pnl(change)
    
    def handle_mf_transaction_change(self, change: Dict[str, Any], processor_id: int):
        """
        Mutual fund transaction changes handle ‡§ï‡§∞‡•ã
        """
        logger.info(f"üè¶ Processing MF transaction change (P{processor_id})")
        
        # SIP processing
        # NAV validation
        # Portfolio rebalancing
        # Goal tracking updates
        
        self.update_sip_tracking(change)
        self.trigger_goal_recalculation(change)
    
    def handle_portfolio_change(self, change: Dict[str, Any], processor_id: int):
        """
        Portfolio changes handle ‡§ï‡§∞‡•ã - real-time wealth tracking
        """
        logger.info(f"üíº Processing portfolio change (P{processor_id})")
        
        # Real-time portfolio valuation
        # Asset allocation updates
        # Performance analytics
        # Alerts for significant changes
        
        self.calculate_portfolio_metrics(change)
        self.check_rebalancing_alerts(change)
    
    def handle_sip_change(self, change: Dict[str, Any], processor_id: int):
        """
        SIP changes handle ‡§ï‡§∞‡•ã - systematic investment processing
        """
        logger.info(f"üîÑ Processing SIP change (P{processor_id})")
        
        # SIP execution scheduling
        # Payment gateway integration
        # Failure handling
        # Investor notifications
        
        self.schedule_sip_execution(change)
        self.send_sip_notification(change)
    
    def send_to_risk_engine(self, change: Dict[str, Any]):
        """Risk engine ‡§ï‡•ã data ‡§≠‡•á‡§ú‡•ã - real-time risk monitoring"""
        # Production ‡§Æ‡•á‡§Ç ‡§Ø‡§π actual risk management system ‡§ï‡•ã call ‡§ï‡§∞‡•á‡§ó‡§æ
        logger.info("‚ö†Ô∏è Sending data to risk engine")
    
    def update_real_time_pnl(self, change: Dict[str, Any]):
        """Real-time P&L calculation update ‡§ï‡§∞‡•ã"""
        logger.info("üí∞ Updating real-time P&L calculations")
    
    def update_sip_tracking(self, change: Dict[str, Any]):
        """SIP tracking system update ‡§ï‡§∞‡•ã"""
        logger.info("üìÖ Updating SIP tracking systems")
    
    def trigger_goal_recalculation(self, change: Dict[str, Any]):
        """Financial goals recalculation trigger ‡§ï‡§∞‡•ã"""
        logger.info("üéØ Triggering goal recalculation")
    
    def calculate_portfolio_metrics(self, change: Dict[str, Any]):
        """Portfolio metrics calculate ‡§ï‡§∞‡•ã"""
        logger.info("üìä Calculating portfolio metrics")
    
    def check_rebalancing_alerts(self, change: Dict[str, Any]):
        """Rebalancing alerts check ‡§ï‡§∞‡•ã"""
        logger.info("‚öñÔ∏è Checking rebalancing alerts")
    
    def schedule_sip_execution(self, change: Dict[str, Any]):
        """SIP execution schedule ‡§ï‡§∞‡•ã"""
        logger.info("‚è∞ Scheduling SIP execution")
    
    def send_sip_notification(self, change: Dict[str, Any]):
        """SIP notification ‡§≠‡•á‡§ú‡•ã"""
        logger.info("üì± Sending SIP notification")
    
    async def start(self):
        """
        Complete CDC system start ‡§ï‡§∞‡•ã
        """
        logger.info("üöÄ Starting Zerodha Fintech CDC System")
        
        try:
            # Setup publication and slot
            await self.slot_manager.create_publication_and_slot()
            
            # Setup tables
            await self.setup_tables()
            
            # Generate sample data
            await self.generate_sample_data()
            
            # Start processing
            self.running = True
            
            # Start change processors
            self.start_change_processors(num_processors=3)
            
            # Start replication stream
            replication_thread = self.start_replication_stream()
            
            logger.info("‚úÖ CDC System started successfully")
            
            # Keep running
            while self.running:
                await asyncio.sleep(10)
                logger.info(f"üìä Queue size: {self.change_queue.qsize()}")
            
        except Exception as e:
            logger.error(f"üí• CDC System startup failed: {str(e)}")
            raise
    
    async def stop(self):
        """
        CDC system ‡§ï‡•ã gracefully stop ‡§ï‡§∞‡•ã
        """
        logger.info("üõë Stopping CDC System")
        
        self.running = False
        
        # Wait for processors to finish
        for processor in self.processors:
            processor.join(timeout=5)
        
        # Cleanup
        await self.slot_manager.drop_publication_and_slot()
        
        logger.info("‚úÖ CDC System stopped successfully")
    
    async def generate_sample_data(self):
        """
        Sample trading data generate ‡§ï‡§∞‡•ã - demo ‡§ï‡•á ‡§≤‡§ø‡§è
        """
        logger.info("üìä Generating sample fintech data")
        
        conn = await asyncpg.connect(**self.connection_params)
        
        try:
            # Generate sample stock trades
            for i in range(50):
                trade = StockTrade(
                    trade_id=f"TRD{datetime.now().strftime('%Y%m%d')}{str(uuid.uuid4())[:8].upper()}",
                    symbol=self.nse_stocks[i % len(self.nse_stocks)],
                    exchange="NSE" if i % 2 == 0 else "BSE",
                    trade_type="BUY" if i % 3 == 0 else "SELL",
                    quantity=(i % 10 + 1) * 10,
                    price=Decimal(str(100 + (i % 500) + round(i * 0.25, 2))),
                    user_id=f"user_{(i % 20) + 1:03d}",
                    broker_id=f"ZER{(i % 5) + 1:03d}",
                    timestamp=datetime.now() - timedelta(minutes=i),
                    settlement_date=datetime.now().date() + timedelta(days=2),
                    brokerage=Decimal("20.00"),
                    taxes=Decimal("5.50")
                )
                
                await conn.execute("""
                    INSERT INTO stock_trades 
                    (trade_id, symbol, exchange, trade_type, quantity, price, user_id, broker_id, 
                     trade_timestamp, settlement_date, brokerage, taxes)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                """, trade.trade_id, trade.symbol, trade.exchange, trade.trade_type,
                    trade.quantity, trade.price, trade.user_id, trade.broker_id,
                    trade.timestamp, trade.settlement_date, trade.brokerage, trade.taxes)
            
            # Generate sample MF transactions
            for i in range(30):
                fund = self.mutual_funds[i % len(self.mutual_funds)]
                
                mf_txn = MutualFundTransaction(
                    transaction_id=f"MF{datetime.now().strftime('%Y%m%d')}{str(uuid.uuid4())[:8].upper()}",
                    fund_code=fund["code"],
                    amc=fund["amc"],
                    transaction_type="BUY" if i % 4 != 0 else "SIP",
                    units=Decimal(str(round(100 + i * 0.567, 6))),
                    nav=Decimal(str(round(50 + i * 0.123, 4))),
                    amount=Decimal(str(5000 + (i % 10) * 1000)),
                    user_id=f"user_{(i % 15) + 1:03d}",
                    folio_number=f"FOL{fund['amc']}{(i % 100) + 1:05d}",
                    timestamp=datetime.now() - timedelta(hours=i),
                    sip_id=f"SIP{str(uuid.uuid4())[:8].upper()}" if i % 4 == 0 else None
                )
                
                await conn.execute("""
                    INSERT INTO mutual_fund_transactions
                    (transaction_id, fund_code, amc, transaction_type, units, nav, amount,
                     user_id, folio_number, transaction_timestamp, sip_id)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
                """, mf_txn.transaction_id, mf_txn.fund_code, mf_txn.amc, mf_txn.transaction_type,
                    mf_txn.units, mf_txn.nav, mf_txn.amount, mf_txn.user_id, mf_txn.folio_number,
                    mf_txn.timestamp, mf_txn.sip_id)
            
            logger.info("‚úÖ Sample data generated successfully")
            
        except Exception as e:
            logger.error(f"üí• Sample data generation failed: {str(e)}")
        finally:
            await conn.close()

async def main():
    """
    Main function - Zerodha style fintech CDC demo
    """
    logger.info("üáÆüá≥ Starting Indian Fintech CDC Demo")
    
    # PostgreSQL connection parameters
    connection_params = {
        'host': 'localhost',
        'port': 5432,
        'user': 'fintech_cdc',
        'password': 'secure_fintech_123',
        'database': 'zerodha_fintech'
    }
    
    # Initialize CDC processor
    cdc_processor = ZerodhaFintechCDCProcessor(connection_params)
    
    try:
        # Start the CDC system
        await cdc_processor.start()
        
    except KeyboardInterrupt:
        logger.info("üõë Received interrupt signal")
        await cdc_processor.stop()
    except Exception as e:
        logger.error(f"üí• Unexpected error: {str(e)}")
        await cdc_processor.stop()

if __name__ == "__main__":
    asyncio.run(main())

"""
Production Deployment Guide:

1. PostgreSQL Configuration:
   wal_level = logical
   max_wal_senders = 10
   max_replication_slots = 10
   max_logical_replication_workers = 4

2. Security Setup:
   - Dedicated replication user with minimal privileges
   - SSL/TLS encryption for all connections
   - Network security groups for database access
   - Audit logging for compliance

3. High Availability:
   - Multiple replication slots for redundancy
   - Standby CDC processors for failover
   - Cross-region replication for disaster recovery
   - Health check endpoints for load balancers

4. Performance Tuning:
   - Connection pooling for database connections
   - Batch processing for high throughput
   - Partitioned tables for better performance
   - Indexing strategy for real-time queries

5. Indian Fintech Compliance:
   - SEBI regulations compliance
   - PII data masking for sensitive fields
   - Audit trails for all transactions
   - Regional data residency requirements
   - KYC/AML integration hooks

6. Monitoring & Alerting:
   - Prometheus metrics for replication lag
   - Grafana dashboards for system health
   - PagerDuty alerts for critical failures
   - Business metrics dashboards for stakeholders
"""