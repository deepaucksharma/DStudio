/*
Vector Database for AI at Scale
Episode 5: Go Implementation

Production-ready vector database for similarity search and RAG systems
Optimized for Indian AI applications with multilingual embeddings

Author: Code Developer Agent
Context: Indian AI/ML systems with Hindi/English content and cost optimization
*/

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"math"
	"math/rand"
	"net/http"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/gorilla/mux"
)

// Main vector database system
// ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø AI companies ‡§ï‡•á ‡§≤‡§ø‡§è high-performance vector search
type VectorDatabase struct {
	indexes         map[string]*VectorIndex
	collections     map[string]*Collection
	searchEngine    *SearchEngine
	metadataStore   *MetadataStore
	embeddingCache  *EmbeddingCache
	costTracker     *CostTracker
	metricsCollector *MetricsCollector
	mu              sync.RWMutex
	ctx             context.Context
	cancel          context.CancelFunc
}

// Vector index for efficient similarity search
// HNSW (Hierarchical Navigable Small World) algorithm implementation
type VectorIndex struct {
	ID           string                 `json:"id"`
	Name         string                 `json:"name"`
	Dimension    int                    `json:"dimension"`
	Metric       DistanceMetric         `json:"metric"`
	IndexType    IndexType              `json:"index_type"`
	Vectors      map[string]*Vector     `json:"-"`
	Graph        map[string][]string    `json:"-"` // HNSW graph structure
	Company      string                 `json:"company"`
	UseCase      string                 `json:"use_case"`
	Language     string                 `json:"language"`
	CreatedAt    time.Time              `json:"created_at"`
	LastModified time.Time              `json:"last_modified"`
	VectorCount  int64                  `json:"vector_count"`
	IndexSize    int64                  `json:"index_size_bytes"`
	mu           sync.RWMutex
}

// Individual vector with metadata
type Vector struct {
	ID          string                 `json:"id"`
	Embedding   []float32              `json:"embedding"`
	Metadata    map[string]interface{} `json:"metadata"`
	Text        string                 `json:"text"`        // Original text for RAG
	Language    string                 `json:"language"`    // Hindi/English/Tamil etc.
	Source      string                 `json:"source"`      // Document source
	ChunkIndex  int                    `json:"chunk_index"` // For document chunking
	Timestamp   time.Time              `json:"timestamp"`
	CostINR     float64                `json:"cost_inr"`    // Cost to generate embedding
}

// Collection groups related vectors
type Collection struct {
	ID            string              `json:"id"`
	Name          string              `json:"name"`
	Description   string              `json:"description"`
	Company       string              `json:"company"`
	UseCase       string              `json:"use_case"`
	VectorIndexID string              `json:"vector_index_id"`
	Schema        map[string]string   `json:"schema"`        // Metadata schema
	Languages     []string            `json:"languages"`     // Supported languages
	DocumentCount int64               `json:"document_count"`
	VectorCount   int64               `json:"vector_count"`
	CreatedAt     time.Time           `json:"created_at"`
	mu            sync.RWMutex
}

type DistanceMetric int

const (
	DistanceMetricCosine DistanceMetric = iota
	DistanceMetricEuclidean
	DistanceMetricDotProduct
	DistanceMetricManhattan
)

func (d DistanceMetric) String() string {
	switch d {
	case DistanceMetricCosine:
		return "cosine"
	case DistanceMetricEuclidean:
		return "euclidean"
	case DistanceMetricDotProduct:
		return "dot_product"
	case DistanceMetricManhattan:
		return "manhattan"
	default:
		return "unknown"
	}
}

type IndexType int

const (
	IndexTypeHNSW IndexType = iota
	IndexTypeFlatL2
	IndexTypeIVF
	IndexTypeLSH
)

// Search request structure
type SearchRequest struct {
	CollectionID  string                 `json:"collection_id"`
	Query         string                 `json:"query"`         // Text query
	QueryVector   []float32              `json:"query_vector"`  // Pre-computed embedding
	Language      string                 `json:"language"`
	TopK          int                    `json:"top_k"`
	Filter        map[string]interface{} `json:"filter"`        // Metadata filters
	Threshold     float32                `json:"threshold"`     // Similarity threshold
	IncludeText   bool                   `json:"include_text"`  // Include original text
	UserID        string                 `json:"user_id"`
	RequestID     string                 `json:"request_id"`
}

// Search result structure
type SearchResult struct {
	ID          string                 `json:"id"`
	Score       float32                `json:"score"`
	Vector      *Vector                `json:"vector"`
	Text        string                 `json:"text,omitempty"`
	Metadata    map[string]interface{} `json:"metadata"`
	Source      string                 `json:"source,omitempty"`
	ChunkIndex  int                    `json:"chunk_index,omitempty"`
}

// Search response
type SearchResponse struct {
	RequestID     string         `json:"request_id"`
	Results       []SearchResult `json:"results"`
	SearchTimeMs  int64          `json:"search_time_ms"`
	TotalResults  int            `json:"total_results"`
	CostINR       float64        `json:"cost_inr"`
	Language      string         `json:"language"`
	CollectionID  string         `json:"collection_id"`
}

// Search engine for query processing
type SearchEngine struct {
	embeddingService *EmbeddingService
	queryProcessor   *QueryProcessor
	ranker          *Ranker
	mu              sync.RWMutex
}

// Embedding service for text-to-vector conversion
// Supports Indian language models
type EmbeddingService struct {
	models        map[string]*EmbeddingModel
	defaultModel  string
	costTracker   *CostTracker
	cache         *EmbeddingCache
	mu            sync.RWMutex
}

type EmbeddingModel struct {
	ID                string   `json:"id"`
	Name              string   `json:"name"`
	Dimension         int      `json:"dimension"`
	SupportedLanguages []string `json:"supported_languages"`
	CostPerTokenINR   float64  `json:"cost_per_token_inr"`
	MaxTokens         int      `json:"max_tokens"`
	Provider          string   `json:"provider"` // OpenAI, HuggingFace, AI4Bharat, etc.
}

// Query processor for multilingual queries
type QueryProcessor struct {
	languageDetector *LanguageDetector
	textPreprocessor *TextPreprocessor
	mu               sync.RWMutex
}

// Language detector for Indian languages
type LanguageDetector struct {
	supportedLanguages map[string]*LanguageModel
	mu                sync.RWMutex
}

type LanguageModel struct {
	Language    string             `json:"language"`
	Code        string             `json:"code"` // hi, en, ta, bn, etc.
	Scripts     []string           `json:"scripts"` // Devanagari, Latin, Tamil, etc.
	Patterns    map[string]float64 `json:"patterns"` // Character patterns for detection
	Confidence  float64            `json:"confidence"`
}

// Text preprocessor for cleaning and normalizing text
type TextPreprocessor struct {
	stopWords    map[string][]string // Stop words by language
	normalizers  map[string]func(string) string
	tokenizers   map[string]func(string) []string
	mu           sync.RWMutex
}

// Ranking system for search results
type Ranker struct {
	algorithms map[string]RankingAlgorithm
	mu         sync.RWMutex
}

type RankingAlgorithm interface {
	Rank(results []SearchResult, query string, metadata map[string]interface{}) []SearchResult
}

// Metadata store for additional document information
type MetadataStore struct {
	store    map[string]map[string]interface{}
	indexes  map[string]map[string][]string // Secondary indexes for filtering
	mu       sync.RWMutex
}

// Embedding cache for performance optimization
type EmbeddingCache struct {
	cache        map[string]*CacheEntry
	maxSize      int
	currentSize  int
	hitCount     int64
	missCount    int64
	mu           sync.RWMutex
}

type CacheEntry struct {
	Embedding  []float32 `json:"embedding"`
	Text       string    `json:"text"`
	Language   string    `json:"language"`
	Model      string    `json:"model"`
	CreatedAt  time.Time `json:"created_at"`
	AccessedAt time.Time `json:"accessed_at"`
	AccessCount int64     `json:"access_count"`
}

// Cost tracker for Indian pricing
type CostTracker struct {
	dailyCosts     map[string]float64    // Cost by date
	monthlyCosts   map[string]float64    // Cost by month
	operationCosts map[string]float64    // Cost by operation type
	budgetLimits   map[string]float64    // Budget limits
	mu             sync.RWMutex
}

// Metrics collector for monitoring
type MetricsCollector struct {
	searchLatency      map[string][]time.Duration
	indexingLatency    map[string][]time.Duration
	embeddingLatency   map[string][]time.Duration
	searchAccuracy     map[string][]float64
	cacheHitRates     map[string]float64
	languageDistribution map[string]int64
	mu                 sync.RWMutex
}

// Initialize vector database
func NewVectorDatabase() *VectorDatabase {
	ctx, cancel := context.WithCancel(context.Background())
	
	db := &VectorDatabase{
		indexes:         make(map[string]*VectorIndex),
		collections:     make(map[string]*Collection),
		searchEngine:    NewSearchEngine(),
		metadataStore:   NewMetadataStore(),
		embeddingCache:  NewEmbeddingCache(10000), // 10K cache entries
		costTracker:     NewCostTracker(),
		metricsCollector: NewMetricsCollector(),
		ctx:             ctx,
		cancel:          cancel,
	}
	
	// Start background processes
	go db.startCacheCleanup()
	go db.startMetricsCollection()
	go db.startCostTracking()
	
	log.Println("üöÄ Vector Database initialized for Indian AI systems")
	return db
}

func NewSearchEngine() *SearchEngine {
	return &SearchEngine{
		embeddingService: NewEmbeddingService(),
		queryProcessor:   NewQueryProcessor(),
		ranker:          NewRanker(),
	}
}

func NewEmbeddingService() *EmbeddingService {
	service := &EmbeddingService{
		models:       make(map[string]*EmbeddingModel),
		defaultModel: "multilingual-e5-large",
		costTracker:  NewCostTracker(),
		cache:        NewEmbeddingCache(5000),
	}
	
	// Add Indian language-optimized models
	service.addModel(&EmbeddingModel{
		ID:                "multilingual-e5-large",
		Name:              "Multilingual E5 Large",
		Dimension:         1024,
		SupportedLanguages: []string{"hindi", "english", "tamil", "bengali", "telugu", "marathi"},
		CostPerTokenINR:   0.0001, // ‚Çπ0.0001 per token
		MaxTokens:         512,
		Provider:          "huggingface",
	})
	
	service.addModel(&EmbeddingModel{
		ID:                "indic-bert-base",
		Name:              "AI4Bharat IndicBERT",
		Dimension:         768,
		SupportedLanguages: []string{"hindi", "bengali", "gujarati", "malayalam", "marathi", "nepali", "oriya", "punjabi", "sanskrit", "tamil", "telugu", "urdu"},
		CostPerTokenINR:   0.00005, // Cheaper for Indian languages
		MaxTokens:         512,
		Provider:          "ai4bharat",
	})
	
	service.addModel(&EmbeddingModel{
		ID:                "openai-text-embedding-3-large",
		Name:              "OpenAI Text Embedding 3 Large",
		Dimension:         3072,
		SupportedLanguages: []string{"english", "hindi"}, // Limited Hindi support
		CostPerTokenINR:   0.00013, // More expensive but high quality
		MaxTokens:         8191,
		Provider:          "openai",
	})
	
	return service
}

func (es *EmbeddingService) addModel(model *EmbeddingModel) {
	es.mu.Lock()
	defer es.mu.Unlock()
	es.models[model.ID] = model
}

func NewQueryProcessor() *QueryProcessor {
	return &QueryProcessor{
		languageDetector: NewLanguageDetector(),
		textPreprocessor: NewTextPreprocessor(),
	}
}

func NewLanguageDetector() *LanguageDetector {
	detector := &LanguageDetector{
		supportedLanguages: make(map[string]*LanguageModel),
	}
	
	// Add language models for detection
	detector.supportedLanguages["hindi"] = &LanguageModel{
		Language: "Hindi",
		Code:     "hi",
		Scripts:  []string{"Devanagari"},
		Patterns: map[string]float64{
			"devanagari_chars": 0.7, // 70% Devanagari characters for Hindi
		},
		Confidence: 0.85,
	}
	
	detector.supportedLanguages["english"] = &LanguageModel{
		Language: "English",
		Code:     "en",
		Scripts:  []string{"Latin"},
		Patterns: map[string]float64{
			"latin_chars": 0.8, // 80% Latin characters for English
		},
		Confidence: 0.9,
	}
	
	detector.supportedLanguages["tamil"] = &LanguageModel{
		Language: "Tamil",
		Code:     "ta",
		Scripts:  []string{"Tamil"},
		Patterns: map[string]float64{
			"tamil_chars": 0.7,
		},
		Confidence: 0.8,
	}
	
	return detector
}

func NewTextPreprocessor() *TextPreprocessor {
	processor := &TextPreprocessor{
		stopWords:   make(map[string][]string),
		normalizers: make(map[string]func(string) string),
		tokenizers:  make(map[string]func(string) []string),
	}
	
	// Add Hindi stop words
	processor.stopWords["hindi"] = []string{
		"‡§π‡•à", "‡§π‡•à‡§Ç", "‡§•‡§æ", "‡§•‡•Ä", "‡§•‡•á", "‡§î‡§∞", "‡§Ø‡§æ", "‡§ï‡•á", "‡§ï‡•Ä", "‡§ï‡•ã", "‡§∏‡•á", "‡§Æ‡•á‡§Ç", "‡§™‡§∞", "‡§ï‡§æ", "‡§ï‡•Ä", "‡§ï‡•á",
		"‡§è‡§ï", "‡§Ø‡§π", "‡§µ‡§π", "‡§ú‡•ã", "‡§ï‡§ø", "‡§≤‡§ø‡§è", "‡§∏‡§æ‡§•", "‡§¨‡§æ‡§¶", "‡§§‡§ï", "‡§≤‡•á‡§ï‡§ø‡§®", "‡§®‡§π‡•Ä‡§Ç", "‡§≠‡•Ä", "‡§Ö‡§ó‡§∞", "‡§ú‡§¨",
	}
	
	// Add English stop words
	processor.stopWords["english"] = []string{
		"the", "is", "at", "which", "on", "and", "or", "of", "to", "in", "a", "an",
		"as", "are", "was", "were", "been", "be", "have", "has", "had", "do", "does", "did",
		"will", "would", "could", "should", "may", "might", "must", "can", "shall",
	}
	
	// Add normalizers
	processor.normalizers["hindi"] = func(text string) string {
		// Remove extra whitespace, normalize Devanagari characters
		text = strings.TrimSpace(text)
		text = strings.ToLower(text)
		// Add more Hindi-specific normalization here
		return text
	}
	
	processor.normalizers["english"] = func(text string) string {
		text = strings.TrimSpace(text)
		text = strings.ToLower(text)
		return text
	}
	
	return processor
}

func NewRanker() *Ranker {
	return &Ranker{
		algorithms: make(map[string]RankingAlgorithm),
	}
}

func NewMetadataStore() *MetadataStore {
	return &MetadataStore{
		store:   make(map[string]map[string]interface{}),
		indexes: make(map[string]map[string][]string),
	}
}

func NewEmbeddingCache(maxSize int) *EmbeddingCache {
	return &EmbeddingCache{
		cache:       make(map[string]*CacheEntry),
		maxSize:     maxSize,
		currentSize: 0,
		hitCount:    0,
		missCount:   0,
	}
}

func NewCostTracker() *CostTracker {
	return &CostTracker{
		dailyCosts:     make(map[string]float64),
		monthlyCosts:   make(map[string]float64),
		operationCosts: make(map[string]float64),
		budgetLimits:   make(map[string]float64),
	}
}

func NewMetricsCollector() *MetricsCollector {
	return &MetricsCollector{
		searchLatency:        make(map[string][]time.Duration),
		indexingLatency:      make(map[string][]time.Duration),
		embeddingLatency:     make(map[string][]time.Duration),
		searchAccuracy:       make(map[string][]float64),
		cacheHitRates:       make(map[string]float64),
		languageDistribution: make(map[string]int64),
	}
}

// Create collection
func (vdb *VectorDatabase) CreateCollection(config CollectionConfig) (*Collection, error) {
	vdb.mu.Lock()
	defer vdb.mu.Unlock()
	
	if _, exists := vdb.collections[config.ID]; exists {
		return nil, fmt.Errorf("collection %s already exists", config.ID)
	}
	
	// Create vector index
	indexConfig := IndexConfig{
		ID:        config.ID + "_index",
		Name:      config.Name + " Index",
		Dimension: config.Dimension,
		Metric:    config.Metric,
		IndexType: config.IndexType,
		Company:   config.Company,
		UseCase:   config.UseCase,
		Language:  config.Language,
	}
	
	index, err := vdb.createVectorIndex(indexConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create vector index: %v", err)
	}
	
	collection := &Collection{
		ID:            config.ID,
		Name:          config.Name,
		Description:   config.Description,
		Company:       config.Company,
		UseCase:       config.UseCase,
		VectorIndexID: index.ID,
		Schema:        config.Schema,
		Languages:     config.Languages,
		DocumentCount: 0,
		VectorCount:   0,
		CreatedAt:     time.Now(),
	}
	
	vdb.collections[config.ID] = collection
	
	log.Printf("‚úÖ Created collection %s for %s (%s)", config.Name, config.Company, config.UseCase)
	return collection, nil
}

type CollectionConfig struct {
	ID          string            `json:"id"`
	Name        string            `json:"name"`
	Description string            `json:"description"`
	Company     string            `json:"company"`
	UseCase     string            `json:"use_case"`
	Dimension   int               `json:"dimension"`
	Metric      DistanceMetric    `json:"metric"`
	IndexType   IndexType         `json:"index_type"`
	Language    string            `json:"language"`
	Languages   []string          `json:"languages"`
	Schema      map[string]string `json:"schema"`
}

type IndexConfig struct {
	ID        string         `json:"id"`
	Name      string         `json:"name"`
	Dimension int            `json:"dimension"`
	Metric    DistanceMetric `json:"metric"`
	IndexType IndexType      `json:"index_type"`
	Company   string         `json:"company"`
	UseCase   string         `json:"use_case"`
	Language  string         `json:"language"`
}

func (vdb *VectorDatabase) createVectorIndex(config IndexConfig) (*VectorIndex, error) {
	index := &VectorIndex{
		ID:           config.ID,
		Name:         config.Name,
		Dimension:    config.Dimension,
		Metric:       config.Metric,
		IndexType:    config.IndexType,
		Vectors:      make(map[string]*Vector),
		Graph:        make(map[string][]string),
		Company:      config.Company,
		UseCase:      config.UseCase,
		Language:     config.Language,
		CreatedAt:    time.Now(),
		LastModified: time.Now(),
		VectorCount:  0,
		IndexSize:    0,
	}
	
	vdb.indexes[config.ID] = index
	return index, nil
}

// Add document to collection
func (vdb *VectorDatabase) AddDocument(collectionID string, document Document) error {
	vdb.mu.RLock()
	collection, exists := vdb.collections[collectionID]
	if !exists {
		vdb.mu.RUnlock()
		return fmt.Errorf("collection %s not found", collectionID)
	}
	
	index, exists := vdb.indexes[collection.VectorIndexID]
	if !exists {
		vdb.mu.RUnlock()
		return fmt.Errorf("vector index %s not found", collection.VectorIndexID)
	}
	vdb.mu.RUnlock()
	
	startTime := time.Now()
	
	// Detect language
	language := vdb.searchEngine.queryProcessor.languageDetector.DetectLanguage(document.Text)
	
	// Chunk document if it's too long
	chunks := vdb.chunkDocument(document.Text, language, 512) // Max 512 tokens per chunk
	
	vectors := make([]*Vector, 0, len(chunks))
	totalCost := 0.0
	
	for i, chunk := range chunks {
		// Generate embedding
		embedding, cost, err := vdb.searchEngine.embeddingService.GenerateEmbedding(chunk, language)
		if err != nil {
			return fmt.Errorf("failed to generate embedding: %v", err)
		}
		
		vector := &Vector{
			ID:          fmt.Sprintf("%s_chunk_%d", document.ID, i),
			Embedding:   embedding,
			Metadata:    document.Metadata,
			Text:        chunk,
			Language:    language,
			Source:      document.Source,
			ChunkIndex:  i,
			Timestamp:   time.Now(),
			CostINR:     cost,
		}
		
		vectors = append(vectors, vector)
		totalCost += cost
	}
	
	// Add vectors to index
	err := vdb.addVectorsToIndex(index, vectors)
	if err != nil {
		return fmt.Errorf("failed to add vectors to index: %v", err)
	}
	
	// Update collection stats
	collection.mu.Lock()
	collection.DocumentCount++
	collection.VectorCount += int64(len(vectors))
	collection.mu.Unlock()
	
	// Record metrics
	indexingTime := time.Since(startTime)
	vdb.metricsCollector.recordIndexing(collectionID, indexingTime, int64(len(vectors)))
	vdb.costTracker.recordCost("indexing", totalCost)
	
	log.Printf("üìù Added document %s to collection %s: %d chunks, ‚Çπ%.4f cost, %v latency",
		document.ID, collectionID, len(chunks), totalCost, indexingTime)
	
	return nil
}

type Document struct {
	ID       string                 `json:"id"`
	Text     string                 `json:"text"`
	Source   string                 `json:"source"`
	Metadata map[string]interface{} `json:"metadata"`
}

func (vdb *VectorDatabase) chunkDocument(text string, language string, maxTokens int) []string {
	// Simple sentence-based chunking
	// In production, use more sophisticated chunking strategies
	
	sentences := strings.Split(text, ".")
	chunks := make([]string, 0)
	currentChunk := ""
	
	for _, sentence := range sentences {
		sentence = strings.TrimSpace(sentence)
		if sentence == "" {
			continue
		}
		
		// Rough token estimation: 1 token ‚âà 4 characters for English, 2.5 for Hindi
		tokensPerChar := 4.0
		if language == "hindi" {
			tokensPerChar = 2.5
		}
		
		sentenceTokens := int(float64(len(sentence)) / tokensPerChar)
		currentTokens := int(float64(len(currentChunk)) / tokensPerChar)
		
		if currentTokens+sentenceTokens > maxTokens && currentChunk != "" {
			chunks = append(chunks, currentChunk)
			currentChunk = sentence
		} else {
			if currentChunk != "" {
				currentChunk += ". " + sentence
			} else {
				currentChunk = sentence
			}
		}
	}
	
	if currentChunk != "" {
		chunks = append(chunks, currentChunk)
	}
	
	return chunks
}

func (vdb *VectorDatabase) addVectorsToIndex(index *VectorIndex, vectors []*Vector) error {
	index.mu.Lock()
	defer index.mu.Unlock()
	
	for _, vector := range vectors {
		index.Vectors[vector.ID] = vector
		
		// Build HNSW graph connections (simplified)
		vdb.addToHNSWGraph(index, vector)
	}
	
	index.VectorCount += int64(len(vectors))
	index.LastModified = time.Now()
	
	return nil
}

func (vdb *VectorDatabase) addToHNSWGraph(index *VectorIndex, newVector *Vector) {
	// Simplified HNSW graph building
	// In production, implement full HNSW algorithm with multiple layers
	
	const maxConnections = 16
	
	// Find nearest neighbors for connections
	neighbors := vdb.findNearestVectors(index, newVector.Embedding, maxConnections)
	
	connections := make([]string, 0, len(neighbors))
	for _, neighbor := range neighbors {
		connections = append(connections, neighbor.ID)
		
		// Add bidirectional connection
		if index.Graph[neighbor.ID] == nil {
			index.Graph[neighbor.ID] = make([]string, 0)
		}
		
		// Limit connections per node
		if len(index.Graph[neighbor.ID]) < maxConnections {
			index.Graph[neighbor.ID] = append(index.Graph[neighbor.ID], newVector.ID)
		}
	}
	
	index.Graph[newVector.ID] = connections
}

func (vdb *VectorDatabase) findNearestVectors(index *VectorIndex, queryVector []float32, k int) []*Vector {
	type scoredVector struct {
		vector *Vector
		score  float32
	}
	
	scored := make([]scoredVector, 0, len(index.Vectors))
	
	for _, vector := range index.Vectors {
		score := vdb.calculateSimilarity(queryVector, vector.Embedding, index.Metric)
		scored = append(scored, scoredVector{vector: vector, score: score})
	}
	
	// Sort by score (descending for cosine similarity)
	sort.Slice(scored, func(i, j int) bool {
		return scored[i].score > scored[j].score
	})
	
	// Return top k
	result := make([]*Vector, 0, k)
	for i := 0; i < k && i < len(scored); i++ {
		result = append(result, scored[i].vector)
	}
	
	return result
}

// Search in collection
func (vdb *VectorDatabase) Search(request *SearchRequest) (*SearchResponse, error) {
	startTime := time.Now()
	
	vdb.mu.RLock()
	collection, exists := vdb.collections[request.CollectionID]
	if !exists {
		vdb.mu.RUnlock()
		return nil, fmt.Errorf("collection %s not found", request.CollectionID)
	}
	
	index, exists := vdb.indexes[collection.VectorIndexID]
	if !exists {
		vdb.mu.RUnlock()
		return nil, fmt.Errorf("vector index %s not found", collection.VectorIndexID)
	}
	vdb.mu.RUnlock()
	
	var queryVector []float32
	var embeddingCost float64
	
	// Generate query embedding if not provided
	if len(request.QueryVector) == 0 {
		language := request.Language
		if language == "" {
			language = vdb.searchEngine.queryProcessor.languageDetector.DetectLanguage(request.Query)
		}
		
		var err error
		queryVector, embeddingCost, err = vdb.searchEngine.embeddingService.GenerateEmbedding(request.Query, language)
		if err != nil {
			return nil, fmt.Errorf("failed to generate query embedding: %v", err)
		}
	} else {
		queryVector = request.QueryVector
	}
	
	// Perform vector search
	searchResults := vdb.performVectorSearch(index, queryVector, request)
	
	// Apply metadata filters
	if len(request.Filter) > 0 {
		searchResults = vdb.applyFilters(searchResults, request.Filter)
	}
	
	// Apply threshold
	if request.Threshold > 0 {
		filteredResults := make([]SearchResult, 0, len(searchResults))
		for _, result := range searchResults {
			if result.Score >= request.Threshold {
				filteredResults = append(filteredResults, result)
			}
		}
		searchResults = filteredResults
	}
	
	// Limit to top K
	if request.TopK > 0 && len(searchResults) > request.TopK {
		searchResults = searchResults[:request.TopK]
	}
	
	searchTime := time.Since(startTime)
	totalCost := embeddingCost + 0.001 // Small search cost
	
	response := &SearchResponse{
		RequestID:    request.RequestID,
		Results:      searchResults,
		SearchTimeMs: searchTime.Milliseconds(),
		TotalResults: len(searchResults),
		CostINR:      totalCost,
		Language:     request.Language,
		CollectionID: request.CollectionID,
	}
	
	// Record metrics
	vdb.metricsCollector.recordSearch(request.CollectionID, searchTime, len(searchResults))
	vdb.costTracker.recordCost("search", totalCost)
	
	return response, nil
}

func (vdb *VectorDatabase) performVectorSearch(index *VectorIndex, queryVector []float32, request *SearchRequest) []SearchResult {
	type scoredResult struct {
		vector *Vector
		score  float32
	}
	
	scored := make([]scoredResult, 0, len(index.Vectors))
	
	// Search using HNSW graph (simplified)
	// In production, implement full HNSW search algorithm
	
	for _, vector := range index.Vectors {
		score := vdb.calculateSimilarity(queryVector, vector.Embedding, index.Metric)
		scored = append(scored, scoredResult{vector: vector, score: score})
	}
	
	// Sort by score
	sort.Slice(scored, func(i, j int) bool {
		if index.Metric == DistanceMetricCosine || index.Metric == DistanceMetricDotProduct {
			return scored[i].score > scored[j].score // Higher is better
		}
		return scored[i].score < scored[j].score // Lower is better for distance metrics
	})
	
	// Convert to search results
	results := make([]SearchResult, 0, len(scored))
	for _, scored := range scored {
		result := SearchResult{
			ID:       scored.vector.ID,
			Score:    scored.score,
			Vector:   scored.vector,
			Metadata: scored.vector.Metadata,
			Source:   scored.vector.Source,
			ChunkIndex: scored.vector.ChunkIndex,
		}
		
		if request.IncludeText {
			result.Text = scored.vector.Text
		}
		
		results = append(results, result)
	}
	
	return results
}

func (vdb *VectorDatabase) calculateSimilarity(vec1, vec2 []float32, metric DistanceMetric) float32 {
	if len(vec1) != len(vec2) {
		return 0.0
	}
	
	switch metric {
	case DistanceMetricCosine:
		return vdb.cosineSimilarity(vec1, vec2)
	case DistanceMetricEuclidean:
		return vdb.euclideanDistance(vec1, vec2)
	case DistanceMetricDotProduct:
		return vdb.dotProduct(vec1, vec2)
	case DistanceMetricManhattan:
		return vdb.manhattanDistance(vec1, vec2)
	default:
		return vdb.cosineSimilarity(vec1, vec2)
	}
}

func (vdb *VectorDatabase) cosineSimilarity(vec1, vec2 []float32) float32 {
	var dotProduct, norm1, norm2 float32
	
	for i := 0; i < len(vec1); i++ {
		dotProduct += vec1[i] * vec2[i]
		norm1 += vec1[i] * vec1[i]
		norm2 += vec2[i] * vec2[i]
	}
	
	if norm1 == 0 || norm2 == 0 {
		return 0
	}
	
	return dotProduct / (float32(math.Sqrt(float64(norm1))) * float32(math.Sqrt(float64(norm2))))
}

func (vdb *VectorDatabase) euclideanDistance(vec1, vec2 []float32) float32 {
	var sum float32
	for i := 0; i < len(vec1); i++ {
		diff := vec1[i] - vec2[i]
		sum += diff * diff
	}
	return float32(math.Sqrt(float64(sum)))
}

func (vdb *VectorDatabase) dotProduct(vec1, vec2 []float32) float32 {
	var sum float32
	for i := 0; i < len(vec1); i++ {
		sum += vec1[i] * vec2[i]
	}
	return sum
}

func (vdb *VectorDatabase) manhattanDistance(vec1, vec2 []float32) float32 {
	var sum float32
	for i := 0; i < len(vec1); i++ {
		sum += float32(math.Abs(float64(vec1[i] - vec2[i])))
	}
	return sum
}

func (vdb *VectorDatabase) applyFilters(results []SearchResult, filters map[string]interface{}) []SearchResult {
	filtered := make([]SearchResult, 0, len(results))
	
	for _, result := range results {
		matches := true
		
		for key, value := range filters {
			if metaValue, exists := result.Metadata[key]; exists {
				if metaValue != value {
					matches = false
					break
				}
			} else {
				matches = false
				break
			}
		}
		
		if matches {
			filtered = append(filtered, result)
		}
	}
	
	return filtered
}

// Language detection
func (ld *LanguageDetector) DetectLanguage(text string) string {
	ld.mu.RLock()
	defer ld.mu.RUnlock()
	
	scores := make(map[string]float64)
	
	// Count character types
	devanagariCount := 0
	tamilCount := 0
	latinCount := 0
	totalChars := 0
	
	for _, char := range text {
		totalChars++
		
		// Devanagari script (Hindi)
		if char >= 0x0900 && char <= 0x097F {
			devanagariCount++
		}
		// Tamil script
		if char >= 0x0B80 && char <= 0x0BFF {
			tamilCount++
		}
		// Latin script (English)
		if (char >= 'a' && char <= 'z') || (char >= 'A' && char <= 'Z') {
			latinCount++
		}
	}
	
	if totalChars == 0 {
		return "english" // Default
	}
	
	// Calculate percentages
	devanagariPercent := float64(devanagariCount) / float64(totalChars)
	tamilPercent := float64(tamilCount) / float64(totalChars)
	latinPercent := float64(latinCount) / float64(totalChars)
	
	// Score languages
	if devanagariPercent > 0.3 {
		scores["hindi"] = devanagariPercent * 0.9
	}
	if tamilPercent > 0.3 {
		scores["tamil"] = tamilPercent * 0.9
	}
	if latinPercent > 0.5 {
		scores["english"] = latinPercent * 0.8
	}
	
	// Find best match
	bestLang := "english"
	bestScore := 0.0
	
	for lang, score := range scores {
		if score > bestScore {
			bestScore = score
			bestLang = lang
		}
	}
	
	return bestLang
}

// Generate embedding
func (es *EmbeddingService) GenerateEmbedding(text string, language string) ([]float32, float64, error) {
	es.mu.RLock()
	defer es.mu.RUnlock()
	
	// Check cache first
	cacheKey := fmt.Sprintf("%s_%s_%s", es.defaultModel, language, text)
	if entry, exists := es.cache.get(cacheKey); exists {
		return entry.Embedding, 0.0, nil // No cost for cached embeddings
	}
	
	// Select appropriate model
	model := es.models[es.defaultModel]
	if model == nil {
		return nil, 0.0, fmt.Errorf("model %s not found", es.defaultModel)
	}
	
	// Check if model supports the language
	supportsLanguage := false
	for _, lang := range model.SupportedLanguages {
		if lang == language {
			supportsLanguage = true
			break
		}
	}
	
	if !supportsLanguage {
		// Fallback to multilingual model
		for _, m := range es.models {
			for _, lang := range m.SupportedLanguages {
				if lang == language {
					model = m
					supportsLanguage = true
					break
				}
			}
			if supportsLanguage {
				break
			}
		}
	}
	
	// Generate embedding (simulate)
	embedding := vdb.generateMockEmbedding(text, model.Dimension)
	
	// Calculate cost
	tokenCount := len(strings.Fields(text)) // Simple token counting
	cost := float64(tokenCount) * model.CostPerTokenINR
	
	// Cache the result
	es.cache.put(cacheKey, &CacheEntry{
		Embedding:   embedding,
		Text:        text,
		Language:    language,
		Model:       model.ID,
		CreatedAt:   time.Now(),
		AccessedAt:  time.Now(),
		AccessCount: 1,
	})
	
	return embedding, cost, nil
}

func (vdb *VectorDatabase) generateMockEmbedding(text string, dimension int) []float32 {
	// Generate deterministic embedding based on text hash
	// In production, call actual embedding model API
	
	embedding := make([]float32, dimension)
	
	// Simple hash-based embedding generation
	hash := 0
	for _, char := range text {
		hash = hash*31 + int(char)
	}
	
	rand.Seed(int64(hash))
	
	// Generate normalized random vector
	var norm float32 = 0
	for i := 0; i < dimension; i++ {
		value := rand.Float32()*2 - 1 // [-1, 1]
		embedding[i] = value
		norm += value * value
	}
	
	// Normalize
	norm = float32(math.Sqrt(float64(norm)))
	if norm > 0 {
		for i := 0; i < dimension; i++ {
			embedding[i] /= norm
		}
	}
	
	return embedding
}

// Cache operations
func (ec *EmbeddingCache) get(key string) (*CacheEntry, bool) {
	ec.mu.RLock()
	defer ec.mu.RUnlock()
	
	entry, exists := ec.cache[key]
	if exists {
		entry.AccessedAt = time.Now()
		entry.AccessCount++
		ec.hitCount++
		return entry, true
	}
	
	ec.missCount++
	return nil, false
}

func (ec *EmbeddingCache) put(key string, entry *CacheEntry) {
	ec.mu.Lock()
	defer ec.mu.Unlock()
	
	// Check if cache is full
	if ec.currentSize >= ec.maxSize {
		// Remove least recently used entry
		ec.evictLRU()
	}
	
	ec.cache[key] = entry
	ec.currentSize++
}

func (ec *EmbeddingCache) evictLRU() {
	var oldestKey string
	var oldestTime time.Time = time.Now()
	
	for key, entry := range ec.cache {
		if entry.AccessedAt.Before(oldestTime) {
			oldestTime = entry.AccessedAt
			oldestKey = key
		}
	}
	
	if oldestKey != "" {
		delete(ec.cache, oldestKey)
		ec.currentSize--
	}
}

// Cost tracking
func (ct *CostTracker) recordCost(operation string, costINR float64) {
	ct.mu.Lock()
	defer ct.mu.Unlock()
	
	today := time.Now().Format("2006-01-02")
	month := time.Now().Format("2006-01")
	
	ct.dailyCosts[today] += costINR
	ct.monthlyCosts[month] += costINR
	ct.operationCosts[operation] += costINR
}

// Metrics recording
func (mc *MetricsCollector) recordSearch(collectionID string, latency time.Duration, resultCount int) {
	mc.mu.Lock()
	defer mc.mu.Unlock()
	
	if mc.searchLatency[collectionID] == nil {
		mc.searchLatency[collectionID] = make([]time.Duration, 0)
	}
	
	mc.searchLatency[collectionID] = append(mc.searchLatency[collectionID], latency)
	
	// Keep only last 1000 measurements
	if len(mc.searchLatency[collectionID]) > 1000 {
		mc.searchLatency[collectionID] = mc.searchLatency[collectionID][1:]
	}
}

func (mc *MetricsCollector) recordIndexing(collectionID string, latency time.Duration, vectorCount int64) {
	mc.mu.Lock()
	defer mc.mu.Unlock()
	
	if mc.indexingLatency[collectionID] == nil {
		mc.indexingLatency[collectionID] = make([]time.Duration, 0)
	}
	
	mc.indexingLatency[collectionID] = append(mc.indexingLatency[collectionID], latency)
	
	// Keep only last 1000 measurements
	if len(mc.indexingLatency[collectionID]) > 1000 {
		mc.indexingLatency[collectionID] = mc.indexingLatency[collectionID][1:]
	}
}

// Background processes
func (vdb *VectorDatabase) startCacheCleanup() {
	ticker := time.NewTicker(30 * time.Minute)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			vdb.performCacheCleanup()
		case <-vdb.ctx.Done():
			return
		}
	}
}

func (vdb *VectorDatabase) performCacheCleanup() {
	// Clean up old cache entries
	cutoff := time.Now().Add(-2 * time.Hour)
	
	vdb.embeddingCache.mu.Lock()
	defer vdb.embeddingCache.mu.Unlock()
	
	for key, entry := range vdb.embeddingCache.cache {
		if entry.AccessedAt.Before(cutoff) {
			delete(vdb.embeddingCache.cache, key)
			vdb.embeddingCache.currentSize--
		}
	}
	
	log.Printf("üßπ Cache cleanup completed - Size: %d/%d, Hit rate: %.2f%%",
		vdb.embeddingCache.currentSize, vdb.embeddingCache.maxSize,
		float64(vdb.embeddingCache.hitCount)/float64(vdb.embeddingCache.hitCount+vdb.embeddingCache.missCount)*100)
}

func (vdb *VectorDatabase) startMetricsCollection() {
	ticker := time.NewTicker(5 * time.Minute)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			vdb.collectMetrics()
		case <-vdb.ctx.Done():
			return
		}
	}
}

func (vdb *VectorDatabase) collectMetrics() {
	vdb.mu.RLock()
	totalCollections := len(vdb.collections)
	totalIndexes := len(vdb.indexes)
	totalVectors := int64(0)
	
	for _, index := range vdb.indexes {
		totalVectors += index.VectorCount
	}
	vdb.mu.RUnlock()
	
	log.Printf("üìä Vector DB Status - Collections: %d, Indexes: %d, Vectors: %d",
		totalCollections, totalIndexes, totalVectors)
}

func (vdb *VectorDatabase) startCostTracking() {
	ticker := time.NewTicker(1 * time.Hour)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			vdb.trackCosts()
		case <-vdb.ctx.Done():
			return
		}
	}
}

func (vdb *VectorDatabase) trackCosts() {
	vdb.costTracker.mu.RLock()
	today := time.Now().Format("2006-01-02")
	dailyCost := vdb.costTracker.dailyCosts[today]
	vdb.costTracker.mu.RUnlock()
	
	log.Printf("üí∞ Daily cost so far: ‚Çπ%.4f", dailyCost)
}

// HTTP API endpoints
func (vdb *VectorDatabase) handleCreateCollection(w http.ResponseWriter, r *http.Request) {
	var config CollectionConfig
	if err := json.NewDecoder(r.Body).Decode(&config); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}
	
	collection, err := vdb.CreateCollection(config)
	if err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(collection)
}

func (vdb *VectorDatabase) handleAddDocument(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	collectionID := vars["collectionId"]
	
	var document Document
	if err := json.NewDecoder(r.Body).Decode(&document); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}
	
	err := vdb.AddDocument(collectionID, document)
	if err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]string{"status": "success"})
}

func (vdb *VectorDatabase) handleSearch(w http.ResponseWriter, r *http.Request) {
	var request SearchRequest
	if err := json.NewDecoder(r.Body).Decode(&request); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}
	
	if request.RequestID == "" {
		request.RequestID = fmt.Sprintf("req_%d", time.Now().Unix())
	}
	
	response, err := vdb.Search(&request)
	if err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

func (vdb *VectorDatabase) handleStatus(w http.ResponseWriter, r *http.Request) {
	vdb.mu.RLock()
	status := map[string]interface{}{
		"collections":  len(vdb.collections),
		"indexes":      len(vdb.indexes),
		"cache_size":   vdb.embeddingCache.currentSize,
		"cache_hits":   vdb.embeddingCache.hitCount,
		"cache_misses": vdb.embeddingCache.missCount,
		"timestamp":    time.Now(),
	}
	vdb.mu.RUnlock()
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(status)
}

// Start HTTP server
func (vdb *VectorDatabase) StartHTTPServer(port string) {
	router := mux.NewRouter()
	
	// API endpoints
	router.HandleFunc("/collections", vdb.handleCreateCollection).Methods("POST")
	router.HandleFunc("/collections/{collectionId}/documents", vdb.handleAddDocument).Methods("POST")
	router.HandleFunc("/search", vdb.handleSearch).Methods("POST")
	router.HandleFunc("/status", vdb.handleStatus).Methods("GET")
	
	log.Printf("üåê Vector Database HTTP server starting on port %s", port)
	log.Printf("üîç Search endpoint: http://localhost:%s/search", port)
	log.Printf("üìä Status endpoint: http://localhost:%s/status", port)
	
	log.Fatal(http.ListenAndServe(":"+port, router))
}

// Shutdown
func (vdb *VectorDatabase) Shutdown() {
	log.Println("üõë Shutting down Vector Database...")
	vdb.cancel()
	log.Println("‚úÖ Vector Database shutdown complete")
}

// Demo function
func main() {
	fmt.Println("üöÄ Vector Database Demo - Indian AI Systems")
	fmt.Println(strings.Repeat("=", 70))
	
	// Initialize database
	vdb := NewVectorDatabase()
	defer vdb.Shutdown()
	
	// Create collections for different Indian companies
	collections := []CollectionConfig{
		{
			ID:          "paytm-kb",
			Name:        "PayTM Knowledge Base",
			Description: "PayTM support and documentation vectors",
			Company:     "paytm",
			UseCase:     "customer_support",
			Dimension:   768,
			Metric:      DistanceMetricCosine,
			IndexType:   IndexTypeHNSW,
			Language:    "hindi",
			Languages:   []string{"hindi", "english"},
			Schema: map[string]string{
				"category": "string",
				"priority": "string",
				"topic":    "string",
			},
		},
		{
			ID:          "flipkart-products",
			Name:        "Flipkart Product Catalog",
			Description: "Product descriptions and reviews for search",
			Company:     "flipkart",
			UseCase:     "product_search",
			Dimension:   1024,
			Metric:      DistanceMetricCosine,
			IndexType:   IndexTypeHNSW,
			Language:    "english",
			Languages:   []string{"hindi", "english", "tamil"},
			Schema: map[string]string{
				"category":    "string",
				"brand":       "string",
				"price_range": "string",
				"rating":      "float",
			},
		},
		{
			ID:          "zomato-reviews",
			Name:        "Zomato Restaurant Reviews",
			Description: "Restaurant reviews and food descriptions",
			Company:     "zomato",
			UseCase:     "review_search",
			Dimension:   768,
			Metric:      DistanceMetricCosine,
			IndexType:   IndexTypeHNSW,
			Language:    "english",
			Languages:   []string{"hindi", "english"},
			Schema: map[string]string{
				"restaurant_id": "string",
				"cuisine":       "string",
				"rating":        "float",
				"city":          "string",
			},
		},
	}
	
	// Create collections
	for _, config := range collections {
		_, err := vdb.CreateCollection(config)
		if err != nil {
			log.Printf("‚ùå Failed to create collection %s: %v", config.Name, err)
		}
	}
	
	fmt.Println("\n‚úÖ Created 3 vector collections for Indian companies")
	
	// Add sample documents
	documents := []struct {
		collectionID string
		documents    []Document
	}{
		{
			collectionID: "paytm-kb",
			documents: []Document{
				{
					ID:     "paytm_doc_1",
					Text:   "PayTM wallet ‡§Æ‡•á‡§Ç ‡§™‡•à‡§∏‡•á add ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§™‡§®‡§æ debit card ‡§Ø‡§æ net banking use ‡§ï‡§∞‡•á‡§Ç‡•§ UPI ‡§ï‡•á through ‡§≠‡•Ä ‡§™‡•à‡§∏‡•á add ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§",
					Source: "paytm_help_center",
					Metadata: map[string]interface{}{
						"category": "wallet",
						"priority": "high",
						"topic":    "add_money",
					},
				},
				{
					ID:     "paytm_doc_2",
					Text:   "PayTM ‡§∏‡•á bill payment ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ cashback ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§π‡•à‡•§ Electricity, mobile recharge, DTH ‡§ï‡•á bills pay ‡§ï‡§∞‡§ï‡•á rewards earn ‡§ï‡§∞‡•á‡§Ç‡•§",
					Source: "paytm_offers",
					Metadata: map[string]interface{}{
						"category": "payments",
						"priority": "medium",
						"topic":    "cashback",
					},
				},
			},
		},
		{
			collectionID: "flipkart-products",
			documents: []Document{
				{
					ID:     "product_1",
					Text:   "Apple iPhone 15 Pro with titanium design, A17 Pro chip, and advanced camera system. Perfect for photography enthusiasts.",
					Source: "product_catalog",
					Metadata: map[string]interface{}{
						"category":    "smartphones",
						"brand":       "apple",
						"price_range": "premium",
						"rating":      4.5,
					},
				},
				{
					ID:     "product_2",
					Text:   "Samsung Galaxy S24 Ultra featuring S Pen, 200MP camera, and AI-powered photography. Best Android flagship smartphone.",
					Source: "product_catalog",
					Metadata: map[string]interface{}{
						"category":    "smartphones",
						"brand":       "samsung",
						"price_range": "premium",
						"rating":      4.4,
					},
				},
			},
		},
		{
			collectionID: "zomato-reviews",
			documents: []Document{
				{
					ID:     "review_1",
					Text:   "Amazing North Indian food at this restaurant. The butter chicken and naan were outstanding. Great ambiance for family dining.",
					Source: "user_reviews",
					Metadata: map[string]interface{}{
						"restaurant_id": "rest_123",
						"cuisine":       "north_indian",
						"rating":        4.8,
						"city":          "mumbai",
					},
				},
				{
					ID:     "review_2", 
					Text:   "Delicious South Indian breakfast. The dosas were crispy and sambhar was flavorful. Quick service and reasonable prices.",
					Source: "user_reviews",
					Metadata: map[string]interface{}{
						"restaurant_id": "rest_456",
						"cuisine":       "south_indian",
						"rating":        4.3,
						"city":          "bangalore",
					},
				},
			},
		},
	}
	
	// Add documents to collections
	fmt.Println("\nüìù Adding documents to collections...")
	for _, group := range documents {
		for _, doc := range group.documents {
			err := vdb.AddDocument(group.collectionID, doc)
			if err != nil {
				log.Printf("‚ùå Failed to add document %s: %v", doc.ID, err)
			}
		}
	}
	
	// Perform sample searches
	fmt.Println("\nüîç Performing sample searches...")
	
	searches := []SearchRequest{
		{
			CollectionID: "paytm-kb",
			Query:        "wallet ‡§Æ‡•á‡§Ç ‡§™‡•à‡§∏‡•á ‡§ï‡•à‡§∏‡•á ‡§°‡§æ‡§≤‡•á‡§Ç",
			Language:     "hindi",
			TopK:         3,
			IncludeText:  true,
			RequestID:    "search_1",
		},
		{
			CollectionID: "flipkart-products",
			Query:        "best camera phone for photography",
			Language:     "english",
			TopK:         2,
			Filter: map[string]interface{}{
				"category": "smartphones",
			},
			IncludeText: true,
			RequestID:   "search_2",
		},
		{
			CollectionID: "zomato-reviews",
			Query:        "good south indian restaurant",
			Language:     "english",
			TopK:         2,
			Filter: map[string]interface{}{
				"cuisine": "south_indian",
			},
			IncludeText: true,
			RequestID:   "search_3",
		},
	}
	
	for i, searchReq := range searches {
		fmt.Printf("\nüîç Search %d: %s\n", i+1, searchReq.Query)
		
		response, err := vdb.Search(&searchReq)
		if err != nil {
			fmt.Printf("‚ùå Search failed: %v\n", err)
			continue
		}
		
		fmt.Printf("   Results: %d, Time: %dms, Cost: ‚Çπ%.6f\n",
			response.TotalResults, response.SearchTimeMs, response.CostINR)
		
		for j, result := range response.Results {
			fmt.Printf("   Result %d (Score: %.3f):\n", j+1, result.Score)
			fmt.Printf("     Text: %s\n", result.Text)
			fmt.Printf("     Metadata: %v\n", result.Metadata)
			fmt.Printf("     Source: %s\n\n", result.Source)
		}
	}
	
	// Show system status
	fmt.Println("üìä System Status:")
	vdb.mu.RLock()
	totalCollections := len(vdb.collections)
	totalVectors := int64(0)
	
	for _, collection := range vdb.collections {
		totalVectors += collection.VectorCount
		fmt.Printf("   %s (%s): %d vectors\n",
			collection.Name, collection.Company, collection.VectorCount)
	}
	vdb.mu.RUnlock()
	
	fmt.Printf("\nOverall Statistics:\n")
	fmt.Printf("   Collections: %d\n", totalCollections)
	fmt.Printf("   Total Vectors: %d\n", totalVectors)
	fmt.Printf("   Cache Hit Rate: %.2f%%\n",
		float64(vdb.embeddingCache.hitCount)/float64(vdb.embeddingCache.hitCount+vdb.embeddingCache.missCount)*100)
	
	// Show costs
	vdb.costTracker.mu.RLock()
	today := time.Now().Format("2006-01-02")
	dailyCost := vdb.costTracker.dailyCosts[today]
	vdb.costTracker.mu.RUnlock()
	
	fmt.Printf("   Today's Cost: ‚Çπ%.6f\n", dailyCost)
	
	fmt.Println("\nüéØ Indian AI Vector Database Features:")
	fmt.Println("   ‚úÖ Multilingual embedding support (Hindi, English, Tamil)")
	fmt.Println("   ‚úÖ Language detection and model selection")
	fmt.Println("   ‚úÖ Cost tracking with INR pricing")
	fmt.Println("   ‚úÖ Intelligent document chunking")
	fmt.Println("   ‚úÖ HNSW graph-based similarity search")
	fmt.Println("   ‚úÖ Metadata filtering and hybrid search")
	fmt.Println("   ‚úÖ Embedding caching for performance")
	fmt.Println("   ‚úÖ Real-time metrics and monitoring")
	fmt.Println("   ‚úÖ Company-specific use case optimization")
	fmt.Println("   ‚úÖ Production-ready HTTP API")
	
	// Start HTTP server for demonstration
	fmt.Println("\nüåê Starting HTTP server on port 8081...")
	fmt.Println("   Create collection: POST http://localhost:8081/collections")
	fmt.Println("   Add document: POST http://localhost:8081/collections/{id}/documents")
	fmt.Println("   Search: POST http://localhost:8081/search")
	fmt.Println("   Status: GET http://localhost:8081/status")
	
	go vdb.StartHTTPServer("8081")
	
	// Keep demo running
	fmt.Println("\n‚è∞ Demo running for 60 seconds...")
	time.Sleep(60 * time.Second)
	
	fmt.Println("\nüéâ Vector Database demo completed successfully!")
}