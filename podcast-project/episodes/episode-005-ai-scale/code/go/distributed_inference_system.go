/*
Distributed Inference System for AI at Scale
Episode 5: Go Implementation

Production-ready distributed inference system for Indian AI infrastructure
Optimized for high-throughput, low-latency serving with cost optimization

Author: Code Developer Agent
Context: Indian AI/ML production systems with multi-region deployment
*/

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"math"
	"math/rand"
	"net/http"
	"runtime"
	"sync"
	"sync/atomic"
	"time"

	"github.com/gorilla/mux"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
)

// Main inference system struct
// ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø AI companies ‡§ï‡•á ‡§≤‡§ø‡§è high-performance inference serving
type DistributedInferenceSystem struct {
	nodes           map[string]*InferenceNode
	loadBalancer    *LoadBalancer
	requestRouter   *RequestRouter
	metricsCollector *MetricsCollector
	healthChecker   *HealthChecker
	nodeManager     *NodeManager
	costOptimizer   *CostOptimizer
	mu              sync.RWMutex
	ctx             context.Context
	cancel          context.CancelFunc
}

// Individual inference node representing a model server
// Indian infrastructure ‡§ï‡•á ‡§∏‡§æ‡§• optimized node configuration
type InferenceNode struct {
	ID               string            `json:"id"`
	Region           string            `json:"region"`
	ModelName        string            `json:"model_name"`
	ModelVersion     string            `json:"model_version"`
	GPUType          string            `json:"gpu_type"`
	MaxConcurrent    int32             `json:"max_concurrent"`
	CurrentLoad      int32             `json:"current_load"`
	Endpoint         string            `json:"endpoint"`
	Status           NodeStatus        `json:"status"`
	LastHeartbeat    time.Time         `json:"last_heartbeat"`
	AvgLatencyMs     float64           `json:"avg_latency_ms"`
	SuccessRate      float64           `json:"success_rate"`
	CostPerRequestINR float64          `json:"cost_per_request_inr"`
	SupportedLangs   []string          `json:"supported_languages"`
	UseCase          string            `json:"use_case"`
	Company          string            `json:"company"`
	
	// Performance metrics
	totalRequests    int64
	successfulReqs   int64
	totalLatencyMs   int64
	lastUpdated      time.Time
	mu               sync.RWMutex
}

type NodeStatus int

const (
	NodeStatusHealthy NodeStatus = iota
	NodeStatusDegraded
	NodeStatusUnhealthy
	NodeStatusMaintenance
)

func (s NodeStatus) String() string {
	switch s {
	case NodeStatusHealthy:
		return "healthy"
	case NodeStatusDegraded:
		return "degraded"
	case NodeStatusUnhealthy:
		return "unhealthy"
	case NodeStatusMaintenance:
		return "maintenance"
	default:
		return "unknown"
	}
}

// Request structure for inference
type InferenceRequest struct {
	ModelID     string            `json:"model_id"`
	Input       string            `json:"input"`
	Language    string            `json:"language"`
	UserID      string            `json:"user_id"`
	Region      string            `json:"region"`
	Priority    RequestPriority   `json:"priority"`
	MaxLatency  int               `json:"max_latency_ms"`
	Metadata    map[string]string `json:"metadata"`
	Timestamp   time.Time         `json:"timestamp"`
}

type RequestPriority int

const (
	PriorityLow RequestPriority = iota
	PriorityNormal
	PriorityHigh
	PriorityCritical
)

// Response structure
type InferenceResponse struct {
	RequestID     string        `json:"request_id"`
	ModelID       string        `json:"model_id"`
	Output        string        `json:"output"`
	Confidence    float64       `json:"confidence"`
	LatencyMs     int64         `json:"latency_ms"`
	NodeID        string        `json:"node_id"`
	CostINR       float64       `json:"cost_inr"`
	Success       bool          `json:"success"`
	ErrorMessage  string        `json:"error_message,omitempty"`
	Metadata      map[string]string `json:"metadata"`
	ProcessedAt   time.Time     `json:"processed_at"`
}

// Load balancer for distributing requests across nodes
// Indian regions ‡§ï‡•á ‡§∏‡§æ‡§• intelligent routing
type LoadBalancer struct {
	strategy      LoadBalancingStrategy
	nodes         map[string]*InferenceNode
	regionWeights map[string]float64 // Regional preference weights
	mu            sync.RWMutex
}

type LoadBalancingStrategy int

const (
	StrategyRoundRobin LoadBalancingStrategy = iota
	StrategyLeastLoaded
	StrategyWeightedRandom
	StrategyGeographicAffinity
	StrategyLatencyOptimized
	StrategyCostOptimized
)

// Request router for intelligent request routing
type RequestRouter struct {
	routingRules  []RoutingRule
	fallbackNodes []string
	mu            sync.RWMutex
}

type RoutingRule struct {
	ModelID     string
	Language    string
	Region      string
	UseCase     string
	NodeIDs     []string
	Priority    RequestPriority
	Condition   func(*InferenceRequest) bool
	Weight      float64
}

// Metrics collector using Prometheus
// Indian infrastructure ‡§ï‡•á ‡§≤‡§ø‡§è comprehensive monitoring
type MetricsCollector struct {
	requestsTotal       *prometheus.CounterVec
	requestDuration     *prometheus.HistogramVec
	activeConnections   *prometheus.GaugeVec
	nodeHealth          *prometheus.GaugeVec
	costTracking        *prometheus.CounterVec
	languageDistribution *prometheus.CounterVec
	regionalLatency     *prometheus.HistogramVec
}

// Health checker for node monitoring
type HealthChecker struct {
	interval    time.Duration
	timeout     time.Duration
	retryCount  int
	unhealthyThreshold int
	mu          sync.RWMutex
}

// Node manager for dynamic node management
type NodeManager struct {
	autoScaling   bool
	minNodes      int
	maxNodes      int
	scaleUpThreshold   float64 // CPU/Memory threshold for scaling up
	scaleDownThreshold float64 // CPU/Memory threshold for scaling down
	cooldownPeriod     time.Duration
	lastScaleAction    time.Time
	mu                 sync.RWMutex
}

// Cost optimizer for Indian cloud infrastructure
// AWS Mumbai, Azure India, GCP Mumbai pricing optimization
type CostOptimizer struct {
	regionCosts   map[string]float64 // Cost per hour per region
	spotPricing   map[string]float64 // Spot instance discounts
	peakHours     []int              // Peak hours in IST
	budgetLimits  map[string]float64 // Daily budget limits
	currentCosts  map[string]float64 // Current daily costs
	mu            sync.RWMutex
}

// Regional costs in INR per hour for different Indian cloud regions
var DefaultRegionalCosts = map[string]float64{
	"ap-south-1":          50.0, // AWS Mumbai
	"azure-centralindia":  48.0, // Azure Central India
	"asia-south1":         45.0, // GCP Mumbai
	"ap-southeast-1":      55.0, // AWS Singapore (backup)
	"on-premise-mumbai":   25.0, // On-premise Mumbai
	"on-premise-bangalore": 22.0, // On-premise Bangalore
}

// Initialize the distributed inference system
func NewDistributedInferenceSystem() *DistributedInferenceSystem {
	ctx, cancel := context.WithCancel(context.Background())
	
	system := &DistributedInferenceSystem{
		nodes:           make(map[string]*InferenceNode),
		loadBalancer:    NewLoadBalancer(),
		requestRouter:   NewRequestRouter(),
		metricsCollector: NewMetricsCollector(),
		healthChecker:   NewHealthChecker(),
		nodeManager:     NewNodeManager(),
		costOptimizer:   NewCostOptimizer(),
		ctx:             ctx,
		cancel:          cancel,
	}
	
	// Start background processes
	go system.startHealthChecking()
	go system.startMetricsCollection()
	go system.startAutoScaling()
	go system.startCostOptimization()
	
	log.Println("üöÄ Distributed Inference System initialized for Indian AI infrastructure")
	return system
}

func NewLoadBalancer() *LoadBalancer {
	return &LoadBalancer{
		strategy:      StrategyLatencyOptimized, // Default to latency optimization for Indian users
		nodes:         make(map[string]*InferenceNode),
		regionWeights: map[string]float64{
			"ap-south-1":          1.0, // Prefer Mumbai region
			"azure-centralindia":  0.9,
			"asia-south1":         0.95,
			"ap-southeast-1":      0.3, // Lower preference for Singapore
			"on-premise-mumbai":   0.8,
			"on-premise-bangalore": 0.7,
		},
	}
}

func NewRequestRouter() *RequestRouter {
	return &RequestRouter{
		routingRules:  make([]RoutingRule, 0),
		fallbackNodes: make([]string, 0),
	}
}

func NewMetricsCollector() *MetricsCollector {
	collector := &MetricsCollector{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "inference_requests_total",
				Help: "Total number of inference requests processed",
			},
			[]string{"model_id", "node_id", "region", "language", "status", "company"},
		),
		
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "inference_request_duration_seconds",
				Help:    "Duration of inference requests",
				Buckets: prometheus.DefBuckets,
			},
			[]string{"model_id", "node_id", "region", "language", "company"},
		),
		
		activeConnections: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "inference_active_connections",
				Help: "Number of active connections per node",
			},
			[]string{"node_id", "region", "company"},
		),
		
		nodeHealth: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "inference_node_health",
				Help: "Health status of inference nodes (1=healthy, 0=unhealthy)",
			},
			[]string{"node_id", "region", "model_id", "company"},
		),
		
		costTracking: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "inference_cost_inr_total",
				Help: "Total cost incurred in INR",
			},
			[]string{"region", "node_id", "company"},
		),
		
		languageDistribution: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "inference_requests_by_language_total",
				Help: "Total requests processed by language",
			},
			[]string{"language", "region", "company"},
		),
		
		regionalLatency: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "inference_regional_latency_seconds",
				Help:    "Latency distribution by region",
				Buckets: []float64{0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0},
			},
			[]string{"region", "company"},
		),
	}
	
	// Register metrics with Prometheus
	prometheus.MustRegister(
		collector.requestsTotal,
		collector.requestDuration,
		collector.activeConnections,
		collector.nodeHealth,
		collector.costTracking,
		collector.languageDistribution,
		collector.regionalLatency,
	)
	
	return collector
}

func NewHealthChecker() *HealthChecker {
	return &HealthChecker{
		interval:           30 * time.Second, // Check every 30 seconds
		timeout:            5 * time.Second,  // 5 second timeout
		retryCount:         3,
		unhealthyThreshold: 3, // Mark unhealthy after 3 failed checks
	}
}

func NewNodeManager() *NodeManager {
	return &NodeManager{
		autoScaling:        true,
		minNodes:           2, // Minimum nodes for redundancy
		maxNodes:           20, // Maximum nodes for cost control
		scaleUpThreshold:   80.0, // Scale up when 80% utilized
		scaleDownThreshold: 20.0, // Scale down when below 20% utilized
		cooldownPeriod:     5 * time.Minute, // Wait 5 minutes between scale actions
	}
}

func NewCostOptimizer() *CostOptimizer {
	return &CostOptimizer{
		regionCosts:  DefaultRegionalCosts,
		spotPricing: map[string]float64{
			"ap-south-1":          0.7, // 30% discount for spot instances
			"azure-centralindia":  0.75,
			"asia-south1":         0.68,
			"ap-southeast-1":      0.72,
		},
		peakHours:    []int{9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, // 9 AM to 6 PM IST
		budgetLimits: make(map[string]float64),
		currentCosts: make(map[string]float64),
	}
}

// Register a new inference node
func (sys *DistributedInferenceSystem) RegisterNode(nodeConfig NodeConfig) error {
	sys.mu.Lock()
	defer sys.mu.Unlock()
	
	if _, exists := sys.nodes[nodeConfig.ID]; exists {
		return fmt.Errorf("node %s already exists", nodeConfig.ID)
	}
	
	node := &InferenceNode{
		ID:               nodeConfig.ID,
		Region:           nodeConfig.Region,
		ModelName:        nodeConfig.ModelName,
		ModelVersion:     nodeConfig.ModelVersion,
		GPUType:          nodeConfig.GPUType,
		MaxConcurrent:    int32(nodeConfig.MaxConcurrent),
		CurrentLoad:      0,
		Endpoint:         nodeConfig.Endpoint,
		Status:           NodeStatusHealthy,
		LastHeartbeat:    time.Now(),
		AvgLatencyMs:     0.0,
		SuccessRate:      100.0,
		CostPerRequestINR: nodeConfig.CostPerRequestINR,
		SupportedLangs:   nodeConfig.SupportedLanguages,
		UseCase:          nodeConfig.UseCase,
		Company:          nodeConfig.Company,
		totalRequests:    0,
		successfulReqs:   0,
		totalLatencyMs:   0,
		lastUpdated:      time.Now(),
	}
	
	sys.nodes[nodeConfig.ID] = node
	sys.loadBalancer.AddNode(node)
	
	// Set budget limit for cost optimization
	dailyBudgetINR := nodeConfig.DailyBudgetINR
	if dailyBudgetINR > 0 {
		sys.costOptimizer.SetBudgetLimit(nodeConfig.ID, dailyBudgetINR)
	}
	
	log.Printf("‚úÖ Registered node %s (%s) in region %s for %s - %s",
		nodeConfig.ID, nodeConfig.ModelName, nodeConfig.Region,
		nodeConfig.Company, nodeConfig.UseCase)
	
	return nil
}

type NodeConfig struct {
	ID                  string   `json:"id"`
	Region              string   `json:"region"`
	ModelName           string   `json:"model_name"`
	ModelVersion        string   `json:"model_version"`
	GPUType             string   `json:"gpu_type"`
	MaxConcurrent       int      `json:"max_concurrent"`
	Endpoint            string   `json:"endpoint"`
	CostPerRequestINR   float64  `json:"cost_per_request_inr"`
	SupportedLanguages  []string `json:"supported_languages"`
	UseCase             string   `json:"use_case"`
	Company             string   `json:"company"`
	DailyBudgetINR      float64  `json:"daily_budget_inr"`
}

// Process inference request
func (sys *DistributedInferenceSystem) ProcessInference(req *InferenceRequest) (*InferenceResponse, error) {
	startTime := time.Now()
	requestID := fmt.Sprintf("req_%d_%d", time.Now().Unix(), rand.Int63())
	
	// Find optimal node for the request
	node, err := sys.findOptimalNode(req)
	if err != nil {
		return &InferenceResponse{
			RequestID:    requestID,
			Success:      false,
			ErrorMessage: fmt.Sprintf("No available nodes: %v", err),
			ProcessedAt:  time.Now(),
		}, err
	}
	
	// Check if node can handle the request
	if !atomic.CompareAndSwapInt32(&node.CurrentLoad, node.CurrentLoad, node.CurrentLoad+1) {
		if node.CurrentLoad >= node.MaxConcurrent {
			return &InferenceResponse{
				RequestID:    requestID,
				Success:      false,
				ErrorMessage: "Node at maximum capacity",
				ProcessedAt:  time.Now(),
			}, fmt.Errorf("node %s at maximum capacity", node.ID)
		}
	}
	
	defer func() {
		atomic.AddInt32(&node.CurrentLoad, -1)
	}()
	
	// Process the request (simulate model inference)
	response, err := sys.executeInference(node, req, requestID)
	if err != nil {
		return response, err
	}
	
	// Update metrics
	latency := time.Since(startTime)
	sys.updateNodeMetrics(node, latency, response.Success)
	sys.recordMetrics(node, req, response, latency)
	
	return response, nil
}

// Find optimal node based on multiple factors
func (sys *DistributedInferenceSystem) findOptimalNode(req *InferenceRequest) (*InferenceNode, error) {
	sys.mu.RLock()
	defer sys.mu.RUnlock()
	
	candidateNodes := make([]*InferenceNode, 0)
	
	// Filter nodes by model capability and health
	for _, node := range sys.nodes {
		if node.Status != NodeStatusHealthy && node.Status != NodeStatusDegraded {
			continue
		}
		
		// Check if node supports the required language
		if req.Language != "" && !contains(node.SupportedLangs, req.Language) {
			continue
		}
		
		// Check capacity
		if node.CurrentLoad >= node.MaxConcurrent {
			continue
		}
		
		candidateNodes = append(candidateNodes, node)
	}
	
	if len(candidateNodes) == 0 {
		return nil, fmt.Errorf("no available nodes for request")
	}
	
	// Select best node based on strategy
	return sys.selectBestNode(candidateNodes, req)
}

func (sys *DistributedInferenceSystem) selectBestNode(nodes []*InferenceNode, req *InferenceRequest) (*InferenceNode, error) {
	if len(nodes) == 0 {
		return nil, fmt.Errorf("no candidate nodes")
	}
	
	switch sys.loadBalancer.strategy {
	case StrategyLeastLoaded:
		return sys.selectLeastLoadedNode(nodes), nil
		
	case StrategyLatencyOptimized:
		return sys.selectLowestLatencyNode(nodes), nil
		
	case StrategyCostOptimized:
		return sys.selectCheapestNode(nodes), nil
		
	case StrategyGeographicAffinity:
		return sys.selectRegionalNode(nodes, req.Region), nil
		
	default:
		// Round robin fallback
		return nodes[rand.Intn(len(nodes))], nil
	}
}

func (sys *DistributedInferenceSystem) selectLeastLoadedNode(nodes []*InferenceNode) *InferenceNode {
	var bestNode *InferenceNode
	lowestLoad := int32(math.MaxInt32)
	
	for _, node := range nodes {
		if node.CurrentLoad < lowestLoad {
			lowestLoad = node.CurrentLoad
			bestNode = node
		}
	}
	
	return bestNode
}

func (sys *DistributedInferenceSystem) selectLowestLatencyNode(nodes []*InferenceNode) *InferenceNode {
	var bestNode *InferenceNode
	lowestLatency := math.MaxFloat64
	
	for _, node := range nodes {
		if node.AvgLatencyMs < lowestLatency {
			lowestLatency = node.AvgLatencyMs
			bestNode = node
		}
	}
	
	return bestNode
}

func (sys *DistributedInferenceSystem) selectCheapestNode(nodes []*InferenceNode) *InferenceNode {
	var bestNode *InferenceNode
	lowestCost := math.MaxFloat64
	
	for _, node := range nodes {
		if node.CostPerRequestINR < lowestCost {
			lowestCost = node.CostPerRequestINR
			bestNode = node
		}
	}
	
	return bestNode
}

func (sys *DistributedInferenceSystem) selectRegionalNode(nodes []*InferenceNode, preferredRegion string) *InferenceNode {
	// First try to find a node in the preferred region
	for _, node := range nodes {
		if node.Region == preferredRegion {
			return node
		}
	}
	
	// Fallback to any available node
	return nodes[0]
}

// Execute inference on selected node (simulate model inference)
func (sys *DistributedInferenceSystem) executeInference(node *InferenceNode, req *InferenceRequest, requestID string) (*InferenceResponse, error) {
	startTime := time.Now()
	
	// Simulate different inference times based on model type and input
	baseLatency := sys.calculateBaseLatency(node, req)
	
	// Add some randomness to simulate real-world variance
	actualLatency := time.Duration(float64(baseLatency) * (0.8 + rand.Float64()*0.4))
	
	// Simulate processing time
	time.Sleep(actualLatency)
	
	// Simulate occasional failures (2% failure rate)
	success := rand.Float64() > 0.02
	
	var output string
	var confidence float64
	var errorMsg string
	
	if success {
		output = sys.generateMockOutput(node, req)
		confidence = 0.85 + rand.Float64()*0.15 // 85-100% confidence
	} else {
		errorMsg = "Model inference failed"
		confidence = 0.0
	}
	
	processingTime := time.Since(startTime)
	
	response := &InferenceResponse{
		RequestID:     requestID,
		ModelID:       node.ID,
		Output:        output,
		Confidence:    confidence,
		LatencyMs:     processingTime.Milliseconds(),
		NodeID:        node.ID,
		CostINR:       node.CostPerRequestINR,
		Success:       success,
		ErrorMessage:  errorMsg,
		Metadata: map[string]string{
			"region":     node.Region,
			"gpu_type":   node.GPUType,
			"model_version": node.ModelVersion,
		},
		ProcessedAt:   time.Now(),
	}
	
	return response, nil
}

func (sys *DistributedInferenceSystem) calculateBaseLatency(node *InferenceNode, req *InferenceRequest) time.Duration {
	// Base latency depends on model type and complexity
	baseLatency := 100 * time.Millisecond // Default 100ms
	
	// Adjust based on GPU type
	switch node.GPUType {
	case "RTX_3060", "RTX_3070":
		baseLatency = 200 * time.Millisecond
	case "RTX_4080", "RTX_4090":
		baseLatency = 120 * time.Millisecond
	case "A100", "H100":
		baseLatency = 80 * time.Millisecond
	}
	
	// Adjust based on input size (rough estimate)
	inputSize := len(req.Input)
	if inputSize > 1000 {
		baseLatency += time.Duration(inputSize/100) * time.Millisecond
	}
	
	// Adjust based on current load
	loadFactor := float64(node.CurrentLoad) / float64(node.MaxConcurrent)
	if loadFactor > 0.7 {
		baseLatency = time.Duration(float64(baseLatency) * (1.0 + loadFactor))
	}
	
	return baseLatency
}

func (sys *DistributedInferenceSystem) generateMockOutput(node *InferenceNode, req *InferenceRequest) string {
	// Generate mock output based on use case
	switch node.UseCase {
	case "fraud_detection":
		if rand.Float64() > 0.8 {
			return "FRAUD_DETECTED: High risk transaction identified"
		}
		return "LEGITIMATE: Transaction appears normal"
		
	case "recommendation":
		return fmt.Sprintf("RECOMMENDED_ITEMS: [item_1, item_2, item_3] based on user preferences")
		
	case "text_classification":
		categories := []string{"positive", "negative", "neutral"}
		return fmt.Sprintf("CLASSIFICATION: %s", categories[rand.Intn(len(categories))])
		
	case "image_classification":
		foods := []string{"pizza", "burger", "biryani", "dosa", "samosa"}
		return fmt.Sprintf("DETECTED_FOOD: %s", foods[rand.Intn(len(foods))])
		
	default:
		return "PROCESSED: Input successfully processed by model"
	}
}

// Update node performance metrics
func (sys *DistributedInferenceSystem) updateNodeMetrics(node *InferenceNode, latency time.Duration, success bool) {
	node.mu.Lock()
	defer node.mu.Unlock()
	
	node.totalRequests++
	if success {
		node.successfulReqs++
	}
	
	node.totalLatencyMs += latency.Milliseconds()
	node.AvgLatencyMs = float64(node.totalLatencyMs) / float64(node.totalRequests)
	node.SuccessRate = (float64(node.successfulReqs) / float64(node.totalRequests)) * 100
	node.lastUpdated = time.Now()
}

// Record metrics for monitoring
func (sys *DistributedInferenceSystem) recordMetrics(node *InferenceNode, req *InferenceRequest, resp *InferenceResponse, latency time.Duration) {
	status := "success"
	if !resp.Success {
		status = "error"
	}
	
	// Record request metrics
	sys.metricsCollector.requestsTotal.WithLabelValues(
		node.ID, node.ID, node.Region, req.Language, status, node.Company,
	).Inc()
	
	// Record latency metrics
	sys.metricsCollector.requestDuration.WithLabelValues(
		node.ID, node.ID, node.Region, req.Language, node.Company,
	).Observe(latency.Seconds())
	
	// Record regional latency
	sys.metricsCollector.regionalLatency.WithLabelValues(
		node.Region, node.Company,
	).Observe(latency.Seconds())
	
	// Record language distribution
	sys.metricsCollector.languageDistribution.WithLabelValues(
		req.Language, node.Region, node.Company,
	).Inc()
	
	// Record cost
	sys.metricsCollector.costTracking.WithLabelValues(
		node.Region, node.ID, node.Company,
	).Add(resp.CostINR)
	
	// Update active connections
	sys.metricsCollector.activeConnections.WithLabelValues(
		node.ID, node.Region, node.Company,
	).Set(float64(node.CurrentLoad))
}

// Background health checking
func (sys *DistributedInferenceSystem) startHealthChecking() {
	ticker := time.NewTicker(sys.healthChecker.interval)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			sys.performHealthChecks()
		case <-sys.ctx.Done():
			return
		}
	}
}

func (sys *DistributedInferenceSystem) performHealthChecks() {
	sys.mu.RLock()
	nodes := make([]*InferenceNode, 0, len(sys.nodes))
	for _, node := range sys.nodes {
		nodes = append(nodes, node)
	}
	sys.mu.RUnlock()
	
	for _, node := range nodes {
		go sys.checkNodeHealth(node)
	}
}

func (sys *DistributedInferenceSystem) checkNodeHealth(node *InferenceNode) {
	// Simple health check - in production would ping the actual endpoint
	healthy := true
	
	// Check if node hasn't been updated recently
	if time.Since(node.lastUpdated) > 2*time.Minute {
		healthy = false
	}
	
	// Check success rate
	if node.SuccessRate < 90.0 && node.totalRequests > 10 {
		healthy = false
	}
	
	// Check latency
	if node.AvgLatencyMs > 2000.0 && node.totalRequests > 10 { // 2 seconds threshold
		healthy = false
	}
	
	// Update node status
	node.mu.Lock()
	previousStatus := node.Status
	
	if healthy {
		if node.Status == NodeStatusUnhealthy || node.Status == NodeStatusDegraded {
			node.Status = NodeStatusHealthy
		}
	} else {
		if node.Status == NodeStatusHealthy {
			node.Status = NodeStatusDegraded
		} else if node.Status == NodeStatusDegraded {
			node.Status = NodeStatusUnhealthy
		}
	}
	
	node.LastHeartbeat = time.Now()
	node.mu.Unlock()
	
	// Log status changes
	if previousStatus != node.Status {
		log.Printf("Node %s status changed: %s -> %s", 
			node.ID, previousStatus.String(), node.Status.String())
	}
	
	// Update Prometheus metrics
	healthValue := 0.0
	if node.Status == NodeStatusHealthy {
		healthValue = 1.0
	}
	
	sys.metricsCollector.nodeHealth.WithLabelValues(
		node.ID, node.Region, node.ModelName, node.Company,
	).Set(healthValue)
}

// Background metrics collection
func (sys *DistributedInferenceSystem) startMetricsCollection() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			sys.collectSystemMetrics()
		case <-sys.ctx.Done():
			return
		}
	}
}

func (sys *DistributedInferenceSystem) collectSystemMetrics() {
	// Collect system-level metrics
	var memStats runtime.MemStats
	runtime.ReadMemStats(&memStats)
	
	// Log system status
	sys.mu.RLock()
	totalNodes := len(sys.nodes)
	healthyNodes := 0
	totalRequests := int64(0)
	
	for _, node := range sys.nodes {
		if node.Status == NodeStatusHealthy {
			healthyNodes++
		}
		totalRequests += node.totalRequests
	}
	sys.mu.RUnlock()
	
	log.Printf("üìä System Status - Nodes: %d/%d healthy, Total requests: %d, Memory: %.2f MB",
		healthyNodes, totalNodes, totalRequests, float64(memStats.Alloc)/1024/1024)
}

// Background auto-scaling
func (sys *DistributedInferenceSystem) startAutoScaling() {
	if !sys.nodeManager.autoScaling {
		return
	}
	
	ticker := time.NewTicker(1 * time.Minute)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			sys.performAutoScaling()
		case <-sys.ctx.Done():
			return
		}
	}
}

func (sys *DistributedInferenceSystem) performAutoScaling() {
	sys.nodeManager.mu.Lock()
	defer sys.nodeManager.mu.Unlock()
	
	// Check if we're in cooldown period
	if time.Since(sys.nodeManager.lastScaleAction) < sys.nodeManager.cooldownPeriod {
		return
	}
	
	sys.mu.RLock()
	healthyNodes := 0
	totalLoad := int32(0)
	totalCapacity := int32(0)
	
	for _, node := range sys.nodes {
		if node.Status == NodeStatusHealthy {
			healthyNodes++
			totalLoad += node.CurrentLoad
			totalCapacity += node.MaxConcurrent
		}
	}
	sys.mu.RUnlock()
	
	if totalCapacity == 0 {
		return
	}
	
	utilizationPercent := (float64(totalLoad) / float64(totalCapacity)) * 100
	
	// Scale up if utilization is high
	if utilizationPercent > sys.nodeManager.scaleUpThreshold && healthyNodes < sys.nodeManager.maxNodes {
		log.Printf("üîº Auto-scaling UP triggered - Utilization: %.1f%%", utilizationPercent)
		// In production, this would trigger node provisioning
		sys.nodeManager.lastScaleAction = time.Now()
	}
	
	// Scale down if utilization is low
	if utilizationPercent < sys.nodeManager.scaleDownThreshold && healthyNodes > sys.nodeManager.minNodes {
		log.Printf("üîΩ Auto-scaling DOWN triggered - Utilization: %.1f%%", utilizationPercent)
		// In production, this would trigger node termination
		sys.nodeManager.lastScaleAction = time.Now()
	}
}

// Background cost optimization
func (sys *DistributedInferenceSystem) startCostOptimization() {
	ticker := time.NewTicker(15 * time.Minute) // Check every 15 minutes
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			sys.performCostOptimization()
		case <-sys.ctx.Done():
			return
		}
	}
}

func (sys *DistributedInferenceSystem) performCostOptimization() {
	sys.costOptimizer.mu.Lock()
	defer sys.costOptimizer.mu.Unlock()
	
	currentHour := time.Now().Hour()
	
	// Check if we're in peak hours (Indian business hours)
	isPeakHour := contains(sys.costOptimizer.peakHours, currentHour)
	
	if !isPeakHour {
		// During off-peak hours, prefer cheaper regions and spot instances
		log.Println("üí∞ Off-peak optimization: Preferring cost-optimized routing")
		sys.loadBalancer.strategy = StrategyCostOptimized
	} else {
		// During peak hours, prefer performance
		log.Println("‚ö° Peak-hour optimization: Preferring performance-optimized routing")
		sys.loadBalancer.strategy = StrategyLatencyOptimized
	}
	
	// Check budget limits
	sys.mu.RLock()
	for _, node := range sys.nodes {
		dailyBudget := sys.costOptimizer.budgetLimits[node.ID]
		if dailyBudget > 0 {
			currentCost := sys.costOptimizer.currentCosts[node.ID]
			if currentCost > dailyBudget*0.9 { // Alert at 90% of budget
				log.Printf("‚ö†Ô∏è Budget alert for node %s: ‚Çπ%.2f / ‚Çπ%.2f (%.1f%%)",
					node.ID, currentCost, dailyBudget, (currentCost/dailyBudget)*100)
			}
		}
	}
	sys.mu.RUnlock()
}

// Set budget limit for cost optimization
func (co *CostOptimizer) SetBudgetLimit(nodeID string, dailyBudgetINR float64) {
	co.mu.Lock()
	defer co.mu.Unlock()
	co.budgetLimits[nodeID] = dailyBudgetINR
}

// Add node to load balancer
func (lb *LoadBalancer) AddNode(node *InferenceNode) {
	lb.mu.Lock()
	defer lb.mu.Unlock()
	lb.nodes[node.ID] = node
}

// Get system status
func (sys *DistributedInferenceSystem) GetSystemStatus() map[string]interface{} {
	sys.mu.RLock()
	defer sys.mu.RUnlock()
	
	status := make(map[string]interface{})
	
	// Node statistics
	totalNodes := len(sys.nodes)
	healthyNodes := 0
	degradedNodes := 0
	unhealthyNodes := 0
	totalCapacity := int32(0)
	totalLoad := int32(0)
	totalRequests := int64(0)
	totalCost := 0.0
	
	nodeDetails := make([]map[string]interface{}, 0, totalNodes)
	
	for _, node := range sys.nodes {
		switch node.Status {
		case NodeStatusHealthy:
			healthyNodes++
		case NodeStatusDegraded:
			degradedNodes++
		case NodeStatusUnhealthy:
			unhealthyNodes++
		}
		
		totalCapacity += node.MaxConcurrent
		totalLoad += node.CurrentLoad
		totalRequests += node.totalRequests
		totalCost += float64(node.totalRequests) * node.CostPerRequestINR
		
		nodeDetail := map[string]interface{}{
			"id":                node.ID,
			"region":            node.Region,
			"company":           node.Company,
			"model":             node.ModelName,
			"status":            node.Status.String(),
			"current_load":      node.CurrentLoad,
			"max_concurrent":    node.MaxConcurrent,
			"avg_latency_ms":    node.AvgLatencyMs,
			"success_rate":      node.SuccessRate,
			"total_requests":    node.totalRequests,
			"cost_per_req_inr":  node.CostPerRequestINR,
			"supported_langs":   node.SupportedLangs,
			"use_case":          node.UseCase,
		}
		nodeDetails = append(nodeDetails, nodeDetail)
	}
	
	// Overall statistics
	utilizationPercent := 0.0
	if totalCapacity > 0 {
		utilizationPercent = (float64(totalLoad) / float64(totalCapacity)) * 100
	}
	
	status["summary"] = map[string]interface{}{
		"total_nodes":         totalNodes,
		"healthy_nodes":       healthyNodes,
		"degraded_nodes":      degradedNodes,
		"unhealthy_nodes":     unhealthyNodes,
		"utilization_percent": utilizationPercent,
		"total_requests":      totalRequests,
		"total_cost_inr":      totalCost,
		"load_balancer_strategy": sys.loadBalancer.strategy,
		"auto_scaling_enabled": sys.nodeManager.autoScaling,
	}
	
	status["nodes"] = nodeDetails
	status["timestamp"] = time.Now()
	
	return status
}

// HTTP handlers for API endpoints
func (sys *DistributedInferenceSystem) handleInference(w http.ResponseWriter, r *http.Request) {
	var req InferenceRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}
	
	req.Timestamp = time.Now()
	
	response, err := sys.ProcessInference(&req)
	if err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}
	
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

func (sys *DistributedInferenceSystem) handleStatus(w http.ResponseWriter, r *http.Request) {
	status := sys.GetSystemStatus()
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(status)
}

func (sys *DistributedInferenceSystem) handleHealth(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusOK)
	json.NewEncoder(w).Encode(map[string]string{
		"status": "healthy",
		"timestamp": time.Now().Format(time.RFC3339),
	})
}

// Start HTTP server
func (sys *DistributedInferenceSystem) StartHTTPServer(port string) {
	router := mux.NewRouter()
	
	// API endpoints
	router.HandleFunc("/inference", sys.handleInference).Methods("POST")
	router.HandleFunc("/status", sys.handleStatus).Methods("GET")
	router.HandleFunc("/health", sys.handleHealth).Methods("GET")
	
	// Prometheus metrics endpoint
	router.Handle("/metrics", promhttp.Handler())
	
	// Static files for dashboard
	router.PathPrefix("/").Handler(http.FileServer(http.Dir("./static/")))
	
	log.Printf("üåê HTTP server starting on port %s", port)
	log.Printf("üìä Metrics available at http://localhost:%s/metrics", port)
	log.Printf("üíª Dashboard available at http://localhost:%s/status", port)
	
	log.Fatal(http.ListenAndServe(":"+port, router))
}

// Shutdown system gracefully
func (sys *DistributedInferenceSystem) Shutdown() {
	log.Println("üõë Shutting down Distributed Inference System...")
	
	sys.cancel() // Cancel background processes
	
	log.Println("‚úÖ Distributed Inference System shutdown complete")
}

// Utility functions
func contains(slice []string, item string) bool {
	for _, s := range slice {
		if s == item {
			return true
		}
	}
	return false
}

func containsInt(slice []int, item int) bool {
	for _, s := range slice {
		if s == item {
			return true
		}
	}
	return false
}

// Demo function showing typical usage for Indian AI companies
func main() {
	fmt.Println("üöÄ Distributed Inference System Demo - Indian AI Infrastructure")
	fmt.Println(strings.Repeat("=", 70))
	
	// Initialize system
	system := NewDistributedInferenceSystem()
	defer system.Shutdown()
	
	// Register nodes for different Indian companies
	nodeConfigs := []NodeConfig{
		{
			ID:                 "paytm-mumbai-fraud-01",
			Region:             "ap-south-1",
			ModelName:          "PayTM Fraud Detection",
			ModelVersion:       "v2.1",
			GPUType:            "RTX_4080",
			MaxConcurrent:      50,
			Endpoint:           "https://api.paytm.com/ml/fraud",
			CostPerRequestINR:  0.08,
			SupportedLanguages: []string{"hindi", "english"},
			UseCase:            "fraud_detection",
			Company:            "paytm",
			DailyBudgetINR:     5000.0,
		},
		{
			ID:                 "flipkart-bangalore-recom-01",
			Region:             "azure-centralindia",
			ModelName:          "Flipkart Recommendation Engine",
			ModelVersion:       "v3.2",
			GPUType:            "A100",
			MaxConcurrent:      80,
			Endpoint:           "https://api.flipkart.com/ml/recommend",
			CostPerRequestINR:  0.25,
			SupportedLanguages: []string{"hindi", "english", "tamil", "bengali"},
			UseCase:            "recommendation",
			Company:            "flipkart",
			DailyBudgetINR:     8000.0,
		},
		{
			ID:                 "zomato-mumbai-food-01",
			Region:             "ap-south-1",
			ModelName:          "Zomato Food Classifier",
			ModelVersion:       "v1.8",
			GPUType:            "RTX_4090",
			MaxConcurrent:      30,
			Endpoint:           "https://api.zomato.com/ml/classify",
			CostPerRequestINR:  0.15,
			SupportedLanguages: []string{"hindi", "english"},
			UseCase:            "image_classification",
			Company:            "zomato",
			DailyBudgetINR:     3000.0,
		},
		{
			ID:                 "ola-onprem-routing-01",
			Region:             "on-premise-bangalore",
			ModelName:          "Ola Route Optimization",
			ModelVersion:       "v2.5",
			GPUType:            "RTX_3080",
			MaxConcurrent:      40,
			Endpoint:           "https://api.olacabs.com/ml/routing",
			CostPerRequestINR:  0.05,
			SupportedLanguages: []string{"hindi", "english", "kannada"},
			UseCase:            "route_optimization",
			Company:            "ola",
			DailyBudgetINR:     2000.0,
		},
	}
	
	// Register all nodes
	for _, config := range nodeConfigs {
		err := system.RegisterNode(config)
		if err != nil {
			log.Printf("‚ùå Failed to register node %s: %v", config.ID, err)
		}
	}
	
	fmt.Println("\n‚úÖ Registered 4 inference nodes across Indian companies")
	
	// Simulate inference requests
	fmt.Println("\nüîÑ Simulating inference requests...")
	
	testRequests := []InferenceRequest{
		{
			ModelID:    "paytm-mumbai-fraud-01",
			Input:      "Transaction amount: ‚Çπ50,000, Merchant: Electronics Store, Time: 2:30 AM",
			Language:   "english",
			UserID:     "user_123",
			Region:     "ap-south-1",
			Priority:   PriorityHigh,
			MaxLatency: 200,
			Metadata:   map[string]string{"transaction_id": "txn_456"},
		},
		{
			ModelID:    "flipkart-bangalore-recom-01", 
			Input:      "User viewed smartphones, laptops, and headphones in last 7 days",
			Language:   "hindi",
			UserID:     "user_789",
			Region:     "azure-centralindia",
			Priority:   PriorityNormal,
			MaxLatency: 500,
			Metadata:   map[string]string{"user_segment": "premium"},
		},
		{
			ModelID:    "zomato-mumbai-food-01",
			Input:      "Image data: /images/food_item_123.jpg",
			Language:   "english",
			UserID:     "user_456",
			Region:     "ap-south-1",
			Priority:   PriorityNormal,
			MaxLatency: 800,
			Metadata:   map[string]string{"restaurant_id": "rest_789"},
		},
	}
	
	// Process test requests concurrently
	var wg sync.WaitGroup
	for i, req := range testRequests {
		wg.Add(1)
		go func(index int, request InferenceRequest) {
			defer wg.Done()
			
			response, err := system.ProcessInference(&request)
			if err != nil {
				fmt.Printf("‚ùå Request %d failed: %v\n", index+1, err)
				return
			}
			
			fmt.Printf("‚úÖ Request %d processed:\n", index+1)
			fmt.Printf("   Model: %s\n", response.NodeID)
			fmt.Printf("   Latency: %dms\n", response.LatencyMs)
			fmt.Printf("   Success: %t\n", response.Success)
			fmt.Printf("   Cost: ‚Çπ%.4f\n", response.CostINR)
			fmt.Printf("   Output: %s\n\n", response.Output)
		}(i, req)
	}
	
	wg.Wait()
	
	// Generate load for demonstration
	fmt.Println("üî• Generating load for 30 seconds...")
	
	loadCtx, loadCancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer loadCancel()
	
	// Generate concurrent requests
	for i := 0; i < 10; i++ {
		go func(workerID int) {
			for {
				select {
				case <-loadCtx.Done():
					return
				default:
					// Random request
					req := testRequests[rand.Intn(len(testRequests))]
					req.UserID = fmt.Sprintf("load_user_%d", rand.Intn(1000))
					
					_, err := system.ProcessInference(&req)
					if err != nil {
						log.Printf("Load test error: %v", err)
					}
					
					time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
				}
			}
		}(i)
	}
	
	// Wait for load generation to complete
	<-loadCtx.Done()
	
	// Show final system status
	fmt.Println("\nüìä Final System Status:")
	status := system.GetSystemStatus()
	
	summary := status["summary"].(map[string]interface{})
	fmt.Printf("Total Nodes: %v\n", summary["total_nodes"])
	fmt.Printf("Healthy Nodes: %v\n", summary["healthy_nodes"])
	fmt.Printf("Utilization: %.1f%%\n", summary["utilization_percent"])
	fmt.Printf("Total Requests: %v\n", summary["total_requests"])
	fmt.Printf("Total Cost: ‚Çπ%.2f\n", summary["total_cost_inr"])
	
	fmt.Println("\nüìà Node Performance:")
	nodes := status["nodes"].([]map[string]interface{})
	for _, node := range nodes {
		fmt.Printf("   %s (%s):\n", node["id"], node["company"])
		fmt.Printf("      Status: %s | Load: %v/%v | Latency: %.1fms\n",
			node["status"], node["current_load"], node["max_concurrent"], node["avg_latency_ms"])
		fmt.Printf("      Requests: %v | Success Rate: %.1f%% | Cost: ‚Çπ%.4f/req\n",
			node["total_requests"], node["success_rate"], node["cost_per_req_inr"])
		fmt.Printf("      Use Case: %s | Languages: %v\n\n",
			node["use_case"], node["supported_langs"])
	}
	
	fmt.Println("üéØ Indian AI Infrastructure Features:")
	fmt.Println("   ‚úÖ Multi-region deployment (Mumbai, Bangalore, Singapore)")
	fmt.Println("   ‚úÖ Multi-language support (Hindi, English, Tamil, Bengali, Kannada)")
	fmt.Println("   ‚úÖ Cost optimization with INR pricing")
	fmt.Println("   ‚úÖ Intelligent load balancing and routing")
	fmt.Println("   ‚úÖ Real-time health monitoring and auto-recovery")
	fmt.Println("   ‚úÖ Performance-based node selection")
	fmt.Println("   ‚úÖ Budget tracking and cost alerts")
	fmt.Println("   ‚úÖ Auto-scaling based on demand")
	fmt.Println("   ‚úÖ Prometheus metrics integration")
	fmt.Println("   ‚úÖ Production-ready error handling")
	
	// Start HTTP server in background for demonstration
	fmt.Println("\nüåê Starting HTTP server on port 8080...")
	fmt.Println("   API endpoint: http://localhost:8080/inference")
	fmt.Println("   Status: http://localhost:8080/status")
	fmt.Println("   Metrics: http://localhost:8080/metrics")
	fmt.Println("   Health: http://localhost:8080/health")
	
	// Start server in goroutine
	go system.StartHTTPServer("8080")
	
	// Keep demo running for a bit longer
	fmt.Println("\n‚è∞ Demo running for 60 seconds...")
	time.Sleep(60 * time.Second)
	
	fmt.Println("\nüéâ Demo completed successfully!")
}