#!/usr/bin/env python3
"""
Transformer Attention Mechanism Optimized for Indian Languages
Episode 5: Code Example 4

Production-ready attention implementation with optimizations for:
- Hindi/Devanagari script processing
- Code-mixed text (Hindi+English)
- Multi-head attention with positional encoding
- Flash attention optimization for cost efficiency

Author: Code Developer Agent
Context: Indian language models at ChatGPT scale
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
import math
import time
import numpy as np
from typing import Optional, Tuple, Dict, List
from dataclasses import dataclass
import logging
from enum import Enum

# Production logging for Mumbai-style debugging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AttentionType(Enum):
    STANDARD = "standard"
    FLASH = "flash" 
    SPARSE = "sparse"
    SLIDING_WINDOW = "sliding_window"

@dataclass
class AttentionConfig:
    """Configuration for attention mechanism"""
    d_model: int = 768
    n_heads: int = 12
    dropout: float = 0.1
    max_seq_length: int = 2048
    attention_type: AttentionType = AttentionType.STANDARD
    
    # Indian language specific optimizations
    supports_devanagari: bool = True
    code_mixed_support: bool = True
    sliding_window_size: int = 512  # For long Hindi texts
    
    # Cost optimization
    use_gradient_checkpointing: bool = True
    mixed_precision: bool = True
    cost_per_token_inr: float = 0.00001  # ‚Çπ0.00001 per token

class PositionalEncoding(nn.Module):
    """
    Positional encoding optimized for Indian languages
    Handles variable character lengths in Devanagari script
    """
    
    def __init__(self, d_model: int, max_seq_length: int = 2048, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
        self.max_seq_length = max_seq_length
        
        # Create positional encoding matrix
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        
        # Use different frequencies for different script types
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
        # Devanagari-specific adjustments
        self.script_embeddings = nn.Embedding(4, d_model)  # latin, devanagari, mixed, other
        
    def forward(self, x: torch.Tensor, script_types: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Add positional encoding with script-aware adjustments
        
        Args:
            x: Input embeddings [batch_size, seq_len, d_model]
            script_types: Script type indicators [batch_size, seq_len] (optional)
        """
        batch_size, seq_len, d_model = x.size()
        
        # Standard positional encoding
        pos_encoding = self.pe[:, :seq_len, :].expand(batch_size, -1, -1)
        
        # Add script-specific encoding if available
        if script_types is not None:
            script_encoding = self.script_embeddings(script_types)
            pos_encoding = pos_encoding + script_encoding
        
        x = x + pos_encoding
        return self.dropout(x)

class FlashAttention(nn.Module):
    """
    Flash Attention implementation for memory efficiency
    Optimized for long Hindi texts and cost reduction
    """
    
    def __init__(self, config: AttentionConfig):
        super().__init__()
        self.config = config
        self.head_dim = config.d_model // config.n_heads
        self.scale = self.head_dim ** -0.5
        
        # Tile size for flash attention (optimized for V100/A100)
        self.tile_size = 128
        
    def forward(self, 
                query: torch.Tensor,
                key: torch.Tensor, 
                value: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Flash attention forward pass
        
        Args:
            query, key, value: [batch_size, n_heads, seq_len, head_dim]
            attention_mask: [batch_size, seq_len] or None
        """
        batch_size, n_heads, seq_len, head_dim = query.shape
        
        if seq_len <= self.tile_size:
            # Use standard attention for short sequences
            return self._standard_attention(query, key, value, attention_mask)
        
        # Flash attention for long sequences
        return self._flash_attention_tiled(query, key, value, attention_mask)
    
    def _standard_attention(self, 
                          query: torch.Tensor,
                          key: torch.Tensor,
                          value: torch.Tensor,
                          attention_mask: Optional[torch.Tensor]) -> torch.Tensor:
        """Standard scaled dot-product attention"""
        
        # Compute attention scores
        scores = torch.matmul(query, key.transpose(-2, -1)) * self.scale
        
        # Apply mask if provided
        if attention_mask is not None:
            # Expand mask to match score dimensions
            mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]
            mask = mask.expand(-1, n_heads, seq_len, -1)     # [batch, heads, seq_len, seq_len]
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # Apply softmax
        attention_probs = F.softmax(scores, dim=-1)
        
        # Apply dropout
        attention_probs = F.dropout(attention_probs, p=self.config.dropout, training=self.training)
        
        # Apply attention to values
        context = torch.matmul(attention_probs, value)
        
        return context
    
    def _flash_attention_tiled(self,
                             query: torch.Tensor,
                             key: torch.Tensor, 
                             value: torch.Tensor,
                             attention_mask: Optional[torch.Tensor]) -> torch.Tensor:
        """
        Tiled flash attention for memory efficiency
        Breaks computation into tiles to reduce memory usage
        """
        batch_size, n_heads, seq_len, head_dim = query.shape
        tile_size = self.tile_size
        
        # Initialize output
        output = torch.zeros_like(query)
        
        # Process in tiles
        for i in range(0, seq_len, tile_size):
            end_i = min(i + tile_size, seq_len)
            
            # Query tile
            q_tile = query[:, :, i:end_i, :]  # [batch, heads, tile_size, head_dim]
            
            # Initialize tile output and normalization
            tile_output = torch.zeros_like(q_tile)
            tile_max = torch.full((batch_size, n_heads, end_i - i, 1), 
                                float('-inf'), device=query.device)
            tile_sum = torch.zeros((batch_size, n_heads, end_i - i, 1), device=query.device)
            
            for j in range(0, seq_len, tile_size):
                end_j = min(j + tile_size, seq_len)
                
                # Key-Value tiles
                k_tile = key[:, :, j:end_j, :]
                v_tile = value[:, :, j:end_j, :]
                
                # Compute attention scores for this tile pair
                scores = torch.matmul(q_tile, k_tile.transpose(-2, -1)) * self.scale
                
                # Apply mask if needed
                if attention_mask is not None:
                    mask_tile = attention_mask[:, j:end_j].unsqueeze(1).unsqueeze(2)
                    mask_tile = mask_tile.expand(-1, n_heads, end_i - i, -1)
                    scores = scores.masked_fill(mask_tile == 0, float('-inf'))
                
                # Online softmax computation (numerically stable)
                current_max = torch.max(scores, dim=-1, keepdim=True)[0]
                new_max = torch.maximum(tile_max, current_max)
                
                # Update previous contributions
                old_scale = torch.exp(tile_max - new_max)
                tile_output *= old_scale
                tile_sum *= old_scale
                
                # Current contribution
                current_scale = torch.exp(current_max - new_max)
                current_exp = torch.exp(scores - new_max)
                
                # Update output and normalization
                tile_output += current_scale * torch.matmul(current_exp, v_tile)
                tile_sum += current_scale * torch.sum(current_exp, dim=-1, keepdim=True)
                tile_max = new_max
            
            # Normalize and store result
            output[:, :, i:end_i, :] = tile_output / tile_sum
        
        return output

class MultiHeadAttention(nn.Module):
    """
    Multi-head attention optimized for Indian languages
    Supports different attention patterns for different scripts
    """
    
    def __init__(self, config: AttentionConfig):
        super().__init__()
        self.config = config
        self.n_heads = config.n_heads
        self.head_dim = config.d_model // config.n_heads
        
        assert config.d_model % config.n_heads == 0, "d_model must be divisible by n_heads"
        
        # Linear projections
        self.query_proj = nn.Linear(config.d_model, config.d_model)
        self.key_proj = nn.Linear(config.d_model, config.d_model)
        self.value_proj = nn.Linear(config.d_model, config.d_model)
        self.output_proj = nn.Linear(config.d_model, config.d_model)
        
        # Dropout
        self.dropout = nn.Dropout(config.dropout)
        
        # Attention mechanism
        if config.attention_type == AttentionType.FLASH:
            self.attention = FlashAttention(config)
        else:
            self.attention = None  # Use standard attention in forward pass
        
        # Script-specific attention weights
        if config.supports_devanagari:
            self.script_attention_weights = nn.Parameter(torch.ones(4, config.n_heads))
        
        # Sliding window support for long texts
        self.sliding_window_size = config.sliding_window_size if config.attention_type == AttentionType.SLIDING_WINDOW else None
    
    def forward(self,
                hidden_states: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                script_types: Optional[torch.Tensor] = None,
                position_ids: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass with Indian language optimizations
        
        Args:
            hidden_states: [batch_size, seq_len, d_model]
            attention_mask: [batch_size, seq_len] (1 for valid tokens, 0 for padding)
            script_types: [batch_size, seq_len] (0=latin, 1=devanagari, 2=mixed, 3=other)
            position_ids: [batch_size, seq_len] (optional)
        """
        batch_size, seq_len, d_model = hidden_states.shape
        
        # Linear projections
        query = self.query_proj(hidden_states)  # [batch, seq_len, d_model]
        key = self.key_proj(hidden_states)
        value = self.value_proj(hidden_states)
        
        # Reshape for multi-head attention
        query = self._reshape_to_heads(query)  # [batch, n_heads, seq_len, head_dim]
        key = self._reshape_to_heads(key)
        value = self._reshape_to_heads(value)
        
        # Apply script-specific attention weights
        if script_types is not None and hasattr(self, 'script_attention_weights'):
            script_weights = self.script_attention_weights[script_types]  # [batch, seq_len, n_heads]
            script_weights = script_weights.transpose(1, 2).unsqueeze(-1)  # [batch, n_heads, seq_len, 1]
            query = query * script_weights
        
        # Apply sliding window mask if configured
        if self.sliding_window_size and seq_len > self.sliding_window_size:
            attention_mask = self._apply_sliding_window_mask(attention_mask, seq_len)
        
        # Compute attention
        if self.attention:  # Flash attention
            context = self.attention(query, key, value, attention_mask)
        else:  # Standard attention
            context = self._compute_standard_attention(query, key, value, attention_mask)
        
        # Reshape back to original dimensions
        context = self._reshape_from_heads(context)  # [batch, seq_len, d_model]
        
        # Output projection
        output = self.output_proj(context)
        output = self.dropout(output)
        
        # Compute attention weights for visualization (only for standard attention)
        attention_weights = None
        if not self.attention and not self.training:
            attention_weights = self._compute_attention_weights(query, key, attention_mask)
        
        return output, attention_weights
    
    def _reshape_to_heads(self, x: torch.Tensor) -> torch.Tensor:
        """Reshape tensor for multi-head attention"""
        batch_size, seq_len, d_model = x.shape
        x = x.reshape(batch_size, seq_len, self.n_heads, self.head_dim)
        return x.transpose(1, 2)  # [batch, n_heads, seq_len, head_dim]
    
    def _reshape_from_heads(self, x: torch.Tensor) -> torch.Tensor:
        """Reshape tensor from multi-head attention"""
        batch_size, n_heads, seq_len, head_dim = x.shape
        x = x.transpose(1, 2)  # [batch, seq_len, n_heads, head_dim]
        return x.reshape(batch_size, seq_len, self.config.d_model)
    
    def _compute_standard_attention(self,
                                  query: torch.Tensor,
                                  key: torch.Tensor,
                                  value: torch.Tensor,
                                  attention_mask: Optional[torch.Tensor]) -> torch.Tensor:
        """Standard scaled dot-product attention"""
        
        # Attention scores
        scores = torch.matmul(query, key.transpose(-2, -1))
        scores = scores / math.sqrt(self.head_dim)
        
        # Apply mask
        if attention_mask is not None:
            # Expand mask dimensions: [batch, seq_len] -> [batch, 1, 1, seq_len] -> [batch, n_heads, seq_len, seq_len]
            mask = attention_mask.unsqueeze(1).unsqueeze(2)
            mask = mask.expand(-1, self.n_heads, scores.size(-2), -1)
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # Softmax
        attention_probs = F.softmax(scores, dim=-1)
        attention_probs = self.dropout(attention_probs)
        
        # Apply to values
        context = torch.matmul(attention_probs, value)
        return context
    
    def _apply_sliding_window_mask(self, mask: Optional[torch.Tensor], seq_len: int) -> torch.Tensor:
        """Apply sliding window attention mask for long sequences"""
        
        device = mask.device if mask is not None else torch.device('cpu')
        window_mask = torch.ones(seq_len, seq_len, device=device)
        
        # Create sliding window pattern
        for i in range(seq_len):
            start = max(0, i - self.sliding_window_size // 2)
            end = min(seq_len, i + self.sliding_window_size // 2 + 1)
            window_mask[i, :start] = 0
            window_mask[i, end:] = 0
        
        # Combine with existing mask
        if mask is not None:
            batch_size = mask.size(0)
            # Expand mask to [batch, seq_len, seq_len]
            combined_mask = mask.unsqueeze(2) & mask.unsqueeze(1) & window_mask.unsqueeze(0)
            return combined_mask.view(batch_size, seq_len)
        else:
            return window_mask.sum(dim=1) > 0  # [seq_len] - any position has at least one valid attention
    
    def _compute_attention_weights(self,
                                 query: torch.Tensor,
                                 key: torch.Tensor, 
                                 attention_mask: Optional[torch.Tensor]) -> torch.Tensor:
        """Compute attention weights for visualization"""
        
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if attention_mask is not None:
            mask = attention_mask.unsqueeze(1).unsqueeze(2)
            mask = mask.expand(-1, self.n_heads, scores.size(-2), -1)
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        return F.softmax(scores, dim=-1)

class TransformerBlock(nn.Module):
    """
    Complete transformer block with attention optimized for Indian languages
    """
    
    def __init__(self, config: AttentionConfig):
        super().__init__()
        self.config = config
        
        # Multi-head attention
        self.attention = MultiHeadAttention(config)
        
        # Layer normalization
        self.ln1 = nn.LayerNorm(config.d_model)
        self.ln2 = nn.LayerNorm(config.d_model)
        
        # Feed-forward network
        self.ffn = nn.Sequential(
            nn.Linear(config.d_model, config.d_model * 4),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_model * 4, config.d_model),
            nn.Dropout(config.dropout)
        )
        
        # Gradient checkpointing for memory efficiency
        self.gradient_checkpointing = config.use_gradient_checkpointing
    
    def forward(self,
                hidden_states: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                script_types: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass through transformer block"""
        
        if self.gradient_checkpointing and self.training:
            return checkpoint.checkpoint(self._forward_impl, hidden_states, attention_mask, script_types)
        else:
            return self._forward_impl(hidden_states, attention_mask, script_types)
    
    def _forward_impl(self,
                     hidden_states: torch.Tensor,
                     attention_mask: Optional[torch.Tensor],
                     script_types: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """Implementation of forward pass"""
        
        # Pre-layer norm
        normed_hidden = self.ln1(hidden_states)
        
        # Self-attention
        attention_output, attention_weights = self.attention(
            normed_hidden, attention_mask, script_types
        )
        
        # Residual connection
        hidden_states = hidden_states + attention_output
        
        # Feed-forward
        normed_hidden = self.ln2(hidden_states)
        ffn_output = self.ffn(normed_hidden)
        
        # Residual connection
        hidden_states = hidden_states + ffn_output
        
        return hidden_states, attention_weights

class IndianLanguageTransformer(nn.Module):
    """
    Complete transformer model optimized for Indian languages
    Production-ready with cost tracking and monitoring
    """
    
    def __init__(self, config: AttentionConfig, vocab_size: int = 50000):
        super().__init__()
        self.config = config
        self.vocab_size = vocab_size
        
        # Token embeddings
        self.token_embeddings = nn.Embedding(vocab_size, config.d_model)
        
        # Positional encoding
        self.pos_encoding = PositionalEncoding(
            config.d_model, config.max_seq_length, config.dropout
        )
        
        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(config) for _ in range(12)  # 12 layers like BERT-base
        ])
        
        # Output layer norm
        self.ln_f = nn.LayerNorm(config.d_model)
        
        # Cost tracking
        self.total_tokens_processed = 0
        self.total_cost_inr = 0.0
        
        # Apply weight initialization
        self.apply(self._init_weights)
        
        logger.info(f"Indian Language Transformer initialized: "
                   f"{sum(p.numel() for p in self.parameters()) / 1e6:.1f}M parameters")
    
    def _init_weights(self, module):
        """Initialize weights with appropriate scaling"""
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
            if isinstance(module, nn.Linear) and module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
    
    def forward(self,
                input_ids: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                script_types: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass with cost tracking
        
        Args:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]
            script_types: [batch_size, seq_len]
        """
        start_time = time.time()
        
        batch_size, seq_len = input_ids.shape
        
        # Track tokens processed
        self.total_tokens_processed += batch_size * seq_len
        
        # Token embeddings
        hidden_states = self.token_embeddings(input_ids)  # [batch, seq_len, d_model]
        
        # Add positional encoding
        hidden_states = self.pos_encoding(hidden_states, script_types)
        
        # Pass through transformer blocks
        all_attention_weights = []
        for block in self.blocks:
            hidden_states, attention_weights = block(hidden_states, attention_mask, script_types)
            if attention_weights is not None:
                all_attention_weights.append(attention_weights)
        
        # Final layer norm
        hidden_states = self.ln_f(hidden_states)
        
        # Calculate cost
        processing_time = time.time() - start_time
        cost_inr = batch_size * seq_len * self.config.cost_per_token_inr
        self.total_cost_inr += cost_inr
        
        # Log performance metrics
        if self.training and batch_size == 1:  # Avoid spam during training
            tokens_per_second = batch_size * seq_len / processing_time
            logger.info(f"Processed {batch_size * seq_len} tokens in {processing_time:.3f}s "
                       f"({tokens_per_second:.1f} tokens/s), cost: ‚Çπ{cost_inr:.6f}")
        
        return {
            "last_hidden_state": hidden_states,
            "attention_weights": all_attention_weights if all_attention_weights else None,
            "processing_time": processing_time,
            "cost_inr": cost_inr,
            "tokens_processed": batch_size * seq_len
        }
    
    def get_cost_summary(self) -> Dict[str, Any]:
        """Get cost and performance summary"""
        return {
            "total_tokens_processed": self.total_tokens_processed,
            "total_cost_inr": f"‚Çπ{self.total_cost_inr:.6f}",
            "avg_cost_per_token": f"‚Çπ{self.total_cost_inr / max(1, self.total_tokens_processed):.8f}",
            "model_parameters": f"{sum(p.numel() for p in self.parameters()) / 1e6:.1f}M",
            "attention_type": self.config.attention_type.value
        }

def create_sample_indian_text() -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """Create sample Indian text for testing"""
    
    # Sample Hindi-English code-mixed text (tokenized)
    # In practice, you'd use a proper tokenizer
    texts = [
        "‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Æ‡•à‡§Ç ‡§è‡§ï AI model ‡§π‡•Ç‡§Ç‡•§",  # Hello, I am an AI model
        "‡§Ø‡§π transformer attention ‡§ï‡§æ example ‡§π‡•à‡•§",  # This is an example of transformer attention  
        "Mumbai ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ traffic ‡§π‡•à today‡•§"  # There is a lot of traffic in Mumbai today
    ]
    
    # Mock tokenization (in production, use proper tokenizer)
    max_len = 50
    vocab_size = 50000
    
    input_ids = torch.randint(1, vocab_size, (len(texts), max_len))
    attention_mask = torch.ones(len(texts), max_len)
    
    # Mark last 10 tokens as padding
    attention_mask[:, -10:] = 0
    
    # Script types: 0=latin, 1=devanagari, 2=mixed, 3=other
    script_types = torch.zeros(len(texts), max_len, dtype=torch.long)
    script_types[0, :] = 1  # Pure Hindi/Devanagari
    script_types[1, :] = 2  # Mixed Hindi-English
    script_types[2, :] = 2  # Mixed Hindi-English
    
    return input_ids, attention_mask, script_types

def test_transformer_attention():
    """Test transformer with Indian language optimizations"""
    
    print("üß† Transformer Attention Test - Indian Language Optimization")
    print("=" * 60)
    
    # Test different attention types
    attention_types = [AttentionType.STANDARD, AttentionType.FLASH]
    
    for attention_type in attention_types:
        print(f"\nüîç Testing {attention_type.value.upper()} Attention:")
        
        # Create config
        config = AttentionConfig(
            d_model=768,
            n_heads=12,
            max_seq_length=512,
            attention_type=attention_type,
            supports_devanagari=True,
            code_mixed_support=True,
            use_gradient_checkpointing=True,
            mixed_precision=True
        )
        
        # Create model
        model = IndianLanguageTransformer(config)
        model.eval()
        
        # Create sample input
        input_ids, attention_mask, script_types = create_sample_indian_text()
        
        print(f"   Input shape: {input_ids.shape}")
        print(f"   Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
        
        # Forward pass
        with torch.no_grad():
            start_time = time.time()
            outputs = model(input_ids, attention_mask, script_types)
            end_time = time.time()
        
        # Results
        print(f"   Output shape: {outputs['last_hidden_state'].shape}")
        print(f"   Processing time: {outputs['processing_time']:.3f}s")
        print(f"   Tokens/second: {outputs['tokens_processed'] / outputs['processing_time']:.1f}")
        print(f"   Cost: ‚Çπ{outputs['cost_inr']:.6f}")
        print(f"   Memory usage: ~{torch.cuda.memory_allocated() / 1e9:.2f} GB" if torch.cuda.is_available() else "   Memory: CPU mode")
        
        # Attention weights analysis
        if outputs['attention_weights']:
            attention = outputs['attention_weights'][0]  # First layer
            avg_attention = attention.mean(dim=(0, 1))  # Average across batch and heads
            print(f"   Avg attention entropy: {(-avg_attention * torch.log(avg_attention + 1e-9)).sum():.3f}")
    
    print(f"\nüí∞ Cost Summary:")
    cost_summary = model.get_cost_summary()
    for key, value in cost_summary.items():
        print(f"   {key}: {value}")
    
    print(f"\nüéØ Indian Language Optimizations:")
    print(f"   ‚úÖ Devanagari script support")
    print(f"   ‚úÖ Code-mixed text handling") 
    print(f"   ‚úÖ Regional script embeddings")
    print(f"   ‚úÖ Memory-efficient flash attention")
    print(f"   ‚úÖ Cost tracking per token")
    print(f"   ‚úÖ Gradient checkpointing for training")

if __name__ == "__main__":
    test_transformer_attention()