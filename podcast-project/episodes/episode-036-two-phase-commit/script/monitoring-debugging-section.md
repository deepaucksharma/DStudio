# Monitoring and Debugging 2PC Systems
## Observability Patterns & Production Incident Response - à¤œà¤¬ Systems à¤®à¥‡à¤‚ Aankhein Lagà¤¾à¤¨à¥€ à¤ªà¤¡à¤¼à¤¤à¥€ à¤¹à¥ˆ

---

### Opening: The Mumbai Control Room Analogy

*[Control room sounds, multiple monitors, alert notifications]*

"Mumbai mein Traffic Control Room dekha hai kabhi? Hundreds of screens, real-time monitoring, alert systems - ek bhi signal fail ho jaye toh immediately pata chal jaata hai. Similarly, 2PC systems mein bhi comprehensive monitoring zaroori hai."

"Aaj hum seekhenge:
- Real-time observability patterns
- Common failure modes aur unke symptoms  
- Debugging tools aur techniques
- Production incident response playbook
- Performance monitoring strategies
- Alerting systems design"

---

## Section 1: Observability Patterns - Mumbai à¤•à¥€ Eyes and Ears (600 words)

### Comprehensive Monitoring Dashboard

*[Dashboard loading sounds, metrics updating]*

"Production 2PC system monitor karna matlab Mumbai ke traffic signals monitor karne jaisa hai - har intersection (transaction) pe nazar rakhni padti hai."

```python
# Production-grade 2PC monitoring system
import time
import threading
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum
import json

class MetricType(Enum):
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    TIMER = "timer"

@dataclass
class TransactionMetrics:
    transaction_id: str
    start_time: float
    prepare_phase_duration: float
    commit_phase_duration: float
    total_duration: float
    participant_count: int
    success: bool
    failure_reason: Optional[str]
    coordinator_node: str
    business_context: Dict

class TwoPCObservabilitySystem:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
        self.dashboard = MonitoringDashboard()
        self.log_aggregator = LogAggregator()
        
        # Mumbai-specific monitoring
        self.business_hours_threshold = BusinessHoursThreshold()
        self.monsoon_mode_detector = MonsoonModeDetector()
        
    def start_monitoring(self):
        """Start all monitoring components"""
        
        # Start metrics collection
        self.metrics_collector.start()
        
        # Start real-time dashboard
        self.dashboard.start_real_time_updates()
        
        # Start alert processing
        self.alert_manager.start_alert_processing()
        
        # Start log aggregation
        self.log_aggregator.start_log_streaming()
        
        print("ðŸ” 2PC Monitoring System Started")
        print("ðŸ“Š Dashboard available at: http://localhost:8080/2pc-dashboard")
        print("ðŸš¨ Alerts configured for Slack #2pc-alerts")

class MetricsCollector:
    def __init__(self):
        self.active_transactions = {}
        self.completed_transactions = []
        self.system_health_metrics = SystemHealthMetrics()
        
        # Time-series data storage
        self.transaction_rate_history = []
        self.success_rate_history = []
        self.latency_history = []
        
    def record_transaction_start(self, transaction_id: str, context: Dict):
        """Record when a 2PC transaction begins"""
        
        self.active_transactions[transaction_id] = {
            'start_time': time.time(),
            'context': context,
            'phase': 'PREPARE',
            'participants': context.get('participants', []),
            'coordinator': context.get('coordinator_node'),
            'business_type': context.get('business_type')
        }
        
        # Update real-time metrics
        self.system_health_metrics.active_transaction_count += 1
        self.system_health_metrics.total_transactions_today += 1
        
        # Business context logging
        if context.get('business_type') == 'UPI_PAYMENT':
            self.system_health_metrics.upi_transactions_today += 1
        elif context.get('business_type') == 'BANK_TRANSFER':
            self.system_health_metrics.bank_transfers_today += 1
    
    def record_prepare_phase_completion(self, transaction_id: str, 
                                       participant_votes: Dict, 
                                       phase_duration: float):
        """Record prepare phase completion"""
        
        if transaction_id not in self.active_transactions:
            self.log_error(f"Unknown transaction in prepare phase: {transaction_id}")
            return
        
        txn = self.active_transactions[transaction_id]
        txn['prepare_duration'] = phase_duration
        txn['prepare_votes'] = participant_votes
        txn['phase'] = 'COMMIT'
        
        # Analyze prepare phase performance
        if phase_duration > 2.0:  # > 2 seconds
            self.alert_manager.send_alert(
                AlertType.SLOW_PREPARE_PHASE,
                f"Slow prepare phase: {transaction_id} took {phase_duration:.2f}s"
            )
        
        # Check vote distribution
        abort_votes = sum(1 for vote in participant_votes.values() if vote != "VOTE-COMMIT")
        if abort_votes > 0:
            self.system_health_metrics.prepare_failures_today += 1
    
    def record_transaction_completion(self, transaction_id: str, 
                                    success: bool, 
                                    failure_reason: Optional[str] = None):
        """Record transaction completion"""
        
        if transaction_id not in self.active_transactions:
            self.log_error(f"Unknown transaction completion: {transaction_id}")
            return
        
        txn = self.active_transactions[transaction_id]
        end_time = time.time()
        total_duration = end_time - txn['start_time']
        
        # Create completed transaction record
        completed_txn = TransactionMetrics(
            transaction_id=transaction_id,
            start_time=txn['start_time'],
            prepare_phase_duration=txn.get('prepare_duration', 0),
            commit_phase_duration=total_duration - txn.get('prepare_duration', 0),
            total_duration=total_duration,
            participant_count=len(txn['participants']),
            success=success,
            failure_reason=failure_reason,
            coordinator_node=txn['coordinator'],
            business_context=txn['context']
        )
        
        # Store completed transaction
        self.completed_transactions.append(completed_txn)
        
        # Update system metrics
        self.system_health_metrics.active_transaction_count -= 1
        
        if success:
            self.system_health_metrics.successful_transactions_today += 1
        else:
            self.system_health_metrics.failed_transactions_today += 1
            
            # Alert on failure
            self.alert_manager.send_alert(
                AlertType.TRANSACTION_FAILURE,
                f"Transaction failed: {transaction_id}, Reason: {failure_reason}"
            )
        
        # Remove from active transactions
        del self.active_transactions[transaction_id]
        
        # Update time-series data
        self.update_time_series_metrics(completed_txn)
    
    def get_real_time_metrics(self) -> Dict:
        """Get current system metrics for dashboard"""
        
        current_time = time.time()
        
        # Calculate success rate (last 5 minutes)
        recent_transactions = [
            txn for txn in self.completed_transactions 
            if current_time - txn.start_time < 300  # 5 minutes
        ]
        
        success_rate = 0.0
        if recent_transactions:
            successful = sum(1 for txn in recent_transactions if txn.success)
            success_rate = (successful / len(recent_transactions)) * 100
        
        # Calculate average latency
        avg_latency = 0.0
        if recent_transactions:
            avg_latency = sum(txn.total_duration for txn in recent_transactions) / len(recent_transactions)
        
        # Calculate transaction rate (per minute)
        transaction_rate = len(recent_transactions) / 5  # Per minute
        
        return {
            'active_transactions': len(self.active_transactions),
            'success_rate_5min': round(success_rate, 2),
            'avg_latency_5min': round(avg_latency, 3),
            'transaction_rate_per_min': round(transaction_rate, 2),
            'total_transactions_today': self.system_health_metrics.total_transactions_today,
            'failed_transactions_today': self.system_health_metrics.failed_transactions_today,
            'upi_transactions_today': self.system_health_metrics.upi_transactions_today,
            'bank_transfers_today': self.system_health_metrics.bank_transfers_today,
            'prepare_failures_today': self.system_health_metrics.prepare_failures_today,
            'longest_active_transaction': self.get_longest_active_transaction(),
            'participant_health_scores': self.get_participant_health_scores()
        }

class MonitoringDashboard:
    def __init__(self):
        self.flask_app = self.create_dashboard_app()
        self.websocket_clients = []
        
    def create_dashboard_app(self):
        """Create Flask-based monitoring dashboard"""
        from flask import Flask, render_template, jsonify
        from flask_socketio import SocketIO
        
        app = Flask(__name__)
        socketio = SocketIO(app)
        
        @app.route('/2pc-dashboard')
        def dashboard():
            return render_template('2pc_dashboard.html')
        
        @app.route('/api/metrics')
        def get_metrics():
            metrics = self.metrics_collector.get_real_time_metrics()
            return jsonify(metrics)
        
        @app.route('/api/active-transactions')
        def get_active_transactions():
            return jsonify(self.metrics_collector.active_transactions)
        
        @app.route('/api/failed-transactions')
        def get_failed_transactions():
            failed_txns = [
                txn for txn in self.metrics_collector.completed_transactions
                if not txn.success
            ]
            return jsonify([{
                'transaction_id': txn.transaction_id,
                'failure_reason': txn.failure_reason,
                'duration': txn.total_duration,
                'business_context': txn.business_context
            } for txn in failed_txns[-50:]])  # Last 50 failures
        
        return app
    
    def start_real_time_updates(self):
        """Start real-time dashboard updates"""
        
        def update_dashboard():
            while True:
                metrics = self.metrics_collector.get_real_time_metrics()
                
                # Send to all connected WebSocket clients
                for client in self.websocket_clients:
                    try:
                        client.send(json.dumps(metrics))
                    except:
                        self.websocket_clients.remove(client)
                
                time.sleep(1)  # Update every second
        
        threading.Thread(target=update_dashboard, daemon=True).start()
```

---

## Section 2: Common Failure Modes - Mumbai à¤•à¥‡ Traffic Problems à¤œà¥ˆà¤¸à¥‡ (500 words)

### Identifying and Diagnosing 2PC Failures

*[Alert sounds, system diagnostics]*

"Mumbai traffic mein common problems hain - signal failure, road blockage, VIP movement. Similarly, 2PC mein bhi predictable failure patterns hote hain."

```python
class FailureModeAnalyzer:
    def __init__(self):
        self.failure_patterns = self.load_known_patterns()
        self.diagnostic_rules = self.create_diagnostic_rules()
        
    def load_known_patterns(self):
        """Load known 2PC failure patterns"""
        return {
            'COORDINATOR_CRASH': {
                'symptoms': [
                    'Multiple transactions stuck in PREPARE phase',
                    'Participants waiting for coordinator response',
                    'Coordinator node unresponsive',
                    'No new transactions being accepted'
                ],
                'detection_time': '30-60 seconds',
                'business_impact': 'HIGH',
                'mumbai_analogy': 'Main traffic control center failure'
            },
            
            'PARTICIPANT_TIMEOUT': {
                'symptoms': [
                    'Specific participant not responding to PREPARE',
                    'Timeout exceptions in coordinator logs',
                    'Participant database locks held indefinitely',
                    'Transaction abortion rate spike'
                ],
                'detection_time': '5-30 seconds',
                'business_impact': 'MEDIUM',
                'mumbai_analogy': 'One traffic signal not working'
            },
            
            'NETWORK_PARTITION': {
                'symptoms': [
                    'Intermittent communication failures',
                    'Split-brain scenarios in cluster',
                    'Inconsistent transaction states',
                    'Recovery storms after partition heals'
                ],
                'detection_time': '1-5 minutes',
                'business_impact': 'CRITICAL',
                'mumbai_analogy': 'Road flooding cutting connectivity'
            },
            
            'DEADLOCK_CASCADE': {
                'symptoms': [
                    'Transaction completion rate drops suddenly',
                    'Multiple deadlocks detected simultaneously',
                    'Lock wait times increasing exponentially',
                    'Victim selection algorithm overwhelmed'
                ],
                'detection_time': '10-60 seconds',
                'business_impact': 'HIGH',
                'mumbai_analogy': 'Multiple intersections gridlocked'
            },
            
            'RESOURCE_EXHAUSTION': {
                'symptoms': [
                    'Memory usage spike on coordinator',
                    'Database connection pool exhausted',
                    'CPU usage sustained above 90%',
                    'Garbage collection pause spikes'
                ],
                'detection_time': '2-10 minutes',
                'business_impact': 'HIGH',
                'mumbai_analogy': 'Traffic controller overloaded'
            }
        }
    
    def diagnose_failure(self, symptoms: List[str], metrics: Dict) -> Dict:
        """Diagnose failure based on symptoms and metrics"""
        
        diagnosis_scores = {}
        
        for failure_type, pattern in self.failure_patterns.items():
            score = 0
            
            # Check symptom matches
            for symptom in symptoms:
                if any(pattern_symptom in symptom.lower() 
                       for pattern_symptom in pattern['symptoms']):
                    score += 1
            
            # Check metric-based indicators
            score += self.check_metric_indicators(failure_type, metrics)
            
            diagnosis_scores[failure_type] = score
        
        # Find most likely failure mode
        most_likely = max(diagnosis_scores.keys(), key=lambda k: diagnosis_scores[k])
        confidence = diagnosis_scores[most_likely] / len(self.failure_patterns[most_likely]['symptoms'])
        
        return {
            'most_likely_failure': most_likely,
            'confidence': confidence,
            'all_scores': diagnosis_scores,
            'recommended_actions': self.get_recovery_actions(most_likely),
            'business_impact': self.failure_patterns[most_likely]['business_impact'],
            'mumbai_analogy': self.failure_patterns[most_likely]['mumbai_analogy']
        }
    
    def check_metric_indicators(self, failure_type: str, metrics: Dict) -> int:
        """Check metric-based failure indicators"""
        
        score = 0
        
        if failure_type == 'COORDINATOR_CRASH':
            if metrics.get('coordinator_response_time', 0) > 30:  # 30+ seconds
                score += 2
            if metrics.get('new_transactions_rate', 1) == 0:
                score += 2
        
        elif failure_type == 'PARTICIPANT_TIMEOUT':
            if metrics.get('avg_prepare_time', 0) > 5:  # 5+ seconds
                score += 1
            if metrics.get('timeout_rate', 0) > 0.1:  # 10%+ timeouts
                score += 2
        
        elif failure_type == 'NETWORK_PARTITION':
            if metrics.get('network_error_rate', 0) > 0.05:  # 5%+ network errors
                score += 2
            if metrics.get('split_brain_detected', False):
                score += 3
        
        elif failure_type == 'DEADLOCK_CASCADE':
            if metrics.get('deadlock_count_5min', 0) > 10:
                score += 2
            if metrics.get('avg_lock_wait_time', 0) > 10:  # 10+ seconds
                score += 1
        
        elif failure_type == 'RESOURCE_EXHAUSTION':
            if metrics.get('coordinator_memory_usage', 0) > 0.9:  # 90%+ memory
                score += 2
            if metrics.get('gc_pause_time', 0) > 1:  # 1+ second GC pauses
                score += 1
        
        return score
    
    def get_recovery_actions(self, failure_type: str) -> List[str]:
        """Get recommended recovery actions for failure type"""
        
        recovery_actions = {
            'COORDINATOR_CRASH': [
                'Initiate coordinator failover to backup node',
                'Run transaction recovery protocol',
                'Query all participants for transaction states',
                'Abort orphaned transactions older than timeout',
                'Verify data consistency across all participants'
            ],
            
            'PARTICIPANT_TIMEOUT': [
                'Check participant node health and connectivity',
                'Increase timeout values temporarily',
                'Retry failed transactions with backoff',
                'Consider removing slow participant from pool',
                'Monitor database lock contention'
            ],
            
            'NETWORK_PARTITION': [
                'Implement partition tolerance measures',
                'Activate read-only mode for affected partitions',
                'Queue transactions for replay after healing',
                'Monitor partition healing indicators',
                'Run consistency checks after recovery'
            ],
            
            'DEADLOCK_CASCADE': [
                'Temporarily reduce transaction concurrency',
                'Clear all held locks and restart transactions',
                'Analyze deadlock patterns for optimization',
                'Implement more aggressive deadlock detection',
                'Consider transaction ordering improvements'
            ],
            
            'RESOURCE_EXHAUSTION': [
                'Scale up coordinator resources immediately',
                'Implement transaction rate limiting',
                'Clear non-essential cached data',
                'Restart services with increased memory',
                'Monitor resource usage continuously'
            ]
        }
        
        return recovery_actions.get(failure_type, ['Contact support team'])
```

---

## Section 3: Debugging Tools & Techniques (500 words)

### Production Debugging Arsenal

*[Debugging tools sounds, log analysis]*

"Mumbai police ki investigation techniques advanced hoti hain. Similarly, 2PC debugging ke liye bhi specialized tools chahiye."

```python
class TwoPCDebugger:
    def __init__(self):
        self.transaction_tracer = TransactionTracer()
        self.state_inspector = StateInspector()
        self.log_analyzer = LogAnalyzer()
        self.performance_profiler = PerformanceProfiler()
        
    def debug_stuck_transaction(self, transaction_id: str) -> Dict:
        """Debug a stuck transaction"""
        
        print(f"ðŸ” Debugging stuck transaction: {transaction_id}")
        
        # Step 1: Trace transaction flow
        trace = self.transaction_tracer.trace_transaction(transaction_id)
        
        # Step 2: Inspect current state
        current_state = self.state_inspector.inspect_transaction_state(transaction_id)
        
        # Step 3: Analyze related logs
        logs = self.log_analyzer.get_transaction_logs(transaction_id)
        
        # Step 4: Check participant states
        participant_states = self.check_all_participant_states(transaction_id)
        
        # Step 5: Performance analysis
        performance_data = self.performance_profiler.analyze_transaction(transaction_id)
        
        return {
            'transaction_id': transaction_id,
            'trace': trace,
            'current_state': current_state,
            'logs': logs,
            'participant_states': participant_states,
            'performance_data': performance_data,
            'recommendations': self.generate_debug_recommendations(
                trace, current_state, participant_states
            )
        }
    
    def check_all_participant_states(self, transaction_id: str) -> Dict:
        """Check state of transaction across all participants"""
        
        participant_states = {}
        
        # Query each participant for transaction state
        for participant in self.get_transaction_participants(transaction_id):
            try:
                state = participant.query_transaction_state(transaction_id)
                participant_states[participant.id] = {
                    'state': state,
                    'timestamp': time.time(),
                    'locks_held': participant.get_held_locks(transaction_id),
                    'last_activity': participant.get_last_activity(transaction_id)
                }
            except Exception as e:
                participant_states[participant.id] = {
                    'state': 'UNREACHABLE',
                    'error': str(e),
                    'timestamp': time.time()
                }
        
        return participant_states
    
    def generate_debug_recommendations(self, trace: Dict, 
                                     current_state: Dict, 
                                     participant_states: Dict) -> List[str]:
        """Generate debugging recommendations"""
        
        recommendations = []
        
        # Check for coordinator issues
        if current_state.get('coordinator_status') != 'ACTIVE':
            recommendations.append(
                "âš ï¸ Coordinator appears inactive - check coordinator health"
            )
        
        # Check for participant inconsistencies
        states = [p.get('state') for p in participant_states.values()]
        if len(set(states)) > 2:  # More than 2 different states
            recommendations.append(
                "ðŸ”´ Inconsistent participant states detected - manual intervention required"
            )
        
        # Check for stuck prepare phase
        if current_state.get('phase') == 'PREPARE' and current_state.get('age_seconds', 0) > 300:
            recommendations.append(
                "â° Transaction stuck in PREPARE phase >5min - consider aborting"
            )
        
        # Check for orphaned locks
        orphaned_locks = [
            p_id for p_id, p_state in participant_states.items()
            if p_state.get('locks_held', 0) > 0 and p_state.get('state') == 'UNKNOWN'
        ]
        if orphaned_locks:
            recommendations.append(
                f"ðŸ”’ Orphaned locks detected on participants: {orphaned_locks}"
            )
        
        # Performance recommendations
        if trace.get('total_duration', 0) > 10:  # >10 seconds
            recommendations.append(
                "ðŸŒ Transaction running longer than expected - check participant performance"
            )
        
        return recommendations

class TransactionTracer:
    def trace_transaction(self, transaction_id: str) -> Dict:
        """Trace complete transaction flow"""
        
        trace_data = {
            'transaction_id': transaction_id,
            'start_time': None,
            'phases': [],
            'participant_interactions': [],
            'coordinator_decisions': [],
            'timing_breakdown': {}
        }
        
        # Parse coordinator logs for this transaction
        coordinator_logs = self.get_coordinator_logs(transaction_id)
        
        for log_entry in coordinator_logs:
            if 'TRANSACTION_START' in log_entry.message:
                trace_data['start_time'] = log_entry.timestamp
                
            elif 'PREPARE_PHASE_START' in log_entry.message:
                trace_data['phases'].append({
                    'phase': 'PREPARE',
                    'start_time': log_entry.timestamp,
                    'status': 'STARTED'
                })
                
            elif 'PARTICIPANT_VOTE' in log_entry.message:
                vote_data = self.parse_vote_log(log_entry)
                trace_data['participant_interactions'].append(vote_data)
                
            elif 'COMMIT_DECISION' in log_entry.message:
                trace_data['coordinator_decisions'].append({
                    'decision': 'COMMIT',
                    'timestamp': log_entry.timestamp,
                    'reason': log_entry.get_field('reason')
                })
        
        # Calculate timing breakdown
        trace_data['timing_breakdown'] = self.calculate_timing_breakdown(trace_data)
        
        return trace_data

# Command-line debugging interface
class CLIDebugger:
    def __init__(self, debugger: TwoPCDebugger):
        self.debugger = debugger
        
    def run_interactive_session(self):
        """Run interactive debugging session"""
        
        print("ðŸ”§ 2PC Interactive Debugger")
        print("Commands: debug <txn_id>, list-stuck, health-check, quit")
        
        while True:
            command = input("2pc-debug> ").strip()
            
            if command.startswith("debug "):
                txn_id = command.split(" ", 1)[1]
                result = self.debugger.debug_stuck_transaction(txn_id)
                self.print_debug_result(result)
                
            elif command == "list-stuck":
                stuck_transactions = self.get_stuck_transactions()
                self.print_stuck_transactions(stuck_transactions)
                
            elif command == "health-check":
                health = self.run_system_health_check()
                self.print_health_check(health)
                
            elif command == "quit":
                break
                
            else:
                print("Unknown command. Type 'help' for available commands.")
    
    def print_debug_result(self, result: Dict):
        """Print formatted debug result"""
        
        print(f"\nðŸ“Š Debug Result for Transaction: {result['transaction_id']}")
        print("=" * 60)
        
        # Current state
        state = result['current_state']
        print(f"Current Phase: {state.get('phase', 'UNKNOWN')}")
        print(f"Age: {state.get('age_seconds', 0)} seconds")
        print(f"Coordinator: {state.get('coordinator_node', 'UNKNOWN')}")
        
        # Participant states
        print("\nðŸ¢ Participant States:")
        for p_id, p_state in result['participant_states'].items():
            status_emoji = "âœ…" if p_state.get('state') == 'PREPARED' else "âŒ"
            print(f"  {status_emoji} {p_id}: {p_state.get('state')}")
        
        # Recommendations
        print("\nðŸ’¡ Recommendations:")
        for rec in result['recommendations']:
            print(f"  â€¢ {rec}")
```

---

## Section 4: Production Incident Response Playbook (500 words)

### Mumbai-Style Crisis Management

*[Emergency response sounds, incident management]*

"Mumbai mein emergency response ki system hai - Fire Brigade, Police, Traffic Control sab coordinate karte hain. Similarly, 2PC incidents mein bhi systematic response chahiye."

```python
class IncidentResponsePlaybook:
    def __init__(self):
        self.severity_levels = self.define_severity_levels()
        self.response_teams = self.setup_response_teams()
        self.escalation_matrix = self.create_escalation_matrix()
        
    def define_severity_levels(self):
        """Define incident severity levels"""
        return {
            'SEV1_CRITICAL': {
                'description': 'Complete 2PC system failure',
                'examples': [
                    'All coordinators down',
                    'Data corruption detected',
                    'Split brain scenario'
                ],
                'response_time': '5 minutes',
                'escalation_time': '15 minutes',
                'mumbai_analogy': 'Complete traffic system failure'
            },
            
            'SEV2_HIGH': {
                'description': 'Significant functionality impacted',
                'examples': [
                    'High transaction failure rate (>10%)',
                    'Multiple participant failures',
                    'Deadlock cascade'
                ],
                'response_time': '15 minutes', 
                'escalation_time': '30 minutes',
                'mumbai_analogy': 'Major road closure'
            },
            
            'SEV3_MEDIUM': {
                'description': 'Degraded performance',
                'examples': [
                    'Single participant slow/failing',
                    'Increased latency (>2x normal)',
                    'Memory/CPU pressure'
                ],
                'response_time': '30 minutes',
                'escalation_time': '60 minutes', 
                'mumbai_analogy': 'Traffic signal malfunction'
            },
            
            'SEV4_LOW': {
                'description': 'Minor issues',
                'examples': [
                    'Monitoring alerts',
                    'Non-critical participant warnings',
                    'Performance degradation <20%'
                ],
                'response_time': '2 hours',
                'escalation_time': '4 hours',
                'mumbai_analogy': 'Street light not working'
            }
        }
    
    def handle_incident(self, incident: Dict) -> Dict:
        """Handle production incident with structured response"""
        
        # Step 1: Classify severity
        severity = self.classify_incident_severity(incident)
        
        # Step 2: Assemble response team
        response_team = self.assemble_response_team(severity)
        
        # Step 3: Execute immediate response
        immediate_actions = self.execute_immediate_response(incident, severity)
        
        # Step 4: Begin investigation
        investigation = self.start_investigation(incident)
        
        # Step 5: Communication plan
        communication_plan = self.activate_communication_plan(incident, severity)
        
        return {
            'incident_id': incident['id'],
            'severity': severity,
            'response_team': response_team,
            'immediate_actions': immediate_actions,
            'investigation': investigation,
            'communication_plan': communication_plan,
            'next_review_time': self.calculate_next_review_time(severity)
        }
    
    def execute_immediate_response(self, incident: Dict, severity: str) -> List[str]:
        """Execute immediate response actions based on severity"""
        
        actions_taken = []
        
        if severity == 'SEV1_CRITICAL':
            # Critical response actions
            actions_taken.extend([
                'Activate emergency pager for all on-call engineers',
                'Switch to backup coordinator cluster',
                'Enable read-only mode to prevent data corruption',
                'Start emergency data backup',
                'Notify C-level executives'
            ])
            
        elif severity == 'SEV2_HIGH':
            # High severity response
            actions_taken.extend([
                'Page primary on-call engineer',
                'Increase monitoring frequency to 30-second intervals',
                'Activate transaction rate limiting',
                'Prepare coordinator failover',
                'Notify engineering management'
            ])
            
        elif severity == 'SEV3_MEDIUM':
            # Medium severity response
            actions_taken.extend([
                'Create incident ticket',
                'Assign to primary on-call engineer',
                'Increase logging verbosity',
                'Monitor participant health closely'
            ])
            
        elif severity == 'SEV4_LOW':
            # Low severity response
            actions_taken.extend([
                'Create monitoring ticket',
                'Schedule investigation during business hours',
                'Add to weekly review agenda'
            ])
        
        # Execute actions
        for action in actions_taken:
            self.execute_action(action)
            
        return actions_taken
    
    def create_post_incident_review(self, incident: Dict) -> Dict:
        """Create comprehensive post-incident review"""
        
        pir = {
            'incident_summary': {
                'id': incident['id'],
                'start_time': incident['start_time'],
                'end_time': incident['end_time'],
                'duration_minutes': incident['duration_minutes'],
                'severity': incident['severity'],
                'root_cause': incident['root_cause']
            },
            
            'timeline': self.create_incident_timeline(incident),
            
            'impact_analysis': {
                'transactions_affected': incident['transactions_affected'],
                'revenue_impact': incident['revenue_impact'],
                'customer_complaints': incident['customer_complaints'],
                'sla_breaches': incident['sla_breaches']
            },
            
            'what_went_well': [
                'Monitoring detected issue within 2 minutes',
                'Response team assembled quickly',
                'Failover executed successfully',
                'Customer communication was timely'
            ],
            
            'what_went_poorly': [
                'Root cause identification took 45 minutes',
                'Manual intervention required',
                'Documentation was outdated',
                'Some alerts were noisy'
            ],
            
            'action_items': [
                {
                    'description': 'Improve automated failure detection',
                    'owner': 'SRE Team',
                    'due_date': '2024-02-15',
                    'priority': 'HIGH'
                },
                {
                    'description': 'Update incident response documentation',
                    'owner': 'Engineering Team',
                    'due_date': '2024-02-10',
                    'priority': 'MEDIUM'
                },
                {
                    'description': 'Implement faster coordinator failover',
                    'owner': 'Platform Team',
                    'due_date': '2024-03-01',
                    'priority': 'HIGH'
                }
            ],
            
            'lessons_learned': [
                'Backup coordinators need regular testing',
                'Participant timeout values need tuning',
                'Cross-team communication protocols work well',
                'Monitoring dashboards saved significant debug time'
            ]
        }
        
        return pir

# Mumbai-specific incident scenarios
class MumbaiIncidentScenarios:
    def generate_monsoon_incident_plan(self):
        """Specific incident response for Mumbai monsoon season"""
        
        return {
            'scenario': 'Mumbai Monsoon Data Center Flooding',
            'trigger_conditions': [
                'Heavy rainfall alert from IMD',
                'Data center basement flooding sensors',
                'Increased network latency to Bandra DC',
                'Power fluctuations detected'
            ],
            
            'pre_emptive_actions': [
                'Activate backup data center in Pune',
                'Reduce transaction timeouts by 50%',
                'Enable aggressive coordinator failover',
                'Switch to monsoon-optimized routing',
                'Notify all teams of elevated alert level'
            ],
            
            'escalation_triggers': [
                'Primary DC inaccessible for >10 minutes',
                'Transaction success rate <80%',
                'Multiple coordinator failures',
                'Customer complaint spike >500% baseline'
            ],
            
            'recovery_checklist': [
                'Verify data consistency across DCs',
                'Run transaction reconciliation',
                'Check participant state synchronization',
                'Validate backup system performance',
                'Gradual traffic shift back to primary DC'
            ]
        }
```

**Monitoring & Debugging Summary:**

Toh doston, production 2PC systems monitor karna matlab:

1. **Comprehensive Observability** - Real-time metrics, dashboards, alerts
2. **Failure Pattern Recognition** - Common problems aur unke solutions  
3. **Debugging Tools** - Transaction tracing, state inspection, log analysis
4. **Incident Response** - Structured response playbook, severity classification
5. **Mumbai-Specific Considerations** - Monsoon planning, peak hour monitoring

**Word Count: 2,000+ words**

---

*Ready to combine all parts into complete script*