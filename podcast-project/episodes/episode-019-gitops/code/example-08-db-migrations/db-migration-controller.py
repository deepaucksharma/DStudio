#!/usr/bin/env python3
"""
Database Schema Migration Controller for GitOps
Indian Banking ‡§î‡§∞ FinTech applications ‡§ï‡•á ‡§≤‡§ø‡§è safe database migrations

‡§Ø‡§π system GitOps approach use ‡§ï‡§∞‡§ï‡•á database schema changes ‡§ï‡•ã manage ‡§ï‡§∞‡§§‡§æ ‡§π‡•à:
- Zero-downtime migrations for 24x7 Indian banking systems
- RBI compliance during schema changes
- Automatic rollback on migration failures
- Multi-region database synchronization
- Audit trail for all schema changes

Features:
- Blue-green database migrations
- Schema validation before deployment
- Performance impact analysis
- Compliance checks during migrations
- Real-time monitoring and alerting

Author: Database Engineering Team - Indian Banking Systems
"""

import asyncio
import json
import logging
import os
import time
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import yaml
import asyncpg
import psycopg2
from kubernetes import client, config
from dataclasses import dataclass
from enum import Enum
import hashlib

# ‡§≤‡•â‡§ó‡§ø‡§Ç‡§ó setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('db-migrations.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MigrationStatus(Enum):
    """Database migration status"""
    PENDING = "pending"
    VALIDATING = "validating"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    ROLLED_BACK = "rolled_back"
    BLOCKED = "blocked"

class MigrationType(Enum):
    """Database migration types"""
    SCHEMA_CHANGE = "schema_change"
    DATA_MIGRATION = "data_migration"
    INDEX_CREATION = "index_creation"
    CONSTRAINT_ADDITION = "constraint_addition"
    PARTITION_CHANGE = "partition_change"
    SEED_DATA = "seed_data"

class DatabaseEngine(Enum):
    """Supported database engines"""
    POSTGRESQL = "postgresql"
    MYSQL = "mysql"
    ORACLE = "oracle"
    MONGODB = "mongodb"

@dataclass
class MigrationScript:
    """Database migration script"""
    id: str
    name: str
    description: str
    migration_type: MigrationType
    database_engine: DatabaseEngine
    version: str
    up_script: str
    down_script: str
    estimated_duration_minutes: int
    requires_downtime: bool
    affected_tables: List[str]
    dependencies: List[str]
    compliance_notes: str
    created_by: str
    created_at: datetime

@dataclass
class MigrationExecution:
    """Migration execution record"""
    execution_id: str
    migration_id: str
    status: MigrationStatus
    start_time: datetime
    end_time: Optional[datetime]
    database_name: str
    environment: str
    executed_by: str
    execution_log: List[str]
    performance_metrics: Dict[str, Any]
    rollback_available: bool
    compliance_verified: bool

class IndianBankingMigrationController:
    """
    Indian Banking Systems ‡§ï‡•á ‡§≤‡§ø‡§è database migration controller
    RBI compliance ‡§î‡§∞ 24x7 availability ‡§ï‡•ã maintain ‡§ï‡§∞‡§§‡•á ‡§π‡•Å‡§è safe migrations ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
    """
    
    def __init__(self, config_file: str = "db-migration-config.yaml"):
        self.config = self._load_config(config_file)
        self.k8s_client = None
        self.db_connections = {}
        self.migration_history = {}
        self.active_migrations = {}
        
        # Indian banking specific configurations
        self.banking_config = {
            "compliance_frameworks": ["rbi", "pci_dss", "iso_27001"],
            "business_hours": {
                "start": "09:00",
                "end": "18:00",
                "timezone": "Asia/Kolkata"
            },
            "maintenance_windows": {
                "weekdays": "02:00-04:00",
                "weekends": "01:00-05:00"
            },
            "critical_systems": [
                "payment_processing",
                "account_management", 
                "transaction_logging",
                "audit_trails",
                "customer_data"
            ],
            "max_downtime_minutes": 5,  # RBI requirement
            "backup_retention_days": 2555,  # 7 years for RBI
            "audit_requirements": {
                "pre_migration_backup": True,
                "schema_validation": True,
                "performance_impact_analysis": True,
                "rollback_plan": True,
                "compliance_sign_off": True
            }
        }
        
        # Migration safety checks
        self.safety_checks = {
            "blocking_operations": [
                "ALTER TABLE.*ADD CONSTRAINT.*NOT NULL",
                "DROP TABLE",
                "DROP COLUMN",
                "ALTER TABLE.*ALTER COLUMN.*TYPE",
                "CREATE UNIQUE INDEX.*(?!CONCURRENTLY)"
            ],
            "performance_impact_operations": [
                "CREATE INDEX(?!.*CONCURRENTLY)",
                "VACUUM FULL",
                "CLUSTER",
                "ALTER TABLE.*ADD COLUMN.*DEFAULT"
            ],
            "max_table_size_gb": 100,  # Tables larger than 100GB need special handling
            "max_migration_duration_minutes": 120,
            "require_approval_for_tables": [
                "customer_accounts",
                "transactions", 
                "payment_records",
                "audit_logs"
            ]
        }
    
    def _load_config(self, config_file: str) -> Dict:
        """Configuration file load ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        try:
            with open(config_file, 'r') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logger.warning(f"‚ö†Ô∏è Config file {config_file} not found, using defaults")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """Default migration configuration"""
        return {
            "databases": {
                "production": {
                    "host": "prod-db.mumbai.bank.com",
                    "port": 5432,
                    "database": "banking_core",
                    "username": "migration_user",
                    "replica_hosts": [
                        "prod-db-replica1.mumbai.bank.com",
                        "prod-db-replica2.bangalore.bank.com"
                    ]
                },
                "staging": {
                    "host": "staging-db.mumbai.bank.com", 
                    "port": 5432,
                    "database": "banking_core",
                    "username": "migration_user"
                }
            },
            "migration_settings": {
                "batch_size": 10000,
                "timeout_minutes": 60,
                "retry_attempts": 3,
                "verify_checksums": True,
                "create_rollback_scripts": True
            },
            "monitoring": {
                "slack_webhook": "https://hooks.slack.com/services/DB_MIGRATIONS",
                "email_alerts": ["dba-team@bank.com", "compliance@bank.com"],
                "metrics_endpoint": "http://prometheus.monitoring.svc.cluster.local:9090"
            }
        }
    
    async def initialize(self):
        """Migration controller ‡§ï‡•ã initialize ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("üóÑÔ∏è Initializing Indian Banking Migration Controller...")
        
        try:
            # Kubernetes client setup
            config.load_incluster_config()
            self.k8s_client = client.ApiClient()
            
            # Database connections setup
            await self._setup_database_connections()
            
            # Verify migration tables exist
            await self._setup_migration_tables()
            
            # Load pending migrations
            await self._load_pending_migrations()
            
            logger.info("‚úÖ Migration controller initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Migration controller initialization failed: {e}")
            raise
    
    async def _setup_database_connections(self):
        """Database connections setup ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("üîå Setting up database connections...")
        
        for env_name, db_config in self.config["databases"].items():
            try:
                # Create connection pool for each environment
                connection_string = f"postgresql://{db_config['username']}@{db_config['host']}:{db_config['port']}/{db_config['database']}"
                
                pool = await asyncpg.create_pool(
                    connection_string,
                    min_size=2,
                    max_size=10,
                    command_timeout=300
                )
                
                self.db_connections[env_name] = pool
                logger.info(f"‚úÖ Database connection established: {env_name}")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to connect to {env_name}: {e}")
                raise
    
    async def _setup_migration_tables(self):
        """Migration tracking tables setup ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("üìã Setting up migration tracking tables...")
        
        migration_table_sql = """
        CREATE TABLE IF NOT EXISTS db_migrations (
            id SERIAL PRIMARY KEY,
            migration_id VARCHAR(255) UNIQUE NOT NULL,
            name VARCHAR(500) NOT NULL,
            description TEXT,
            migration_type VARCHAR(100) NOT NULL,
            version VARCHAR(100) NOT NULL,
            checksum VARCHAR(64) NOT NULL,
            executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            executed_by VARCHAR(255) NOT NULL,
            execution_time_ms INTEGER,
            status VARCHAR(50) NOT NULL,
            environment VARCHAR(100) NOT NULL,
            up_script TEXT NOT NULL,
            down_script TEXT,
            affected_tables TEXT[],
            compliance_notes TEXT,
            rollback_available BOOLEAN DEFAULT TRUE,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );
        
        CREATE INDEX IF NOT EXISTS idx_migrations_status ON db_migrations(status);
        CREATE INDEX IF NOT EXISTS idx_migrations_executed_at ON db_migrations(executed_at);
        CREATE INDEX IF NOT EXISTS idx_migrations_environment ON db_migrations(environment);
        
        -- Audit table for compliance
        CREATE TABLE IF NOT EXISTS migration_audit_log (
            id SERIAL PRIMARY KEY,
            migration_id VARCHAR(255) NOT NULL,
            action VARCHAR(100) NOT NULL,
            details JSONB,
            performed_by VARCHAR(255) NOT NULL,
            performed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            ip_address INET,
            user_agent TEXT
        );
        """
        
        for env_name, pool in self.db_connections.items():
            async with pool.acquire() as conn:
                await conn.execute(migration_table_sql)
                logger.info(f"‚úÖ Migration tables setup completed for {env_name}")
    
    async def _load_pending_migrations(self):
        """Pending migrations ‡§ï‡•ã load ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("üìÇ Loading pending migrations...")
        
        # Migration files ‡§ï‡•ã scan ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
        migrations_dir = self.config.get("migrations_directory", "./migrations")
        
        if os.path.exists(migrations_dir):
            migration_files = sorted([f for f in os.listdir(migrations_dir) if f.endswith('.sql')])
            
            for file_name in migration_files:
                migration = await self._parse_migration_file(os.path.join(migrations_dir, file_name))
                if migration:
                    self.migration_history[migration.id] = migration
                    logger.info(f"üìã Loaded migration: {migration.name}")
        
        logger.info(f"‚úÖ Loaded {len(self.migration_history)} migrations")
    
    async def _parse_migration_file(self, file_path: str) -> Optional[MigrationScript]:
        """Migration file ‡§ï‡•ã parse ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        try:
            with open(file_path, 'r') as f:
                content = f.read()
            
            # Extract metadata from comments
            lines = content.split('\n')
            metadata = {}
            
            for line in lines:
                if line.startswith('-- Migration:'):
                    metadata['name'] = line.split(':', 1)[1].strip()
                elif line.startswith('-- Description:'):
                    metadata['description'] = line.split(':', 1)[1].strip()
                elif line.startswith('-- Type:'):
                    metadata['type'] = line.split(':', 1)[1].strip()
                elif line.startswith('-- Version:'):
                    metadata['version'] = line.split(':', 1)[1].strip()
                elif line.startswith('-- Duration:'):
                    metadata['duration'] = int(line.split(':', 1)[1].strip())
                elif line.startswith('-- Tables:'):
                    metadata['tables'] = [t.strip() for t in line.split(':', 1)[1].split(',')]
                elif line.startswith('-- Compliance:'):
                    metadata['compliance'] = line.split(':', 1)[1].strip()
            
            # Split up and down scripts
            if '-- DOWN' in content:
                up_script, down_script = content.split('-- DOWN', 1)
            else:
                up_script = content
                down_script = ""
            
            up_script = up_script.replace('-- UP', '').strip()
            down_script = down_script.strip()
            
            migration = MigrationScript(
                id=self._generate_migration_id(file_path),
                name=metadata.get('name', os.path.basename(file_path)),
                description=metadata.get('description', ''),
                migration_type=MigrationType(metadata.get('type', 'schema_change')),
                database_engine=DatabaseEngine.POSTGRESQL,
                version=metadata.get('version', '1.0'),
                up_script=up_script,
                down_script=down_script,
                estimated_duration_minutes=metadata.get('duration', 5),
                requires_downtime=self._requires_downtime(up_script),
                affected_tables=metadata.get('tables', []),
                dependencies=[],
                compliance_notes=metadata.get('compliance', ''),
                created_by='GitOps Migration System',
                created_at=datetime.now()
            )
            
            return migration
            
        except Exception as e:
            logger.error(f"‚ùå Failed to parse migration file {file_path}: {e}")
            return None
    
    def _generate_migration_id(self, file_path: str) -> str:
        """Migration ID generate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        file_name = os.path.basename(file_path)
        return hashlib.md5(file_name.encode()).hexdigest()[:12]
    
    def _requires_downtime(self, sql_script: str) -> bool:
        """Script ‡§Æ‡•á‡§Ç downtime required ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç check ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        blocking_patterns = [
            "ALTER TABLE.*ADD CONSTRAINT.*NOT NULL",
            "DROP TABLE",
            "DROP COLUMN", 
            "ALTER TABLE.*ALTER COLUMN.*TYPE"
        ]
        
        script_upper = sql_script.upper()
        for pattern in blocking_patterns:
            if any(bp in script_upper for bp in self.safety_checks["blocking_operations"]):
                return True
        
        return False
    
    async def execute_migration(self, migration_id: str, environment: str, executed_by: str) -> MigrationExecution:
        """
        Migration ‡§ï‡•ã execute ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
        Indian banking compliance requirements ‡§ï‡•á ‡§∏‡§æ‡§•
        """
        if migration_id not in self.migration_history:
            raise ValueError(f"Migration {migration_id} not found")
        
        migration = self.migration_history[migration_id]
        execution_id = f"exec-{uuid.uuid4().hex[:8]}"
        
        logger.info(f"üöÄ Starting migration execution: {migration.name} (ID: {execution_id})")
        
        execution = MigrationExecution(
            execution_id=execution_id,
            migration_id=migration_id,
            status=MigrationStatus.VALIDATING,
            start_time=datetime.now(),
            end_time=None,
            database_name=environment,
            environment=environment,
            executed_by=executed_by,
            execution_log=[],
            performance_metrics={},
            rollback_available=False,
            compliance_verified=False
        )
        
        self.active_migrations[execution_id] = execution
        
        try:
            # Step 1: Pre-migration validation
            execution.execution_log.append(f"{datetime.now()}: Starting pre-migration validation")
            await self._validate_migration_pre_conditions(migration, environment, execution)
            
            # Step 2: Compliance verification
            execution.execution_log.append(f"{datetime.now()}: Verifying compliance requirements")
            await self._verify_compliance_requirements(migration, execution)
            
            # Step 3: Create backup
            execution.execution_log.append(f"{datetime.now()}: Creating pre-migration backup")
            backup_info = await self._create_pre_migration_backup(migration, environment, execution)
            
            # Step 4: Performance impact analysis
            execution.execution_log.append(f"{datetime.now()}: Analyzing performance impact")
            await self._analyze_performance_impact(migration, environment, execution)
            
            # Step 5: Execute migration
            execution.status = MigrationStatus.RUNNING
            execution.execution_log.append(f"{datetime.now()}: Executing migration script")
            
            await self._execute_migration_script(migration, environment, execution)
            
            # Step 6: Post-migration validation
            execution.execution_log.append(f"{datetime.now()}: Running post-migration validation")
            await self._validate_migration_post_conditions(migration, environment, execution)
            
            # Step 7: Performance verification
            execution.execution_log.append(f"{datetime.now()}: Verifying performance impact")
            await self._verify_performance_post_migration(migration, environment, execution)
            
            execution.status = MigrationStatus.COMPLETED
            execution.end_time = datetime.now()
            execution.rollback_available = bool(migration.down_script)
            
            # Log successful completion
            await self._log_migration_completion(execution)
            
            logger.info(f"‚úÖ Migration completed successfully: {execution_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Migration failed: {execution_id} | Error: {e}")
            execution.status = MigrationStatus.FAILED
            execution.end_time = datetime.now()
            execution.execution_log.append(f"{datetime.now()}: Migration failed: {str(e)}")
            
            # Attempt automatic rollback if available
            if migration.down_script and execution.rollback_available:
                await self._attempt_automatic_rollback(migration, environment, execution)
            
        return execution
    
    async def _validate_migration_pre_conditions(self, migration: MigrationScript, environment: str, execution: MigrationExecution):
        """Migration ‡§ï‡•á pre-conditions validate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info(f"üîç Validating pre-conditions for {migration.name}")
        
        pool = self.db_connections[environment]
        
        async with pool.acquire() as conn:
            # Check if migration already executed
            existing = await conn.fetchrow(
                "SELECT * FROM db_migrations WHERE migration_id = $1 AND environment = $2",
                migration.id, environment
            )
            
            if existing:
                raise Exception(f"Migration {migration.id} already executed in {environment}")
            
            # Check dependencies
            for dep_id in migration.dependencies:
                dep_exists = await conn.fetchrow(
                    "SELECT * FROM db_migrations WHERE migration_id = $1 AND environment = $2 AND status = 'completed'",
                    dep_id, environment
                )
                if not dep_exists:
                    raise Exception(f"Dependency {dep_id} not satisfied")
            
            # Verify affected tables exist
            for table_name in migration.affected_tables:
                table_exists = await conn.fetchval(
                    "SELECT EXISTS(SELECT 1 FROM information_schema.tables WHERE table_name = $1)",
                    table_name
                )
                if not table_exists:
                    logger.warning(f"‚ö†Ô∏è Table {table_name} does not exist")
            
            # Check table sizes for performance impact
            for table_name in migration.affected_tables:
                table_size = await conn.fetchval(
                    "SELECT pg_size_pretty(pg_total_relation_size($1))",
                    table_name
                )
                execution.execution_log.append(f"Table {table_name} size: {table_size}")
            
            execution.execution_log.append("‚úÖ Pre-conditions validation passed")
    
    async def _verify_compliance_requirements(self, migration: MigrationScript, execution: MigrationExecution):
        """Indian banking compliance requirements verify ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("üèõÔ∏è Verifying compliance requirements")
        
        compliance_checks = []
        
        # RBI Compliance Checks
        if "rbi" in self.banking_config["compliance_frameworks"]:
            # Check for audit trail requirements
            if any(table in migration.affected_tables for table in ["audit_logs", "transaction_logs"]):
                compliance_checks.append("RBI: Audit trail modification requires special approval")
            
            # Check for customer data modifications
            if any(table in migration.affected_tables for table in ["customers", "customer_accounts"]):
                compliance_checks.append("RBI: Customer data modification requires data protection compliance")
        
        # PCI-DSS Compliance Checks
        if "pci_dss" in self.banking_config["compliance_frameworks"]:
            # Check for payment data modifications
            if any(table in migration.affected_tables for table in ["payment_cards", "transactions"]):
                compliance_checks.append("PCI-DSS: Payment data modification requires PCI compliance")
        
        # Check for critical systems
        for system in self.banking_config["critical_systems"]:
            if system in migration.name.lower() or system in migration.description.lower():
                compliance_checks.append(f"Critical system modification: {system}")
        
        if compliance_checks:
            execution.execution_log.append(f"Compliance notes: {'; '.join(compliance_checks)}")
        
        execution.compliance_verified = True
        execution.execution_log.append("‚úÖ Compliance verification completed")
    
    async def _create_pre_migration_backup(self, migration: MigrationScript, environment: str, execution: MigrationExecution) -> Dict:
        """Pre-migration backup create ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("üíæ Creating pre-migration backup")
        
        if not self.banking_config["audit_requirements"]["pre_migration_backup"]:
            execution.execution_log.append("Backup not required per configuration")
            return {}
        
        backup_info = {
            "backup_id": f"backup-{execution.execution_id}",
            "timestamp": datetime.now().isoformat(),
            "affected_tables": migration.affected_tables,
            "backup_method": "pg_dump"
        }
        
        # Mock backup implementation
        # Real implementation would use pg_dump or similar tools
        execution.execution_log.append(f"Backup created: {backup_info['backup_id']}")
        
        return backup_info
    
    async def _analyze_performance_impact(self, migration: MigrationScript, environment: str, execution: MigrationExecution):
        """Migration ‡§ï‡•á performance impact ‡§ï‡•ã analyze ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("üìä Analyzing performance impact")
        
        pool = self.db_connections[environment]
        
        async with pool.acquire() as conn:
            # Get current performance baseline
            performance_before = {}
            
            for table_name in migration.affected_tables:
                # Table statistics
                stats = await conn.fetchrow("""
                    SELECT 
                        schemaname, tablename, n_tup_ins, n_tup_upd, n_tup_del,
                        n_live_tup, n_dead_tup
                    FROM pg_stat_user_tables 
                    WHERE tablename = $1
                """, table_name)
                
                if stats:
                    performance_before[table_name] = dict(stats)
            
            # Index usage statistics
            index_stats = await conn.fetch("""
                SELECT schemaname, tablename, indexname, idx_scan
                FROM pg_stat_user_indexes
                WHERE tablename = ANY($1)
            """, migration.affected_tables)
            
            performance_before["indexes"] = [dict(row) for row in index_stats]
            
            execution.performance_metrics["before"] = performance_before
            execution.execution_log.append("üìä Performance baseline captured")
    
    async def _execute_migration_script(self, migration: MigrationScript, environment: str, execution: MigrationExecution):
        """Migration script ‡§ï‡•ã execute ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info(f"‚ö° Executing migration script: {migration.name}")
        
        pool = self.db_connections[environment]
        start_time = time.time()
        
        async with pool.acquire() as conn:
            async with conn.transaction():
                try:
                    # Execute the migration script
                    await conn.execute(migration.up_script)
                    
                    # Record migration in tracking table
                    await conn.execute("""
                        INSERT INTO db_migrations (
                            migration_id, name, description, migration_type, version,
                            checksum, executed_by, execution_time_ms, status, environment,
                            up_script, down_script, affected_tables, compliance_notes
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
                    """, 
                        migration.id, migration.name, migration.description,
                        migration.migration_type.value, migration.version,
                        self._calculate_checksum(migration.up_script),
                        execution.executed_by, int((time.time() - start_time) * 1000),
                        'completed', environment, migration.up_script, migration.down_script,
                        migration.affected_tables, migration.compliance_notes
                    )
                    
                    execution.execution_log.append(f"‚úÖ Migration script executed successfully")
                    
                except Exception as e:
                    execution.execution_log.append(f"‚ùå Migration script execution failed: {str(e)}")
                    raise
    
    async def _validate_migration_post_conditions(self, migration: MigrationScript, environment: str, execution: MigrationExecution):
        """Migration ‡§ï‡•á post-conditions validate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("‚úÖ Validating post-migration conditions")
        
        pool = self.db_connections[environment]
        
        async with pool.acquire() as conn:
            # Verify schema changes applied correctly
            for table_name in migration.affected_tables:
                # Check table structure
                columns = await conn.fetch("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns
                    WHERE table_name = $1
                    ORDER BY ordinal_position
                """, table_name)
                
                execution.execution_log.append(f"Table {table_name}: {len(columns)} columns")
            
            # Verify data integrity
            if migration.migration_type == MigrationType.DATA_MIGRATION:
                for table_name in migration.affected_tables:
                    row_count = await conn.fetchval(f"SELECT COUNT(*) FROM {table_name}")
                    execution.execution_log.append(f"Table {table_name}: {row_count} rows")
            
            execution.execution_log.append("‚úÖ Post-conditions validation passed")
    
    async def _verify_performance_post_migration(self, migration: MigrationScript, environment: str, execution: MigrationExecution):
        """Migration ‡§ï‡•á ‡§¨‡§æ‡§¶ performance verify ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.info("üöÄ Verifying post-migration performance")
        
        pool = self.db_connections[environment]
        
        async with pool.acquire() as conn:
            # Get performance metrics after migration
            performance_after = {}
            
            for table_name in migration.affected_tables:
                stats = await conn.fetchrow("""
                    SELECT 
                        schemaname, tablename, n_tup_ins, n_tup_upd, n_tup_del,
                        n_live_tup, n_dead_tup
                    FROM pg_stat_user_tables 
                    WHERE tablename = $1
                """, table_name)
                
                if stats:
                    performance_after[table_name] = dict(stats)
            
            execution.performance_metrics["after"] = performance_after
            
            # Calculate performance impact
            impact_analysis = self._calculate_performance_impact(
                execution.performance_metrics.get("before", {}),
                performance_after
            )
            
            execution.performance_metrics["impact"] = impact_analysis
            execution.execution_log.append("üìä Performance verification completed")
    
    def _calculate_performance_impact(self, before: Dict, after: Dict) -> Dict:
        """Performance impact calculate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        impact = {}
        
        for table_name in before.keys():
            if table_name in after and table_name != "indexes":
                before_stats = before[table_name]
                after_stats = after[table_name]
                
                impact[table_name] = {
                    "row_count_change": after_stats.get("n_live_tup", 0) - before_stats.get("n_live_tup", 0),
                    "dead_tuples_change": after_stats.get("n_dead_tup", 0) - before_stats.get("n_dead_tup", 0)
                }
        
        return impact
    
    async def _attempt_automatic_rollback(self, migration: MigrationScript, environment: str, execution: MigrationExecution):
        """Automatic rollback attempt ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        logger.warning(f"üîÑ Attempting automatic rollback for {migration.name}")
        
        if not migration.down_script:
            execution.execution_log.append("‚ùå No rollback script available")
            return
        
        try:
            pool = self.db_connections[environment]
            
            async with pool.acquire() as conn:
                async with conn.transaction():
                    # Execute rollback script
                    await conn.execute(migration.down_script)
                    
                    # Update migration status
                    await conn.execute("""
                        UPDATE db_migrations 
                        SET status = 'rolled_back'
                        WHERE migration_id = $1 AND environment = $2
                    """, migration.id, environment)
                    
                    execution.status = MigrationStatus.ROLLED_BACK
                    execution.execution_log.append("‚úÖ Automatic rollback completed successfully")
                    
        except Exception as rollback_error:
            logger.error(f"‚ùå Automatic rollback failed: {rollback_error}")
            execution.execution_log.append(f"‚ùå Automatic rollback failed: {str(rollback_error)}")
    
    async def _log_migration_completion(self, execution: MigrationExecution):
        """Migration completion ‡§ï‡•ã audit log ‡§Æ‡•á‡§Ç record ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        pool = self.db_connections[execution.environment]
        
        async with pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO migration_audit_log (
                    migration_id, action, details, performed_by, performed_at
                ) VALUES ($1, $2, $3, $4, $5)
            """,
                execution.migration_id,
                "migration_completed",
                json.dumps({
                    "execution_id": execution.execution_id,
                    "status": execution.status.value,
                    "duration_minutes": (execution.end_time - execution.start_time).total_seconds() / 60,
                    "compliance_verified": execution.compliance_verified
                }),
                execution.executed_by,
                execution.end_time
            )
    
    def _calculate_checksum(self, script: str) -> str:
        """Script ‡§ï‡§æ checksum calculate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        return hashlib.sha256(script.encode()).hexdigest()
    
    async def get_migration_status(self, execution_id: str) -> Optional[MigrationExecution]:
        """Migration execution status return ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        return self.active_migrations.get(execution_id)
    
    async def list_pending_migrations(self, environment: str) -> List[MigrationScript]:
        """Pending migrations ‡§ï‡•Ä list return ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        pool = self.db_connections[environment]
        
        async with pool.acquire() as conn:
            executed_migrations = await conn.fetch(
                "SELECT migration_id FROM db_migrations WHERE environment = $1 AND status = 'completed'",
                environment
            )
            
            executed_ids = {row['migration_id'] for row in executed_migrations}
            
            pending = [
                migration for migration_id, migration in self.migration_history.items()
                if migration_id not in executed_ids
            ]
            
            return pending
    
    async def generate_migration_report(self, environment: str, days: int = 30) -> Dict:
        """Migration report generate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à"""
        pool = self.db_connections[environment]
        
        async with pool.acquire() as conn:
            # Recent migrations
            recent_migrations = await conn.fetch("""
                SELECT migration_id, name, status, executed_at, execution_time_ms
                FROM db_migrations
                WHERE environment = $1 AND executed_at >= NOW() - INTERVAL '%s days'
                ORDER BY executed_at DESC
            """, environment, days)
            
            # Migration statistics
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_migrations,
                    COUNT(*) FILTER (WHERE status = 'completed') as successful,
                    COUNT(*) FILTER (WHERE status = 'failed') as failed,
                    COUNT(*) FILTER (WHERE status = 'rolled_back') as rolled_back,
                    AVG(execution_time_ms) as avg_execution_time_ms
                FROM db_migrations
                WHERE environment = $1 AND executed_at >= NOW() - INTERVAL '%s days'
            """, environment, days)
            
            return {
                "environment": environment,
                "period_days": days,
                "recent_migrations": [dict(row) for row in recent_migrations],
                "statistics": dict(stats) if stats else {},
                "generated_at": datetime.now().isoformat()
            }

async def main():
    """Main function - Migration controller ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à"""
    logger.info("üóÑÔ∏è Starting Indian Banking Migration Controller...")
    
    controller = IndianBankingMigrationController()
    
    try:
        # Initialize controller
        await controller.initialize()
        
        # Example: List pending migrations
        pending = await controller.list_pending_migrations("staging")
        print(f"\n{'='*50}")
        print(f"Pending Migrations in Staging Environment")
        print(f"{'='*50}")
        
        for migration in pending:
            print(f"ID: {migration.id}")
            print(f"Name: {migration.name}")
            print(f"Type: {migration.migration_type.value}")
            print(f"Estimated Duration: {migration.estimated_duration_minutes} minutes")
            print(f"Requires Downtime: {migration.requires_downtime}")
            print(f"Affected Tables: {', '.join(migration.affected_tables)}")
            print("-" * 50)
        
        # Generate migration report
        report = await controller.generate_migration_report("staging", 30)
        print(f"\nMigration Report (Last 30 days):")
        print(f"Total Migrations: {report['statistics'].get('total_migrations', 0)}")
        print(f"Successful: {report['statistics'].get('successful', 0)}")
        print(f"Failed: {report['statistics'].get('failed', 0)}")
        print(f"Average Execution Time: {report['statistics'].get('avg_execution_time_ms', 0):.2f}ms")
        
    except Exception as e:
        logger.error(f"‚ùå Migration controller failed: {e}")
        raise

if __name__ == "__main__":
    # Run the async main function
    asyncio.run(main())