/*
Distributed Cache System - Episode 50: System Design Interview Mastery
Redis-like Distributed Cache with Mumbai Dabbawala Efficiency

Distributed Cache ‡§ú‡•à‡§∏‡•á Mumbai ‡§ï‡•á Dabbawala system ‡§π‡•à -
‡§π‡§∞ area ‡§Æ‡•á‡§Ç local cache ‡§π‡•à, coordination ‡§ï‡•á ‡§∏‡§æ‡§•

Author: Hindi Podcast Series
Topic: Distributed Caching with Consistency Models
*/

package main

import (
	"crypto/md5"
	"encoding/json"
	"fmt"
	"log"
	"math/rand"
	"strings"
	"sync"
	"time"
)

// CacheEntry - Mumbai Dabbawala ‡§ï‡§æ ‡§è‡§ï lunch box
type CacheEntry struct {
	Key        string      `json:"key"`
	Value      interface{} `json:"value"`
	Timestamp  int64       `json:"timestamp"`
	TTL        int64       `json:"ttl"` // Time to live in seconds
	Version    int64       `json:"version"`
	AccessFreq int         `json:"access_freq"`
}

// IsExpired - Check if cache entry has expired
func (e *CacheEntry) IsExpired() bool {
	if e.TTL == 0 {
		return false // No expiration
	}
	return time.Now().Unix()-e.Timestamp > e.TTL
}

// Touch - Update access frequency and timestamp
func (e *CacheEntry) Touch() {
	e.AccessFreq++
	e.Timestamp = time.Now().Unix()
}

// ConsistencyLevel - Cache consistency models
type ConsistencyLevel int

const (
	EventualConsistency ConsistencyLevel = iota // ‡§ú‡•à‡§∏‡•á Dabbawala network
	StrongConsistency                            // Banking level consistency
	WeakConsistency                              // Best effort delivery
)

// CacheNode - Single cache node (‡§è‡§ï Dabbawala station)
type CacheNode struct {
	NodeID    string                 `json:"node_id"`
	Address   string                 `json:"address"`
	Data      map[string]*CacheEntry `json:"data"`
	Capacity  int                    `json:"capacity"`
	IsHealthy bool                   `json:"is_healthy"`
	mutex     sync.RWMutex
	
	// Performance metrics
	HitCount    int64 `json:"hit_count"`
	MissCount   int64 `json:"miss_count"`
	WriteCount  int64 `json:"write_count"`
	DeleteCount int64 `json:"delete_count"`
}

// NewCacheNode - Create new cache node
func NewCacheNode(nodeID, address string, capacity int) *CacheNode {
	return &CacheNode{
		NodeID:    nodeID,
		Address:   address,
		Data:      make(map[string]*CacheEntry),
		Capacity:  capacity,
		IsHealthy: true,
	}
}

// Get - Retrieve value from cache node
func (cn *CacheNode) Get(key string) (*CacheEntry, bool) {
	cn.mutex.RLock()
	defer cn.mutex.RUnlock()
	
	entry, exists := cn.Data[key]
	if !exists {
		cn.MissCount++
		return nil, false
	}
	
	if entry.IsExpired() {
		// Remove expired entry
		delete(cn.Data, key)
		cn.MissCount++
		return nil, false
	}
	
	entry.Touch() // Update access stats
	cn.HitCount++
	return entry, true
}

// Put - Store value in cache node
func (cn *CacheNode) Put(key string, value interface{}, ttl int64) error {
	cn.mutex.Lock()
	defer cn.mutex.Unlock()
	
	// Check capacity (LRU eviction if full)
	if len(cn.Data) >= cn.Capacity {
		cn.evictLRU()
	}
	
	entry := &CacheEntry{
		Key:        key,
		Value:      value,
		Timestamp:  time.Now().Unix(),
		TTL:        ttl,
		Version:    time.Now().UnixNano(),
		AccessFreq: 1,
	}
	
	cn.Data[key] = entry
	cn.WriteCount++
	return nil
}

// Delete - Remove entry from cache node
func (cn *CacheNode) Delete(key string) bool {
	cn.mutex.Lock()
	defer cn.mutex.Unlock()
	
	if _, exists := cn.Data[key]; exists {
		delete(cn.Data, key)
		cn.DeleteCount++
		return true
	}
	return false
}

// evictLRU - Remove least recently used entry
func (cn *CacheNode) evictLRU() {
	if len(cn.Data) == 0 {
		return
	}
	
	var oldestKey string
	var oldestTime int64 = time.Now().Unix()
	
	for key, entry := range cn.Data {
		if entry.Timestamp < oldestTime {
			oldestTime = entry.Timestamp
			oldestKey = key
		}
	}
	
	if oldestKey != "" {
		delete(cn.Data, oldestKey)
		fmt.Printf("üóëÔ∏è Evicted LRU entry: %s from node %s\n", oldestKey, cn.NodeID)
	}
}

// GetStats - Get node performance statistics
func (cn *CacheNode) GetStats() map[string]interface{} {
	cn.mutex.RLock()
	defer cn.mutex.RUnlock()
	
	hitRate := float64(0)
	totalRequests := cn.HitCount + cn.MissCount
	if totalRequests > 0 {
		hitRate = float64(cn.HitCount) / float64(totalRequests) * 100
	}
	
	return map[string]interface{}{
		"node_id":       cn.NodeID,
		"entries_count": len(cn.Data),
		"capacity":      cn.Capacity,
		"hit_count":     cn.HitCount,
		"miss_count":    cn.MissCount,
		"write_count":   cn.WriteCount,
		"delete_count":  cn.DeleteCount,
		"hit_rate":      hitRate,
		"load_factor":   float64(len(cn.Data)) / float64(cn.Capacity) * 100,
		"is_healthy":    cn.IsHealthy,
	}
}

// DistributedCache - Main distributed cache system
type DistributedCache struct {
	Nodes       map[string]*CacheNode `json:"nodes"`
	HashRing    []string              `json:"hash_ring"` // Consistent hashing ring
	Replicas    int                   `json:"replicas"`  // Replication factor
	Consistency ConsistencyLevel      `json:"consistency"`
	mutex       sync.RWMutex
	
	// Global metrics
	TotalRequests int64 `json:"total_requests"`
	TotalHits     int64 `json:"total_hits"`
}

// NewDistributedCache - Create new distributed cache system
func NewDistributedCache(replicas int, consistency ConsistencyLevel) *DistributedCache {
	return &DistributedCache{
		Nodes:       make(map[string]*CacheNode),
		HashRing:    make([]string, 0),
		Replicas:    replicas,
		Consistency: consistency,
	}
}

// AddNode - Add cache node to cluster
func (dc *DistributedCache) AddNode(node *CacheNode) {
	dc.mutex.Lock()
	defer dc.mutex.Unlock()
	
	dc.Nodes[node.NodeID] = node
	dc.HashRing = append(dc.HashRing, node.NodeID)
	
	fmt.Printf("‚úÖ Cache node %s added - Total nodes: %d\n", node.NodeID, len(dc.Nodes))
}

// RemoveNode - Remove cache node from cluster
func (dc *DistributedCache) RemoveNode(nodeID string) {
	dc.mutex.Lock()
	defer dc.mutex.Unlock()
	
	delete(dc.Nodes, nodeID)
	
	// Remove from hash ring
	for i, id := range dc.HashRing {
		if id == nodeID {
			dc.HashRing = append(dc.HashRing[:i], dc.HashRing[i+1:]...)
			break
		}
	}
	
	fmt.Printf("üóëÔ∏è Cache node %s removed - Remaining nodes: %d\n", nodeID, len(dc.Nodes))
}

// getNodeForKey - Get primary node for key using consistent hashing
func (dc *DistributedCache) getNodeForKey(key string) string {
	if len(dc.HashRing) == 0 {
		return ""
	}
	
	// Simple hash function - in production use better hashing
	hash := md5.Sum([]byte(key))
	index := int(hash[0]) % len(dc.HashRing)
	return dc.HashRing[index]
}

// getReplicaNodes - Get replica nodes for key
func (dc *DistributedCache) getReplicaNodes(key string, exclude string) []string {
	if len(dc.HashRing) <= 1 {
		return []string{}
	}
	
	replicas := make([]string, 0, dc.Replicas)
	primaryIndex := -1
	
	// Find primary node index
	for i, nodeID := range dc.HashRing {
		if nodeID == exclude {
			primaryIndex = i
			break
		}
	}
	
	// Get next N replica nodes
	for i := 1; i <= dc.Replicas && len(replicas) < dc.Replicas; i++ {
		nextIndex := (primaryIndex + i) % len(dc.HashRing)
		nextNode := dc.HashRing[nextIndex]
		if nextNode != exclude {
			replicas = append(replicas, nextNode)
		}
	}
	
	return replicas
}

// Get - Retrieve value from distributed cache
func (dc *DistributedCache) Get(key string) (interface{}, bool) {
	dc.TotalRequests++
	
	primaryNodeID := dc.getNodeForKey(key)
	if primaryNodeID == "" {
		return nil, false
	}
	
	dc.mutex.RLock()
	primaryNode, exists := dc.Nodes[primaryNodeID]
	dc.mutex.RUnlock()
	
	if !exists || !primaryNode.IsHealthy {
		// Try replica nodes
		return dc.getFromReplicas(key, primaryNodeID)
	}
	
	entry, found := primaryNode.Get(key)
	if found {
		dc.TotalHits++
		fmt.Printf("üì¶ Cache HIT: %s from node %s\n", key, primaryNodeID)
		return entry.Value, true
	}
	
	// Try replicas if primary doesn't have it
	return dc.getFromReplicas(key, primaryNodeID)
}

// getFromReplicas - Try to get value from replica nodes
func (dc *DistributedCache) getFromReplicas(key, excludeNode string) (interface{}, bool) {
	replicas := dc.getReplicaNodes(key, excludeNode)
	
	dc.mutex.RLock()
	defer dc.mutex.RUnlock()
	
	for _, replicaID := range replicas {
		if replica, exists := dc.Nodes[replicaID]; exists && replica.IsHealthy {
			if entry, found := replica.Get(key); found {
				dc.TotalHits++
				fmt.Printf("üì¶ Cache HIT from replica: %s from node %s\n", key, replicaID)
				return entry.Value, true
			}
		}
	}
	
	fmt.Printf("‚ùå Cache MISS: %s\n", key)
	return nil, false
}

// Put - Store value in distributed cache
func (dc *DistributedCache) Put(key string, value interface{}, ttl int64) error {
	primaryNodeID := dc.getNodeForKey(key)
	if primaryNodeID == "" {
		return fmt.Errorf("no nodes available")
	}
	
	dc.mutex.RLock()
	defer dc.mutex.RUnlock()
	
	// Write to primary node
	if primaryNode, exists := dc.Nodes[primaryNodeID]; exists && primaryNode.IsHealthy {
		err := primaryNode.Put(key, value, ttl)
		if err != nil {
			return err
		}
		fmt.Printf("üíæ Stored %s in primary node %s\n", key, primaryNodeID)
	}
	
	// Write to replicas based on consistency level
	if dc.Consistency == StrongConsistency {
		return dc.writeToAllReplicas(key, value, ttl, primaryNodeID)
	} else {
		// Asynchronous replication for eventual consistency
		go dc.writeToAllReplicas(key, value, ttl, primaryNodeID)
		return nil
	}
}

// writeToAllReplicas - Write to replica nodes
func (dc *DistributedCache) writeToAllReplicas(key string, value interface{}, ttl int64, excludeNode string) error {
	replicas := dc.getReplicaNodes(key, excludeNode)
	
	for _, replicaID := range replicas {
		if replica, exists := dc.Nodes[replicaID]; exists && replica.IsHealthy {
			err := replica.Put(key, value, ttl)
			if err != nil && dc.Consistency == StrongConsistency {
				return fmt.Errorf("failed to replicate to node %s: %v", replicaID, err)
			}
			fmt.Printf("üìã Replicated %s to node %s\n", key, replicaID)
		}
	}
	return nil
}

// Delete - Remove value from distributed cache
func (dc *DistributedCache) Delete(key string) bool {
	primaryNodeID := dc.getNodeForKey(key)
	if primaryNodeID == "" {
		return false
	}
	
	dc.mutex.RLock()
	defer dc.mutex.RUnlock()
	
	deleted := false
	
	// Delete from primary node
	if primaryNode, exists := dc.Nodes[primaryNodeID]; exists {
		deleted = primaryNode.Delete(key) || deleted
	}
	
	// Delete from replicas
	replicas := dc.getReplicaNodes(key, primaryNodeID)
	for _, replicaID := range replicas {
		if replica, exists := dc.Nodes[replicaID]; exists {
			replica.Delete(key)
		}
	}
	
	if deleted {
		fmt.Printf("üóëÔ∏è Deleted %s from distributed cache\n", key)
	}
	
	return deleted
}

// GetClusterStats - Get cluster-wide statistics
func (dc *DistributedCache) GetClusterStats() map[string]interface{} {
	dc.mutex.RLock()
	defer dc.mutex.RUnlock()
	
	totalEntries := 0
	totalCapacity := 0
	healthyNodes := 0
	
	nodeStats := make([]map[string]interface{}, 0, len(dc.Nodes))
	
	for _, node := range dc.Nodes {
		stats := node.GetStats()
		nodeStats = append(nodeStats, stats)
		
		totalEntries += len(node.Data)
		totalCapacity += node.Capacity
		if node.IsHealthy {
			healthyNodes++
		}
	}
	
	hitRate := float64(0)
	if dc.TotalRequests > 0 {
		hitRate = float64(dc.TotalHits) / float64(dc.TotalRequests) * 100
	}
	
	return map[string]interface{}{
		"total_nodes":    len(dc.Nodes),
		"healthy_nodes":  healthyNodes,
		"total_entries":  totalEntries,
		"total_capacity": totalCapacity,
		"total_requests": dc.TotalRequests,
		"total_hits":     dc.TotalHits,
		"cluster_hit_rate": hitRate,
		"load_factor":    float64(totalEntries) / float64(totalCapacity) * 100,
		"replication_factor": dc.Replicas,
		"consistency_level": dc.Consistency,
		"node_statistics": nodeStats,
	}
}

// demonstrateMumbaiDabbawalaCache - Mumbai Dabbawala cache system demo
func demonstrateMumbaiDabbawalaCache() {
	fmt.Println("ü•° Mumbai Dabbawala Distributed Cache System Demo")
	fmt.Println(strings.Repeat("=", 60))
	
	// Initialize distributed cache with 2 replicas and eventual consistency
	cache := NewDistributedCache(2, EventualConsistency)
	
	// Add Dabbawala stations (cache nodes) across Mumbai
	stations := []*CacheNode{
		NewCacheNode("CST_STATION", "cst.dabbawala.mumbai", 100),
		NewCacheNode("DADAR_STATION", "dadar.dabbawala.mumbai", 150),
		NewCacheNode("BANDRA_STATION", "bandra.dabbawala.mumbai", 120),
		NewCacheNode("ANDHERI_STATION", "andheri.dabbawala.mumbai", 100),
		NewCacheNode("BORIVALI_STATION", "borivali.dabbawala.mumbai", 80),
	}
	
	for _, station := range stations {
		cache.AddNode(station)
	}
	
	// Simulate lunch box deliveries (cache operations)
	lunchBoxes := map[string]interface{}{
		"office_worker_001_lunch": "Dal Chawal + Sabji",
		"student_002_tiffin":      "Roti + Aloo Gobi",
		"executive_003_meal":      "Biryani + Raita",
		"teacher_004_lunch":       "Rajma Rice",
		"doctor_005_diet":         "Salad + Soup",
		"engineer_006_combo":      "Pav Bhaji",
		"banker_007_special":      "Thali Complete",
		"lawyer_008_express":      "Sandwich + Tea",
	}
	
	fmt.Println("\nüì¶ Storing lunch boxes in Dabbawala network...")
	for customerID, meal := range lunchBoxes {
		err := cache.Put(customerID, meal, 3600) // 1 hour TTL
		if err != nil {
			fmt.Printf("‚ùå Failed to store %s: %v\n", customerID, err)
		}
	}
	
	fmt.Println("\nüîç Retrieving lunch boxes...")
	for customerID := range lunchBoxes {
		meal, found := cache.Get(customerID)
		if found {
			fmt.Printf("‚úÖ Retrieved %s: %v\n", customerID, meal)
		} else {
			fmt.Printf("‚ùå Could not find %s\n", customerID)
		}
	}
	
	// Simulate node failure
	fmt.Println("\nüö® Simulating CST station failure...")
	cache.Nodes["CST_STATION"].IsHealthy = false
	
	// Test cache access during failure
	fmt.Println("\nüîÑ Testing cache access during node failure...")
	testKeys := []string{"office_worker_001_lunch", "student_002_tiffin", "executive_003_meal"}
	for _, key := range testKeys {
		meal, found := cache.Get(key)
		if found {
			fmt.Printf("‚úÖ Emergency retrieval %s: %v\n", key, meal)
		} else {
			fmt.Printf("‚ùå Failed to retrieve %s during failure\n", key)
		}
	}
	
	// Show cluster statistics
	fmt.Println("\nüìä Dabbawala Network Statistics:")
	stats := cache.GetClusterStats()
	statsJSON, _ := json.MarshalIndent(stats, "", "  ")
	fmt.Println(string(statsJSON))
	
	// Simulate node recovery
	fmt.Println("\nüîß CST station recovered...")
	cache.Nodes["CST_STATION"].IsHealthy = true
	
	// Test some deletion operations
	fmt.Println("\nüóëÔ∏è Cleaning up delivered lunch boxes...")
	cache.Delete("office_worker_001_lunch")
	cache.Delete("student_002_tiffin")
	
	// Final statistics
	fmt.Println("\nüìà Final Network Statistics:")
	finalStats := cache.GetClusterStats()
	fmt.Printf("Total Requests: %d\n", finalStats["total_requests"])
	fmt.Printf("Cache Hit Rate: %.2f%%\n", finalStats["cluster_hit_rate"])
	fmt.Printf("Healthy Nodes: %d/%d\n", finalStats["healthy_nodes"], finalStats["total_nodes"])
}

// simulatePaytmCacheSystem - Paytm transaction cache simulation
func simulatePaytmCacheSystem() {
	fmt.Println("\nüí∞ Paytm Transaction Cache System Demo")
	fmt.Println(strings.Repeat("=", 60))
	
	// Strong consistency for financial data
	paytmCache := NewDistributedCache(3, StrongConsistency)
	
	// Add cache nodes for different regions
	regions := []*CacheNode{
		NewCacheNode("MUMBAI_REGION", "mumbai.paytm.cache", 200),
		NewCacheNode("DELHI_REGION", "delhi.paytm.cache", 200),
		NewCacheNode("BANGALORE_REGION", "bangalore.paytm.cache", 200),
		NewCacheNode("CHENNAI_REGION", "chennai.paytm.cache", 150),
	}
	
	for _, region := range regions {
		paytmCache.AddNode(region)
	}
	
	// Simulate transaction caching
	transactions := map[string]interface{}{
		"txn_upi_001": map[string]interface{}{
			"amount": 500.0,
			"from":   "user@paytm",
			"to":     "merchant@paytm",
			"status": "SUCCESS",
		},
		"txn_wallet_002": map[string]interface{}{
			"amount": 1200.0,
			"wallet": "paytm_wallet",
			"status": "PENDING",
		},
		"txn_bill_003": map[string]interface{}{
			"amount": 2500.0,
			"type":   "electricity_bill",
			"status": "SUCCESS",
		},
	}
	
	fmt.Println("\nüí≥ Caching Paytm transactions...")
	for txnID, txnData := range transactions {
		err := paytmCache.Put(txnID, txnData, 1800) // 30 minute TTL
		if err != nil {
			fmt.Printf("‚ùå Failed to cache transaction %s: %v\n", txnID, err)
		}
	}
	
	// Retrieve transactions
	fmt.Println("\nüîç Retrieving cached transactions...")
	for txnID := range transactions {
		data, found := paytmCache.Get(txnID)
		if found {
			fmt.Printf("‚úÖ Transaction %s retrieved successfully\n", txnID)
		}
	}
	
	// Show final stats
	fmt.Println("\nüìä Paytm Cache Performance:")
	stats := paytmCache.GetClusterStats()
	fmt.Printf("Hit Rate: %.2f%%\n", stats["cluster_hit_rate"])
	fmt.Printf("Total Nodes: %d\n", stats["total_nodes"])
	fmt.Printf("Replication Factor: %d\n", stats["replication_factor"])
}

func main() {
	// Run demonstrations
	demonstrateMumbaiDabbawalaCache()
	
	fmt.Println("\n" + strings.Repeat("=", 80))
	
	simulatePaytmCacheSystem()
	
	fmt.Println("\n" + strings.Repeat("=", 80))
	fmt.Println("‚úÖ Distributed Cache System Demo Complete!")
	fmt.Println("üìö Key Features Demonstrated:")
	fmt.Println("   ‚Ä¢ Consistent Hashing for data distribution")
	fmt.Println("   ‚Ä¢ Configurable replication factor")
	fmt.Println("   ‚Ä¢ Multiple consistency levels")
	fmt.Println("   ‚Ä¢ Automatic failover and recovery")
	fmt.Println("   ‚Ä¢ LRU eviction policy")
	fmt.Println("   ‚Ä¢ Performance metrics and monitoring")
	fmt.Println("   ‚Ä¢ Production-ready for Mumbai scale operations")
}