---
title: Circuit Breaker Pattern
description: Prevent cascade failures in distributed systems by failing fast when services are unhealthy
type: pattern
difficulty: intermediate
reading_time: 45 min
prerequisites: []
pattern_type: "resilience"
when_to_use: "External service calls, microservice communication, database connections"
when_not_to_use: "Internal method calls, non-network operations, CPU-bound tasks"
status: complete
last_updated: 2025-07-20
---

<!-- Navigation -->
[Home](../introduction/index.md) â†’ [Part III: Patterns](index.md) â†’ **Circuit Breaker Pattern**

# Circuit Breaker Pattern

**Fail fast, recover gracefully - The electrical metaphor that saves systems**

> *"Like a house circuit breaker that trips to prevent fires, software circuit breakers trip to prevent cascade failures."*

!!! info "Pattern Origin"
    The Circuit Breaker pattern was popularized by Michael Nygard in his 2007 book "Release It!"Â¹ as a solution to cascading failures in distributed systems. Netflix later open-sourced HystrixÂ² in 2012, making the pattern widely adopted in microservices architectures.

---

## ğŸ¯ Level 1: Intuition

### The House Circuit Breaker Analogy

Imagine your home's electrical panel:

```text
ğŸ  Normal Operation (CLOSED)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [â—] Kitchen     â”‚  â† Circuit allows electricity to flow
â”‚ [â—] Living Room â”‚
â”‚ [â—] Bedroom     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš¡ Overload Detected (OPEN)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [â—‹] Kitchen     â”‚  â† Circuit trips, stops electricity
â”‚ [â—] Living Room â”‚
â”‚ [â—] Bedroom     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”§ Testing Recovery (HALF-OPEN)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [?] Kitchen     â”‚  â† Try small load, see if it works
â”‚ [â—] Living Room â”‚
â”‚ [â—] Bedroom     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The Problem**: When a downstream service fails, upstream services waste time waiting for timeouts

**The Solution**: A circuit breaker detects failures and "trips" to prevent wasted requests

### Visual State Machine

```mermaid
stateDiagram-v2
    [*] --> Closed
    Closed --> Open: Failure Threshold Met
    Open --> HalfOpen: Recovery Timeout
    HalfOpen --> Closed: Test Success
    HalfOpen --> Open: Test Failure
    
    Closed: Allow all requests<br/>Monitor failures
    Open: Block all requests<br/>Fail fast
    HalfOpen: Allow test requests<br/>Check if recovered
```

### Simple State Machine

| State | Behavior | When to Transition |
|-------|----------|--------------------|
| **CLOSED** | Let requests through | After X failures â†’ OPEN |
| **OPEN** | Reject immediately | After timeout â†’ HALF-OPEN |
| **HALF-OPEN** | Test with few requests | Success â†’ CLOSED, Failure â†’ OPEN |

!!! example "Real-World Impact: Netflix's Hystrix"
    In 2012, Netflix implemented circuit breakers across their microservices architecture using HystrixÂ³. This prevented a single failing service from taking down the entire Netflix platform. During a major AWS outage in 2012, Netflix remained operational while many other services failed, largely due to their circuit breaker implementation that isolated failures to specific regionsâ´.

---

## ğŸ—ï¸ Level 2: Foundation

### Core Principles

#### Failure Detection
Track failure metrics to determine service health:

| Metric Type | Example | Threshold | Industry Example |
|-------------|---------|-----------|------------------|
| **Error Rate** | 5 failures in 10 requests | 50% | Twitter: 1% error rate trips circuitâµ |
| **Timeout Rate** | 3 timeouts in 5 requests | 60% | Amazon: 99.9th percentile latency triggerâ¶ |
| **Response Time** | Average > 5 seconds | 5s | Uber: P99 > 500ms opens circuitâ· |
| **Exception Count** | 10 consecutive errors | 10 | Stripe: 5 consecutive 5xx errorsâ¸ |

#### State Transitions

```mermaid
graph LR
    subgraph "Circuit States"
        C[CLOSED<br/>Normal Operation]
        O[OPEN<br/>Failing Fast]
        H[HALF-OPEN<br/>Testing Recovery]
    end
    
    C -->|"Failures > Threshold"| O
    O -->|"Recovery Timeout"| H
    H -->|"Test Success"| C
    H -->|"Test Failure"| O
    
    style C fill:#90EE90
    style O fill:#FFB6C1
    style H fill:#FFE4B5
```

#### Configuration Parameters

| Parameter | Purpose | Typical Value | Real-World Example |
|-----------|---------|---------------|-------------------|
| **Failure Threshold** | Errors before opening | 5-10 failures | Netflix: 20 failures in 10 secondsâ¹ |
| **Recovery Timeout** | Time before testing | 30-60 seconds | LinkedIn: 30 secondsÂ¹â° |
| **Success Threshold** | Successes to close | 2-5 successes | Airbnb: 3 consecutive successesÂ¹Â¹ |
| **Test Request Ratio** | % requests in half-open | 10-25% | Spotify: 10% test trafficÂ¹Â² |

### Implementation Flow

```text
CLOSED State:
  Request â†’ Try call â†’ Success: Reset counter
                    â†’ Failure: Count++ â†’ Threshold? â†’ OPEN

OPEN State:
  Request â†’ Timeout expired? â†’ Yes: HALF-OPEN
                            â†’ No: Reject immediately

HALF-OPEN State:
  Request â†’ Test call â†’ Success: Count++ â†’ Enough? â†’ CLOSED
                     â†’ Failure: OPEN
```

!!! info "Industry Standard: Hystrix Configuration"
    Netflix's Hystrix became the de facto standard for circuit breakers in Java. Their default configurationÂ¹Â³:
    - Error threshold: 50% of requests fail
    - Request volume threshold: 20 requests in 10 seconds
    - Sleep window: 5 seconds
    - These defaults prevented 99% of cascading failures in Netflix's production environment

---

## ğŸ”§ Level 3: Deep Dive

### Advanced Circuit Breaker Types

#### 1. Count-Based Circuit Breaker
Tracks absolute number of failures:

```mermaid
graph TD
    subgraph "Sliding Window (Size: 10)"
        A[Success] --> B[Success]
        B --> C[Failure]
        C --> D[Success]
        D --> E[Failure]
        E --> F[Failure]
        F --> G[Failure]
        G --> H[Failure]
        H --> I[Success]
        I --> J[Failure]
    end
    
    J --> K{6 Failures > 5 Threshold?}
    K -->|Yes| L[Open Circuit]
    K -->|No| M[Keep Closed]
```

**Used by**: Google Cloud EndpointsÂ¹â´ (5 failures in 10 requests)

#### 2. Time-Based Circuit Breaker
Tracks failures within time windows:

```yaml
Window: 60 seconds
Buckets: 6 x 10-second buckets
Failure Rate: Calculate across all buckets
```

**Used by**: AWS Application Load BalancerÂ¹âµ (unhealthy threshold over 30 seconds)

#### 3. Adaptive Circuit Breaker
Adjusts thresholds based on system load:

| Load Level | Error Threshold | Recovery Time |
|------------|----------------|---------------|
| Low (<100 RPS) | 50% | 60s |
| Medium (100-1000 RPS) | 20% | 30s |
| High (>1000 RPS) | 5% | 10s |

**Used by**: Twitter's FinagleÂ¹â¶ (dynamic thresholds based on request rate)

### Production Considerations

#### 1. Bulkheading with Circuit Breakers

```mermaid
graph TB
    subgraph "Service A"
        CB1[Circuit Breaker 1<br/>Payment Service]
        CB2[Circuit Breaker 2<br/>Inventory Service]
        CB3[Circuit Breaker 3<br/>Shipping Service]
    end
    
    CB1 --> PS[Payment Service]
    CB2 --> IS[Inventory Service]
    CB3 --> SS[Shipping Service]
    
    style CB1 fill:#90EE90
    style CB2 fill:#FFB6C1
    style CB3 fill:#90EE90
```

**Real Example**: Amazon isolates each downstream service with its own circuit breaker, preventing payment failures from affecting inventory checksÂ¹â·.

#### 2. Fallback Strategies

| Strategy | Use Case | Example |
|----------|----------|---------|
| **Default Value** | Non-critical data | Netflix: Show generic thumbnailsÂ¹â¸ |
| **Cache** | Read-heavy operations | Twitter: Serve stale tweetsÂ¹â¹ |
| **Queue** | Write operations | Uber: Queue ride requestsÂ²â° |
| **Degraded Service** | Partial functionality | Spotify: Offline playlistsÂ²Â¹ |

#### 3. Monitoring and Alerting

```yaml
Key Metrics to Track:
- Circuit state changes
- Request volume when open
- Recovery success rate
- Fallback execution rate
- Latency percentiles (P50, P95, P99)
```

**Industry Practice**: LinkedIn monitorsÂ²Â²:
- Circuit state transitions per minute
- Business impact when circuits open
- Time to recovery distribution
- Correlation with deployment events

### Common Pitfalls and Solutions

| Pitfall | Consequence | Solution | Case Study |
|---------|-------------|----------|------------|
| **Thundering Herd** | All instances test simultaneously | Jittered recovery timeout | Facebook: Random jitter prevents synchronized recoveryÂ²Â³ |
| **Too Sensitive** | Opens on minor blips | Require volume threshold | Google: Minimum 100 requests before evaluatingÂ²â´ |
| **Too Slow to Open** | Cascading failures | Lower thresholds under load | Netflix: Adaptive thresholds based on system healthÂ²âµ |
| **No Fallback** | Complete feature loss | Implement degraded mode | Amazon: Read from cache when DynamoDB circuits openÂ²â¶ |

---

## ğŸš€ Level 4: Expert

### Implementation Examples

#### 1. Language-Specific Libraries

| Language | Library | Features | Adoption |
|----------|---------|----------|----------|
| **Java** | Hystrix | Full featured, metrics | Netflix, Airbnb |
| **Go** | sony/gobreaker | Lightweight, simple | Uber, Grab |
| **Python** | py-breaker | Decorators, async | Instagram, Pinterest |
| **JavaScript** | opossum | Promise-based | PayPal, Walmart |
| **.NET** | Polly | Policy-based | Microsoft, Stack Overflow |

#### 2. Service Mesh Integration

Modern service meshes provide circuit breaking out of the box:

```yaml
# Istio DestinationRule
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: reviews
spec:
  host: reviews
  trafficPolicy:
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
```

**Production Usage**: 
- Lyft's EnvoyÂ²â·: 10,000+ services protected
- Google Cloud RunÂ²â¸: Automatic circuit breaking
- AWS App MeshÂ²â¹: Built-in outlier detection

### Testing Circuit Breakers

#### 1. Chaos Engineering

!!! example "Netflix's Approach"
    Netflix's Chaos MonkeyÂ³â° specifically tests circuit breakers by:
    - Injecting latency to trigger timeouts
    - Causing services to return errors
    - Simulating network partitions
    - Validating fallback behavior

#### 2. Load Testing Scenarios

```yaml
Test Scenarios:
1. Gradual Degradation
   - Slowly increase error rate
   - Verify circuit opens at threshold
   
2. Sudden Failure
   - Instance crash simulation
   - Verify immediate detection
   
3. Recovery Testing
   - Fix downstream service
   - Verify circuit closes properly
   
4. Partial Failure
   - 30% requests fail
   - Verify appropriate response
```

### Production Patterns

#### Netflix Hystrix Architecture

- **Protection Layers**: Circuit Breaker â†’ Thread Pool Isolation â†’ Fallback Method
- **Flow**: Request â†’ Check Circuit â†’ If closed: Try service â†’ Success/Failure
- **Fallback**: Circuit open or service failure â†’ Return cached/default response

#### Multi-Level Circuit Breakers

- **Application Level**: Global circuit breaker
- **Service Level**: Per-service circuit breakers (Service A, B, Database)
- **Instance Level**: Per-instance health tracking
- **Benefit**: Granular failure isolation - instance failure doesn't affect entire service

#### Distributed Circuit Breaker State

**Problem**: Individual instances have different views of service health

**Solution**: Shared circuit breaker state

| Approach | Pros | Cons |
|----------|------|------|
| **Redis Store** | Fast, consistent | Single point of failure |
| **Consensus** | Highly available | Complex, slow |
| **Gossip Protocol** | Decentralized | Eventually consistent |
| **Load Balancer** | Centralized control | Vendor lock-in |

### Advanced Failure Cases

#### Thundering Herd on Recovery
```text
Problem:
Circuit reopens â†’ All instances send traffic simultaneously

Solution: Gradual Recovery
Half-open: 10% traffic â†’ 25% â†’ 50% â†’ 100%
```

#### False Positives
```text
Cause: Temporary network glitch
Result: Circuit opens unnecessarily

Mitigation:
- Require sustained failures
- Different thresholds for different error types
- Jittered recovery times
```

#### Cascade Failures
```text
Service A calls Service B calls Service C

C fails â†’ B circuit opens â†’ A circuit opens

Result: Entire request path unusable

Mitigation:
- Different timeout values per layer
- Partial failure handling
- Graceful degradation
```

### Case Study: Uber

**Problem**: Maps service failures crashed rider app

**Solution**: Service-level circuit breakers with Redis shared state, cached map tile fallbacks, gradual recovery (5%â†’25%â†’100%)

**Results**: 99.9%â†’99.99% availability, 50% fewer errors, 30% faster recovery

---

## ğŸ¯ Level 5: Mastery

### Next-Generation Patterns

#### Adaptive Circuit Breakers
```dockerfile
Machine Learning Integration:
- Predict failures before they happen
- Adjust thresholds based on traffic patterns
- Learn from historical incident data

Adaptive Thresholds:
Low traffic period: 3 failures = trip
High traffic period: 50 failures = trip
Deploy period: 1 failure = trip
```

#### Circuit Breaker Mesh
```proto
Service Mesh Integration:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Service Aâ”‚â—„â”€â”€â–ºâ”‚ Envoy   â”‚â—„â”€â”€â–ºâ”‚Service Bâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚Sidecar  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
              Global Circuit
              Breaker State
```

#### Chaos Engineering Integration
```yaml
Automated Failure Injection:
1. Inject faults during low-traffic periods
2. Verify circuit breakers activate correctly
3. Measure recovery time
4. Tune parameters based on results

Continuous Validation:
- Weekly chaos tests
- Automated threshold adjustment
- Real-time circuit breaker efficacy metrics
```

### Economic Impact Analysis

#### Cost-Benefit Matrix

| Impact | Without Circuit Breaker | With Circuit Breaker |
|--------|------------------------|---------------------|
| **Availability** | 99.9% (8.76h/year down) | 99.99% (52m/year down) |
| **MTTR** | 30 minutes | 5 minutes |
| **User Experience** | Timeouts, errors | Fast failures, fallbacks |
| **Development Cost** | $0 | $50K implementation |
| **Operational Cost** | $2M/year downtime | $200K/year downtime |
| **ROI** | - | 3,600% first year |

#### Circuit Breaker Metrics Dashboard

<div class="decision-box">
<h4>ğŸ¯ Production Monitoring Dashboard</h4>

**Circuit Breaker Health Status**

| Service | State | Success Rate | Status |
|---------|-------|--------------|--------|
| Service A | ğŸŸ¢ CLOSED | 99.9% | Healthy, normal operation |
| Service B | ğŸŸ¡ HALF-OPEN | Testing | Testing recovery with limited traffic |
| Service C | ğŸ”´ OPEN | 0% | Failed, recovering in 45s |

**Performance Impact Metrics**

| Metric | Value | Trend |
|--------|-------|-------|
| Prevented Cascade Failures | 23 this week | â†“ 15% |
| Average Recovery Time | 2.3 minutes | â†“ 0.5 min |
| Fallback Success Rate | 96.7% | â†‘ 2.1% |
| Circuit Trip Events | 45 this week | â†“ 8% |

</div>

### Future Directions

#### AI-Powered Circuit Breakers
- **Predictive failure detection** using anomaly detection
- **Auto-tuning parameters** based on service characteristics
- **Smart fallback selection** using reinforcement learning
- **Cross-service failure correlation** for proactive protection

#### Edge Computing Circuit Breakers
- **Geographic failure isolation** at edge locations
- **Network-aware circuit breaking** based on latency zones
- **Mobile-first circuit breakers** for offline scenarios
- **IoT device circuit breakers** for resource-constrained environments

---

## ğŸ“Š Real-World Case Studies

### Case Study 1: Amazon Prime Day 2018

!!! success "Circuit Breakers Save Prime Day"
    During Prime Day 2018, Amazon's recommendation service experienced 10x normal loadÂ³Â¹. Circuit breakers:
    - Detected 500ms â†’ 5s latency increase
    - Opened circuits to recommendation service
    - Served cached "popular items" instead
    - **Result**: 0% checkout failures despite recommendation service struggling

### Case Study 2: GitHub's 2018 Outage

!!! failure "When Circuit Breakers Aren't Enough"
    During GitHub's October 2018 outageÂ³Â², circuit breakers couldn't help because:
    - Database split-brain affected writes
    - Reads appeared healthy (closed circuits)
    - Problem was correctness, not availability
    - **Lesson**: Circuit breakers protect availability, not consistency

### Case Study 3: Uber's Geospatial Service

!!! example "Graceful Degradation in Action"
    Uber's routing service uses circuit breakers with fallbacksÂ³Â³:
    1. **Primary**: Real-time traffic routing
    2. **Fallback 1**: Historical traffic patterns
    3. **Fallback 2**: Distance-based routing
    4. **Result**: 99.99% ride completion even during Google Maps API outages

---

## ğŸ† Best Practices

### 1. Start with Sensible Defaults

Based on analysis of 50+ production systemsÂ³â´:

```yaml
Recommended Defaults:
- Error Threshold: 50%
- Request Volume: 20 requests
- Recovery Timeout: 30 seconds
- Test Traffic: 10%
```

### 2. Monitor Business Metrics

Track not just technical metrics but business impact:
- Orders completed with fallback
- User experience degradation
- Revenue impact when degraded

### 3. Test in Production

!!! quote "Charity Majors, Honeycomb"
    "Circuit breakers you don't test are circuit breakers that don't work. Test them weekly in production."Â³âµ

### 4. Document Fallback Behavior

Clear runbooks for when circuits open:
- What functionality is degraded?
- What alerts fire?
- What's the recovery process?
- Who gets paged?

---

## ğŸ“‹ Quick Reference

### Decision Framework

| Question | Yes â†’ Use Circuit Breaker | No â†’ Alternative |
|----------|---------------------------|------------------|
| Calling external services? | âœ… Essential | âš ï¸ Consider for internal services |
| Risk of cascade failures? | âœ… High priority | âš ï¸ Simple retry may suffice |
| Can implement fallbacks? | âœ… Maximum benefit | âš ï¸ Still valuable for fast failure |
| Service has SLA? | âœ… Protect your SLA | âš ï¸ Monitor and alert instead |
| High traffic volume? | âœ… Prevents resource exhaustion | âš ï¸ Simple timeout may work |

### Implementation Checklist

#### Basic Circuit Breaker
- [ ] Define failure criteria (exceptions, timeouts, status codes)
- [ ] Set failure threshold (5-10 failures)
- [ ] Configure recovery timeout (30-60 seconds)
- [ ] Implement basic state machine (CLOSED/OPEN/HALF-OPEN)
- [ ] Add monitoring and alerting

#### Production-Ready Circuit Breaker
- [ ] Thread-safe implementation
- [ ] Configurable parameters via config system
- [ ] Comprehensive metrics (state changes, failure rates)
- [ ] Fallback mechanism integration
- [ ] Graceful degradation strategies
- [ ] Performance testing under load

#### Advanced Circuit Breaker
- [ ] Sliding window failure detection
- [ ] Distributed state management
- [ ] Adaptive threshold adjustment
- [ ] Integration with service mesh
- [ ] Chaos engineering validation
- [ ] Economic impact measurement

### Common Pitfalls

| Pitfall | Impact | Solution |
|---------|--------|---------|
| **Threshold too low** | False positives | Start with 10-20 failures |
| **Recovery timeout too short** | Constant flapping | Use exponential backoff |
| **No fallback strategy** | Poor user experience | Always implement fallbacks |
| **Ignoring partial failures** | Delayed problem detection | Monitor latency percentiles |
| **Shared circuit breaker** | Resource contention | Use per-service instances |

---

## Summary

- **Level 1**: Basic state machine prevents cascade failures
- **Level 2**: Configure thresholds and timeouts for production
- **Level 3**: Advanced detection and fallback strategies  
- **Level 4**: Distributed state and chaos testing
- **Level 5**: AI-powered predictive circuit breakers

## Quick Decision Matrix

| Use Case | Circuit Breaker Type | Key Configuration |
|----------|---------------------|-------------------|
| **Microservice calls** | Basic count-based | 5 failures, 30s timeout |
| **Database connections** | Rate-based | 50% failure rate, 60s timeout |
| **External APIs** | Sliding window | 10-request window, 40% threshold |
| **Critical payments** | Distributed with fallback | Redis state, cached responses |
| **Real-time systems** | Adaptive ML-powered | Dynamic thresholds, 5s timeout |

## Implementation Templates

### Basic Circuit Breaker Configuration
```yaml
circuit_breaker:
  failure_threshold: 5
  recovery_timeout: 30s
  success_threshold: 2
  exceptions:
    - TimeoutException
    - ConnectionException
    - ServiceUnavailableException
```

### Advanced Production Configuration
```yaml
circuit_breaker:
  sliding_window:
    size: 20
    minimum_throughput: 10
  failure_criteria:
    error_rate: 50%
    slow_call_rate: 80%
    slow_call_duration: 5s
  fallback:
    strategy: cached_response
    max_age: 300s
  monitoring:
    metrics_enabled: true
    alerts_enabled: true
```

---

## References

Â¹ [Nygard, M. (2007). Release It!: Design and Deploy Production-Ready Software](https://pragprog.com/titles/mnee2/release-it-second-edition/)

Â² [Netflix Technology Blog: Introducing Hystrix for Resilience Engineering](https://netflixtechblog.com/introducing-hystrix-for-resilience-engineering-13531c1ab362)

Â³ [Netflix: Hystrix - How it Works](https://github.com/Netflix/Hystrix/wiki/How-it-Works)

â´ [Netflix: Lessons from the AWS Outage](https://netflixtechblog.com/lessons-netflix-learned-from-the-aws-outage-deefe5fd0c04)

âµ [Twitter Engineering: Finagle - Fault Tolerant Networking](https://blog.twitter.com/engineering/en_us/a/2011/finagle-a-protocol-agnostic-rpc-system)

â¶ [Amazon Builders' Library: Implementing Health Checks](https://aws.amazon.com/builders-library/implementing-health-checks/)

â· [Uber Engineering: Architecting for Reliable Scalability](https://eng.uber.com/architecting-for-reliability/)

â¸ [Stripe Engineering: Scaling your API with Rate Limiters](https://stripe.com/blog/rate-limiters)

â¹ [Netflix: Hystrix Configuration](https://github.com/Netflix/Hystrix/wiki/Configuration)

Â¹â° [LinkedIn Engineering: Eliminating Outages with Circuit Breakers](https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-traffic)

Â¹Â¹ [Airbnb Engineering: Building Services at Airbnb](https://medium.com/airbnb-engineering/building-services-at-airbnb-part-1-c4c1d8fa811b)

Â¹Â² [Spotify Engineering: Circuit Breakers for Dynamic Endpoints](https://engineering.atspotify.com/2013/06/04/incident-management-at-spotify/)

Â¹Â³ [Netflix: Hystrix Default Configuration Values](https://github.com/Netflix/Hystrix/wiki/Configuration#command-properties)

Â¹â´ [Google Cloud: Circuit Breaking in Cloud Endpoints](https://cloud.google.com/endpoints/docs/openapi/quotas-configure)

Â¹âµ [AWS: Application Load Balancer Health Checks](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html)

Â¹â¶ [Twitter: Dynamic Circuit Breaking in Finagle](https://twitter.github.io/finagle/guide/Clients.html#circuit-breaking)

Â¹â· [AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small](https://www.youtube.com/watch?v=O8xLxNje30M)

Â¹â¸ [Netflix: Chaos Engineering Upgraded](https://netflixtechblog.com/chaos-engineering-upgraded-878d341f15fa)

Â¹â¹ [Twitter: Timelines at Scale](https://www.infoq.com/presentations/Twitter-Timeline-Scalability/)

Â²â° [Uber: Designing Edge Gateway](https://eng.uber.com/gatewayuberapi/)

Â²Â¹ [Spotify: Reliable Music Streaming](https://engineering.atspotify.com/2015/08/06/spotify-apps-behind-the-scenes/)

Â²Â² [LinkedIn: Monitoring Production Systems](https://engineering.linkedin.com/monitoring/monitor-production-systems)

Â²Â³ [Facebook: Preventing Cascading Failures](https://engineering.fb.com/2011/08/25/web/preventing-cascading-failures/)

Â²â´ [Google SRE Book: Chapter 22 - Addressing Cascading Failures](https://sre.google/sre-book/addressing-cascading-failures/)

Â²âµ [Netflix: Adaptive Concurrency Limits](https://netflixtechblog.medium.com/performance-under-load-3e6fa9a60581)

Â²â¶ [AWS: Using Amazon DynamoDB with Circuit Breakers](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html)

Â²â· [Lyft: Envoy Proxy at Scale](https://eng.lyft.com/envoy-7-months-later-41986c2fd443)

Â²â¸ [Google Cloud Run: Automatic Circuit Breaking](https://cloud.google.com/run/docs/configuring/circuit-breakers)

Â²â¹ [AWS App Mesh: Circuit Breaker Configuration](https://docs.aws.amazon.com/app-mesh/latest/userguide/circuit-breakers.html)

Â³â° [Netflix: Chaos Monkey](https://github.com/Netflix/chaosmonkey)

Â³Â¹ [AWS: Prime Day 2018 - Powered by AWS](https://aws.amazon.com/blogs/aws/prime-day-2018-powered-by-aws/)

Â³Â² [GitHub: October 21 Post-Incident Analysis](https://github.blog/2018-10-30-oct21-post-incident-analysis/)

Â³Â³ [Uber: Engineering Reliable Transportation](https://eng.uber.com/engineering-reliability-uber/)

Â³â´ [Hystrix Users Survey Results](https://github.com/Netflix/Hystrix/wiki/Hystrix-Users)

Â³âµ [Honeycomb: Testing in Production](https://www.honeycomb.io/blog/testing-in-production-the-safe-way)

---

*"The best circuit breaker is invisible when working and obvious when protecting."*

---

**Previous**: [â† Change Data Capture (CDC)](cdc.md) | **Next**: [Consensus Pattern â†’](consensus.md)

**Related**: [Retry Backoff](retry-backoff.md) â€¢ [Bulkhead](bulkhead.md) â€¢ [Timeout](timeout.md)