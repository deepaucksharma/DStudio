Page 22: PILLAR I ‚Äì Distribution of Work
Learning Objective: Master the art of spreading computation without spreading complexity.
First Principle of Work Distribution:
Work should flow to where it can be processed most efficiently,
considering:
- Physical location (latency)
- Available capacity
- Required resources
- Failure domains
The Statelessness Imperative:
Stateless Service Properties:
‚úì Any request to any instance
‚úì Instances interchangeable
‚úì Horizontal scaling trivial
‚úì Failure recovery simple
‚úì Load balancing easy

Stateful Service Properties:
‚úó Requests tied to instances
‚úó Complex scaling (resharding)
‚úó Failure means data loss
‚úó Load balancing tricky
‚úó Coordination required
Work Distribution Patterns Hierarchy:
Level 1: Random Distribution
- Round-robin
- Random selection
- DNS load balancing
Efficiency: 60-70%

Level 2: Smart Distribution
- Least connections
- Weighted round-robin
- Response time based
Efficiency: 70-85%

Level 3: Affinity-Based
- Session affinity
- Consistent hashing
- Geographic routing
Efficiency: 80-90%

Level 4: Adaptive Distribution
- Predictive routing
- Cost-aware placement
- SLO-based routing
Efficiency: 90-95%
The Autoscaling Mathematics:
Optimal Instance Count = ceil(
    (Arrival Rate √ó Processing Time) / 
    (Target Utilization)
)

Example:
- Arrival: 1000 requests/second
- Processing: 100ms/request
- Target utilization: 70%

Instances = ceil((1000 √ó 0.1) / 0.7) = ceil(142.8) = 143
Autoscaling Response Curves:
Instances
    ‚Üë
150 |            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (Overprovisioned)
    |          ‚ï±
100 |        ‚ï±‚îÄ‚îÄ‚îÄ (Ideal)
    |      ‚ï±
 50 |    ‚ï±‚ï± ‚Üê (Reactive scaling)
    |  ‚ï±‚ï±
  0 |‚ï±________________________
    0   500   1000   1500  Load (req/s)
Work Distribution Anti-Patterns:

Hot Shard Problem:

Hash(UserID) % N can create:
- Celebrity user ‚Üí overloaded shard
- Power law distribution ‚Üí uneven load
Fix: Virtual shards, consistent hashing

Thundering Herd:

All instances start simultaneously:
- Cache empty ‚Üí database overload
- Health checks ‚Üí false failures
Fix: Staggered starts, cache priming

Work Duplication:

Multiple workers process same item:
- No coordination ‚Üí wasted work
- Optimistic locking ‚Üí conflict storms
Fix: Work stealing, leases
üîß Try This: Build a Work Stealer
pythonimport time
import random
from multiprocessing import Process, Queue, Value

class WorkStealer:
    def __init__(self, num_workers=4):
        self.work_queue = Queue()
        self.steal_queue = Queue()
        self.completed = Value('i', 0)
        self.workers = []
        
    def worker(self, worker_id, local_queue):
        while True:
            # Try local queue first
            if not local_queue.empty():
                work = local_queue.get()
            # Try stealing if local empty
            elif not self.steal_queue.empty():
                work = self.steal_queue.get()
            else:
                time.sleep(0.01)
                continue
                
            if work is None:
                break
                
            # Simulate work
            time.sleep(random.uniform(0.01, 0.1))
            with self.completed.get_lock():
                self.completed.value += 1
    
    def distribute_work(self, items):
        # Create local queues
        local_queues = [Queue() for _ in range(len(self.workers))]
        
        # Initial distribution
        for i, item in enumerate(items):
            if i < len(items) // 2:
                # First half goes to workers
                local_queues[i % len(self.workers)].put(item)
            else:
                # Second half goes to steal queue
                self.steal_queue.put(item)
        
        # Start workers
        for i, q in enumerate(local_queues):
            p = Process(target=self.worker, args=(i, q))
            p.start()
            self.workers.append(p)
        
        # Wait for completion
        start = time.time()
        while self.completed.value < len(items):
            time.sleep(0.1)
        
        # Cleanup
        for q in local_queues:
            q.put(None)
        for w in self.workers:
            w.join()
            
        return time.time() - start

# Compare with and without work stealing
stealer = WorkStealer()
elapsed = stealer.distribute_work(list(range(100)))
print(f"With work stealing: {elapsed:.2f}s")