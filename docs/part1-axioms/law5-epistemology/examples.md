# Law 5: Examples - The Truth Disasters (and Rare Victories) ğŸ­

<div class="truth-box" style="background: #2d3748; border: 2px solid #4ecdc4;">
<h2>The Museum of Distributed Lies</h2>
<p>Every system here believed it knew the truth. Some lost millions discovering they didn't. Others built empires on embracing uncertainty. Learn from bothâ€”your data's integrity depends on it.</p>
</div>

## Quick Reference: Truth Catastrophes & Triumphs

```
THE GALLERY OF EPISTEMOLOGICAL DISASTERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’€ DISASTERS (Truth Failed)          ğŸ† TRIUMPHS (Uncertainty Embraced)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Reddit: Split-brain writes          Google Spanner: True time
â†’ Data corruption                   â†’ Global consistency

Knight Capital: Stale state         Kafka: Ordered logs  
â†’ $440M in 45 minutes              â†’ Truth through sequence

GitHub: Phantom repos               Bitcoin: Probabilistic finality
â†’ Users lost work                   â†’ $1T secured

Cloudflare: Byzantine BGP           DynamoDB: Vector clocks
â†’ 50% packet loss globally         â†’ Automatic reconciliation
```

---

## Case 1: Reddit's Split-Brain Nightmare (2023) ğŸ§ ğŸ’¥

<div class="failure-vignette">
<h3>The Setup: "Our Kubernetes Cluster Is Bulletproof"</h3>

```
THE CONFIDENCE BEFORE THE STORM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What Reddit believed:
- Primary/Secondary replication = Safe
- Network partitions = Rare  
- Kubernetes = Handles everything
- Split-brain = Theoretical problem

What Reddit forgot:
- Networks partition ALL THE TIME
- Both sides think they're right
- Writes don't wait for consensus
- Truth requires coordination
```

### The 6-Hour Double-Truth Disaster

```
MARCH 2023: THE TIMELINE OF LIES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

09:00 - Network blip between data centers
        DC1: "I'm primary, DC2 is dead"
        DC2: "DC1 is dead, I'm primary now"
        
09:01 - Both accepting writes
        DC1: User posts â†’ Subreddit A
        DC2: User posts â†’ Subreddit A
        Different posts, same IDs!
        
09:30 - Ops notices something wrong
        Metrics disagree between DCs
        "Must be monitoring bug"
        
10:00 - Users report missing posts
        "I posted but it disappeared!"
        "My karma is different on mobile!"
        
11:00 - THE HORRIBLE REALIZATION
        Two versions of Reddit exist
        30 minutes of divergent data
        No automatic reconciliation
        
15:00 - Manual data surgery begins
        Pick winning version per conflict
        Some users lose 6 hours of posts
        Trust permanently damaged

COST: Unknown data loss + User trust
```

### What Actually Happened

```
THE SPLIT-BRAIN MECHANICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BEFORE (Happy State):
DC1 (Primary) â†â”€â”€Syncâ”€â”€â†’ DC2 (Secondary)
     â†“                        â†“
   Writes                  Read-only

DURING (Partition):
DC1 (Primary) â†â”€â”€âœ‚ï¸â”€â”€â†’ DC2 ("Primary")
     â†“                      â†“
   Writes                 Writes
   
Post ID #12345:           Post ID #12345:
"Check out my cat"        "Bitcoin to $1M"

AFTER (Reconciliation Hell):
Which Post ID #12345 is real?
- Keep DC1? Lose DC2 users' data
- Keep DC2? Lose DC1 users' data  
- Keep both? Broken foreign keys
- Manual merge? 6-hour downtime
```
</div>

### The Fix They Should Have Had

```python
class SplitBrainProtection:
    """
    What Reddit needed: Majority-based truth
    """
    def __init__(self):
        self.nodes = ['DC1', 'DC2', 'Witness1', 'Witness2', 'Witness3']
        self.quorum_size = 3  # Majority of 5
        
    def can_accept_writes(self, node_id):
        """Only accept writes with majority agreement"""
        reachable = self.count_reachable_nodes(node_id)
        
        if reachable >= self.quorum_size:
            # I can reach majority = I can be primary
            return True
        else:
            # I'm in minority partition = READ ONLY
            self.enter_degraded_mode()
            return False
            
    def handle_network_partition(self):
        """
        Partition scenarios with 5 nodes:
        
        [DC1, DC2, W1] | [W2, W3]
        3 nodes = Majority = Can write
        2 nodes = Minority = Read only
        
        Result: Only one side accepts writes!
        """
```

---

## Case 2: Knight Capital's $440M Race Condition (2012) ğŸ’¸

<div class="axiom-box">
<h3>When Distributed Truth Lag Costs $10M Per Minute</h3>

```
THE DEADLY DEPLOYMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

07:00 - Deploy new trading code to 8 servers
        Server 1-7: New code âœ“
        Server 8: DEPLOYMENT FAILED âŒ
        
        The "truth" about active code:
        - 7 servers: "New version"
        - 1 server: "Old version"
        - No consensus mechanism
        
09:30 - Market opens
        
THE CASCADE OF LIES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Server 8 (old code):
while True:
    if test_flag:  # Flag meant "test" in old code
        BUY_EVERYTHING()  # But means "prod" in new code!
        
Other servers:
    "Why is Server 8 buying so much?"
    "Must know something we don't"
    "Better not interfere"
    
09:31 - Positions exploding
        100,000 shares... 1M shares... 10M shares...
        
09:35 - Traders notice
        "SHUT IT DOWN!"
        But which server is wrong?
        If we kill the wrong one...
        
10:15 - All systems stopped
        45 minutes of carnage
        4 million executions
        $440 MILLION LOSS
        
Truth lag: 1 server
Cost: Company bankruptcy
```
</div>

### The Code That Killed a Company

```python
# SERVER 1-7 (New Code)
def handle_order(order, flags):
    if flags.get('RETAIL_LIQUIDITY'):  # New flag
        route_to_retail_market(order)
    else:
        route_to_main_market(order)

# SERVER 8 (Old Code - Same Flag, Different Meaning!)
def handle_order(order, flags):
    if flags.get('RETAIL_LIQUIDITY'):  # Old test flag
        # Test mode - generate child orders
        for i in range(1000):
            child = create_child_order(order)
            child.quantity *= 10  # Test multiplier
            send_immediate(child)  # NO SAFEGUARDS
```

### What They Should Have Done

```
DISTRIBUTED TRUTH REQUIREMENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Version Consensus:
   "All nodes must agree on code version before trading"
   
2. Truth Beacon:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Config Service  â”‚
   â”‚ Version: 2.0.1  â”‚
   â”‚ Hash: abc123... â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â–¼        â–¼        â–¼
Server1  Server2  Server8
"2.0.1"  "2.0.1"  "1.9.8" â† REJECT ORDERS

3. Kill Switch Consensus:
   "If nodes disagree on truth, STOP EVERYTHING"
```

---

## Case 3: The Bitcoin Double-Truth Crisis (2013) â‚¿

<div class="truth-box">
<h3>When Even Blockchain Can't Agree on Truth</h3>

```
THE "IMMUTABLE" LEDGER THAT SPLIT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

March 11, 2013 - Two Bitcoins Exist

CHAIN A (v0.8 nodes)         CHAIN B (v0.7 nodes)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•         â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Block 225,430 âœ“              Block 225,430 âœ“
Block 225,431 (big) âœ“        Block 225,431' (small) âœ“
Block 225,432 âœ“              Block 225,432' âœ“
Block 225,433 âœ“              Block 225,433' âœ“
...                          ...

$60 BILLION asking: "Which chain is real?"

The Truth Paradox:
- Both chains valid by their rules
- Both have proof-of-work
- Both have miners
- Both process transactions

Some transactions exist in A but not B
Some transactions exist in B but not A
Double-spends now possible!
```

### The Emergency Surgery

```
THE CONSENSUS ASSASSINATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Core developers in IRC:
"We need to pick a truth and kill the other"

Option 1: Let longest chain win
- Chain A (v0.8) winning
- But kills v0.7 transactions
- Many exchanges on v0.7

Option 2: Intentional 51% attack
- Force miners to v0.7 chain
- Sacrifice v0.8 progress
- Centralized decision

THE DECISION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Gavin: "Everyone switch to 0.7"
Pools: "Pointing hashpower at 0.7"

Result: Chain A orphaned
Cost: 6 hours of v0.8 transactions erased
Lesson: Even blockchain truth is negotiable
```
</div>

### The Code Difference That Forked Reality

```python
# Bitcoin 0.7 (BerkeleyDB)
class BlockValidator_v07:
    def validate_block(self, block):
        # BDB limit: ~10,000 database locks
        if count_database_locks(block) > 10000:
            return False  # Block too complex
        return validate_transactions(block)

# Bitcoin 0.8 (LevelDB)  
class BlockValidator_v08:
    def validate_block(self, block):
        # LevelDB: No lock limit
        # Can process bigger blocks
        return validate_transactions(block)

# Result: Block valid for 0.8, invalid for 0.7
# TWO INCOMPATIBLE TRUTHS
```

---

## Case 4: Cloudflare's Byzantine BGP Meltdown (2020) ğŸŒ

<div class="failure-vignette">
<h3>When Routers Lie to Each Other</h3>

```
THE ROUTING TABLE CIVIL WAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Normal BGP Conversation:
Router1: "I know the path to 8.8.8.8"
Router2: "Cool, I'll route through you"
Router3: "I'll use Router2's path"

During The Incident:
Router1: "Best path is through Atlanta"
Router2: "No, best path is through Chicago"  
Router3: "Atlanta is down, use Miami"
Router1: "Miami is congested, use Atlanta"
Router2: "I SAID CHICAGO!"
Router3: "ATLANTA IS DOWN!"

Result: Packets spinning in circles
        50% global packet loss
        Internet partially broken
```

### The Byzantine Generals in Silicon

```
WHICH ROUTER IS LYING?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     "Path A is optimal"     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Router 1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚Router 2 â”‚
â”‚ TRUTH?  â”‚                             â”‚ LYING?  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚                                        â”‚
     â”‚"B is optimal"                         â”‚"A is down"
     â”‚                                        â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     "Everything is fine"    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Router 3 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚Router 4 â”‚
â”‚ BROKEN? â”‚                             â”‚CONFUSED?â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

THE PROBLEM:
- No way to verify router honesty
- BGP trusts by default
- One bad actor poisons entire network
- Truth becomes "whoever shouts loudest"
```

### Real-Time Packet Chaos

```python
class BGPRouter:
    def __init__(self):
        self.routing_table = {}
        self.peers = []
        self.trust_scores = {}  # What they SHOULD have had
        
    def receive_route_update(self, peer, update):
        """The broken trust model"""
        # Old way (what caused outage)
        if self.is_valid_format(update):
            self.routing_table[update.prefix] = update
            self.propagate_to_peers(update)  # Spread the lie!
            
        # What they needed
        if self.verify_route_authenticity(peer, update):
            trust = self.trust_scores.get(peer, 0.5)
            
            # Byzantine fault tolerance
            confirmations = self.get_peer_confirmations(update)
            if confirmations >= 3 or trust > 0.8:
                self.routing_table[update.prefix] = update
            else:
                self.quarantine_suspicious_route(update)
```
</div>

---

## Case 5: DynamoDB's Vector Clock Victory âœ“

<div class="decision-box">
<h3>How Amazon Solved Distributed Truth at Scale</h3>

```
THE SHOPPING CART PROBLEM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User adds items from multiple devices:

Phone (Virginia DC):         Laptop (Oregon DC):
"Add headphones"            "Add laptop"
Vector: {VA:1, OR:0}        Vector: {VA:0, OR:1}
         â†“                           â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€  Merge  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
              Vector: {VA:1, OR:1}
              Cart: [headphones, laptop]

CONFLICT DETECTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phone: "Remove laptop, add mouse"    Laptop: "Add keyboard"
Vector: {VA:2, OR:1}                 Vector: {VA:1, OR:2}

Concurrent updates detected!
VA:2 does not descend from VA:1
OR:2 does not descend from OR:1

RESOLUTION: Keep both paths, let app decide
```

### The Code That Powers Amazon

```python
class DynamoDBItem:
    """Simplified version of Amazon's approach"""
    
    def __init__(self, key):
        self.key = key
        self.vector_clock = {}
        self.values = []  # Can have multiple concurrent values
        
    def put(self, value, client_context):
        """Write with causality tracking"""
        # Increment vector clock
        node = self.get_coordinator_node()
        
        if client_context:
            # Client provided context - update causally
            self.vector_clock = client_context.vector_clock.copy()
            
        self.vector_clock[node] = self.vector_clock.get(node, 0) + 1
        
        # Store value with its vector clock
        self.values = [{
            'value': value,
            'clock': self.vector_clock.copy()
        }]
        
        return self.vector_clock
        
    def get(self):
        """Read potentially conflicting values"""
        if len(self.values) == 1:
            # No conflicts
            return self.values[0]['value'], self.values[0]['clock']
        else:
            # Multiple concurrent values - return all
            return [v['value'] for v in self.values], self.values
            
    def reconcile(self, values):
        """Application-specific conflict resolution"""
        # Shopping cart: Union of all items
        # Last-write-wins: Latest timestamp
        # Custom: Let application decide
        pass
```

### Why This Works

```
VECTOR CLOCKS VISUALIZED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Sequential Updates (No Conflict):
A:{A:1} â†’ A:{A:2} â†’ A:{A:3}
  â†“         â†“         â†“
"v1"      "v2"      "v3"     Clear winner: v3

Concurrent Updates (Conflict):
         A:{A:1}
        â†™      â†˜
A:{A:2,B:0}   B:{A:1,B:1}
    â†“             â†“
  "milk"       "eggs"

Neither vector > other = Concurrent!
Keep both: ["milk", "eggs"]
```
</div>

---

## Case 6: Google Spanner's True Time Revolution ğŸ•

<div class="truth-box">
<h3>The $10B System That Actually Achieved Global Truth</h3>

```
THE IMPOSSIBLE MADE POSSIBLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What everyone said: "You can't have global consistency"
What Google did: "Hold my atomic clock"

THE TRUE TIME API:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

now = TT.now()
Returns: [earliest, latest]

Example at 12:00:00.000:
earliest: 11:59:59.995
latest:   12:00:00.005
Uncertainty: Â±5ms

THE GENIUS MOVE:
If you know time uncertainty,
you can achieve global consistency!
```

### How Spanner Achieves the Impossible

```python
class SpannerTransaction:
    """Simplified version of Google's approach"""
    
    def __init__(self):
        self.true_time = TrueTimeAPI()
        
    def commit(self, writes):
        """Achieve global consistency with uncertain clocks"""
        # Get timestamp for transaction
        timestamp = self.true_time.now().latest
        
        # The key insight: WAIT OUT THE UNCERTAINTY
        commit_wait = self.true_time.now().uncertainty()
        time.sleep(commit_wait)  # ~5-10ms
        
        # After wait, we KNOW this timestamp is in the past
        # everywhere in the world
        
        # Now safe to commit
        for write in writes:
            write.timestamp = timestamp
            write.commit()
            
        return timestamp
        
    def read(self, key, read_timestamp=None):
        """Read with global consistency"""
        if read_timestamp:
            # Read at specific timestamp
            # Will see all commits before this time globally
            return self.storage.read_at_timestamp(key, read_timestamp)
        else:
            # Read at "now"
            safe_time = self.true_time.now().earliest
            return self.storage.read_at_timestamp(key, safe_time)
```

### The Hardware That Makes It Work

```
ATOMIC CLOCK SYNCHRONIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  GPS Time  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Atomic Clockâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Atomic Clockâ”‚
â”‚  Master 1   â”‚            â”‚  Master 2   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                          â”‚
       â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Datacenter Time Masters         â”‚
â”‚    Synchronized to Â±100 microseconds    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼            â–¼            â–¼
   Server A     Server B     Server C
   Â±1-5ms       Â±1-5ms       Â±1-5ms
   
Result: Know time uncertainty globally
        Can achieve true global consistency
        Worth the atomic clock cost!
```
</div>

---

## The Meta-Patterns of Distributed Truth

<div class="axiom-box" style="background: #1a1a1a; border: 2px solid #ff5555;">
<h3>What We Learned From These Disasters</h3>

```
PATTERN 1: TRUTH REQUIRES MAJORITY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Reddit needed: 3+ witness nodes
Knight needed: Version consensus
Bitcoin needed: Clear fork rules

â†’ Truth = Majority agreement, not hope

PATTERN 2: BYZANTINE NODES ARE REAL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Cloudflare: Routers lied
Knight: Servers disagreed
GitHub: Replicas diverged

â†’ Nodes lie accidentally all the time

PATTERN 3: TIME IS TRUTH'S FOUNDATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Spanner: Atomic clocks = consistency
DynamoDB: Vector clocks = causality
Bitcoin: Block time = ordering

â†’ No shared time = No shared truth

PATTERN 4: CONFLICTS REQUIRE STRATEGY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Amazon: Keep all versions
Google: Wait out uncertainty
Bitcoin: Longest chain wins

â†’ Plan for conflicts, don't prevent them

PATTERN 5: PARTIAL TRUTH IS NORMAL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Every system operates with incomplete knowledge
The winners design for it
The losers assume it away

â†’ Embrace uncertainty or it will surprise you
```
</div>

## Your Truth Homework

<div class="decision-box">
<h3>Find These Truth Failures in Your System</h3>

```
THE TRUTH AUDIT CHECKLIST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¡ Do you have split-brain protection?
  Test: Partition network, see who accepts writes

â–¡ Can you detect Byzantine nodes?
  Test: Make one node return wrong data

â–¡ How do you order concurrent events?
  Test: Two updates at the same millisecond

â–¡ What's your conflict resolution?
  Test: Create deliberate conflicts

â–¡ How fast does truth propagate?
  Test: Measure end-to-end consistency time

If you haven't tested these scenarios,
you're not ready for production.
```
</div>

**Next**: [Exercises](exercises.md) - Practice designing for distributed truth