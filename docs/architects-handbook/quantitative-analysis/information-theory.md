---
title: Information Theory
description: Mathematical foundations of information, entropy, and communication theory applied to distributed systems
type: quantitative
difficulty: advanced
reading_time: 45 min
prerequisites: [probability-theory, linear-algebra, calculus]
pattern_type: foundational
status: complete
last_updated: 2025-01-23
---

# Information Theory

## Mathematical Foundations

Information theory, developed by Claude Shannon in 1948, provides the mathematical foundation for understanding information transmission, compression, and storage in distributed systems.

### Shannon Entropy

The fundamental concept of information theory is **entropy**, which measures the uncertainty or information content in a random variable.

For a discrete random variable $X$ with probability mass function $p(x)$:

$$H(X) = -\sum_{x} p(x) \log_2 p(x) \text{ bits}$$

**Key Properties:**
1. $H(X) \geq 0$ (non-negativity)
2. $H(X) = 0$ if and only if $X$ is deterministic
3. $H(X)$ is maximized when $X$ is uniform: $H(X) \leq \log_2 |X|$

#### Proof of Concavity

**Theorem**: Entropy is a concave function of the probability distribution.

**Proof**: For any two distributions $p$ and $q$ and $\lambda \in [0,1]$:
$$H(\lambda p + (1-\lambda)q) \geq \lambda H(p) + (1-\lambda)H(q)$$

Using Jensen's inequality with the concave function $f(x) = -x \log x$:
$$-\sum_i [\lambda p_i + (1-\lambda)q_i] \log[\lambda p_i + (1-\lambda)q_i] \geq \lambda(-\sum_i p_i \log p_i) + (1-\lambda)(-\sum_i q_i \log q_i)$$

### Conditional Entropy

The conditional entropy of $Y$ given $X$ is:
$$H(Y|X) = -\sum_{x,y} p(x,y) \log p(y|x)$$

**Chain Rule**: $H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$

### Mutual Information

Mutual information quantifies the amount of information shared between two random variables:

$$I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)$$

Alternative formulation using KL divergence:
$$I(X;Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$

**Properties:**
- $I(X;Y) \geq 0$ (non-negativity)
- $I(X;Y) = 0$ if and only if $X$ and $Y$ are independent
- $I(X;Y) = I(Y;X)$ (symmetry)

### Channel Capacity

For a discrete memoryless channel with input $X$, output $Y$, and transition probabilities $p(y|x)$:

**Channel Capacity**: $C = \max_{p(x)} I(X;Y)$

#### Binary Symmetric Channel (BSC)

For a BSC with crossover probability $p$:
$$C = 1 - H(p) = 1 + p \log_2 p + (1-p) \log_2 (1-p)$$

#### Additive White Gaussian Noise (AWGN) Channel

For continuous channels with power constraint $P$ and noise power $N$:
$$C = \frac{1}{2} \log_2\left(1 + \frac{P}{N}\right) \text{ bits per channel use}$$

## Compression Theory

### Source Coding Theorem

**Shannon's First Theorem**: For a source with entropy rate $H$, the expected length of any uniquely decodable code satisfies:
$$L \geq H$$

The entropy $H$ represents the fundamental limit for lossless compression.

### Huffman Coding

Huffman coding achieves optimal prefix-free codes for known symbol probabilities.

**Algorithm**:
1. Create leaf nodes for each symbol with their frequencies
2. Build binary tree bottom-up by repeatedly merging two nodes with smallest frequencies
3. Assign codes: left = 0, right = 1

**Optimality**: Huffman codes satisfy $H(X) \leq L < H(X) + 1$

#### Implementation

```python
import heapq
from collections import defaultdict, Counter

class HuffmanNode:
    def __init__(self, char, freq):
        self.char = char
        self.freq = freq
        self.left = None
        self.right = None
    
    def __lt__(self, other):
        return self.freq < other.freq

def huffman_encoding(text):
    # Calculate frequencies
    freq = Counter(text)
    
    # Create priority queue
    heap = [HuffmanNode(char, freq) for char, freq in freq.items()]
    heapq.heapify(heap)
    
    # Build Huffman tree
    while len(heap) > 1:
        node1 = heapq.heappop(heap)
        node2 = heapq.heappop(heap)
        
        merged = HuffmanNode(None, node1.freq + node2.freq)
        merged.left = node1
        merged.right = node2
        
        heapq.heappush(heap, merged)
    
    root = heap[0]
    
    # Generate codes
    codes = {}
    def generate_codes(node, code=""):
        if node.char:
            codes[node.char] = code or "0"
        else:
            generate_codes(node.left, code + "0")
            generate_codes(node.right, code + "1")
    
    generate_codes(root)
    return codes

# Example usage
text = "this is an example for huffman encoding"
codes = huffman_encoding(text)
print(f"Huffman codes: {codes}")

# Calculate compression ratio
original_bits = len(text) * 8
encoded_bits = sum(len(codes[char]) for char in text)
compression_ratio = original_bits / encoded_bits
print(f"Compression ratio: {compression_ratio:.2f}")
```

### Arithmetic Coding

Arithmetic coding can approach the entropy limit more closely than Huffman coding.

**Principle**: Represent the entire message as a single fraction in [0,1).

**Entropy Efficiency**: $L \leq H(X) + 2$

### Lempel-Ziv Compression

Universal compression algorithms that don't require prior knowledge of source statistics.

**LZ77**: Uses sliding window to find matching substrings
**LZ78**: Builds dictionary of encountered patterns

## Error Correction Codes

### Linear Block Codes

A $(n,k,d)$ linear code:
- $n$: codeword length
- $k$: information bits  
- $d$: minimum distance

**Generator Matrix**: $G$ is $k \times n$ matrix where codeword $\mathbf{c} = \mathbf{m}G$

**Parity Check Matrix**: $H$ is $(n-k) \times n$ matrix where $H\mathbf{c}^T = \mathbf{0}$

**Syndrome**: $\mathbf{s} = H\mathbf{r}^T$ where $\mathbf{r}$ is received vector

### Hamming Codes

Hamming codes can correct single-bit errors.

For Hamming(7,4) code:
$$G = \begin{pmatrix}
1 & 0 & 0 & 0 & 1 & 1 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 1 & 1 \\
0 & 0 & 0 & 1 & 1 & 1 & 1
\end{pmatrix}$$

$$H = \begin{pmatrix}
1 & 1 & 0 & 1 & 1 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 1 & 0 \\
0 & 1 & 1 & 1 & 0 & 0 & 1
\end{pmatrix}$$

### Reed-Solomon Codes

Reed-Solomon codes can correct multiple symbol errors and are widely used in storage systems.

**Construction**: Based on polynomial evaluation over finite fields $GF(2^m)$

For RS$(n,k)$ code over $GF(2^m)$:
- Can correct up to $t = \lfloor(n-k)/2\rfloor$ symbol errors
- Often used with $n = 2^m - 1$

#### Implementation

```python
import numpy as np

class ReedSolomon:
    def __init__(self, n, k):
        self.n = n  # codeword length
        self.k = k  # message length
        self.t = (n - k) / 2  # error correction capability
        
    def gf_mult(self, a, b, prim=0x11d, field_bits=8):
        """Galois field multiplication"""
        result = 0
        while b:
            if b & 1:
                result ^= a
            a <<= 1
            if a & (1 << field_bits):
                a ^= prim
            b >>= 1
        return result
    
    def gf_poly_mult(self, p1, p2):
        """Galois field polynomial multiplication"""
        result = [0] * (len(p1) + len(p2) - 1)
        for i in range(len(p1)):
            for j in range(len(p2)):
                result[i + j] ^= self.gf_mult(p1[i], p2[j])
        return result
    
    def generate_polynomial(self):
        """Generate the generator polynomial"""
        g = [1]  # Start with polynomial 1
        for i in range(self.n - self.k):
            # Multiply by (x - Î±^i)
            g = self.gf_poly_mult(g, [1, pow(2, i)])
        return g
    
    def encode(self, message):
        """Encode message using Reed-Solomon"""
        # Systematic encoding: append parity symbols
        g = self.generate_polynomial()
        # Implementation details omitted for brevity
        return message + [0] * (self.n - self.k)  # Simplified

# Example usage
rs = ReedSolomon(15, 11)  # RS(15,11) can correct 2 symbol errors
print(f"Reed-Solomon RS(15,11) can correct up to {rs.t} symbol errors")
```

### Turbo Codes and LDPC

Modern error correction codes that approach the Shannon limit.

**Turbo Codes**: Use iterative decoding with soft-decision feedback
- Consist of parallel or serial concatenated convolutional codes
- Achieve performance within 0.7 dB of Shannon limit

**LDPC (Low-Density Parity-Check) Codes**: Use sparse parity check matrices
- Bipartite graph representation enables efficient belief propagation decoding
- Can approach Shannon limit with proper design

## Applications in Distributed Systems

### Data Deduplication

Information theory guides optimal deduplication strategies:

**Content-Defined Chunking**: Uses Rabin fingerprinting to identify chunk boundaries
$$h(x) = x^0 a_0 + x^1 a_1 + \cdots + x^{w-1} a_{w-1} \bmod p$$

**Similarity Detection**: Locality-sensitive hashing for finding similar content
$$P[\text{hash}(A) = \text{hash}(B)] \propto \text{similarity}(A,B)$$

### Network Coding

Linear network coding allows intermediate nodes to combine packets:

**Benefit**: Can achieve capacity bounds for multicast scenarios
**Implementation**: Random linear combinations over finite fields

```python
import numpy as np

def random_linear_network_coding(packets, num_encoded=None):
    """Generate random linear combinations of packets"""
    if num_encoded is None:
        num_encoded = len(packets)
    
    # Generate random coefficients over GF(2^8)
    coeffs = np.random.randint(0, 256, (num_encoded, len(packets)))
    
    encoded_packets = []
    for i in range(num_encoded):
        encoded = 0
        for j, packet in enumerate(packets):
            encoded ^= gf_mult(coeffs[i][j], packet)
        encoded_packets.append((encoded, coeffs[i]))
    
    return encoded_packets

def gf_mult(a, b):
    """Galois field multiplication in GF(2^8)"""
    result = 0
    while b:
        if b & 1:
            result ^= a
        a <<= 1
        if a & 0x100:
            a ^= 0x11d  # Primitive polynomial x^8 + x^4 + x^3 + x^2 + 1
        b >>= 1
    return result & 0xff
```

### Erasure Coding in Storage

Distributed storage systems use erasure codes for reliability:

**Reed-Solomon**: $(n,k)$ code stores $k$ data blocks across $n$ nodes
- Can tolerate $n-k$ node failures
- Storage overhead: $n/k$

**Local Reconstruction Codes (LRC)**: Optimize for local repair
$$d = \ell + 1 + \lceil r/(\ell+1) \rceil$$
where $d$ is minimum distance, $\ell$ is locality, $r$ is redundancy

### Information-Theoretic Security

**Perfect Secrecy**: $I(M;C) = 0$ where $M$ is message, $C$ is ciphertext
- One-time pad achieves perfect secrecy
- Key length must equal message length

**Computational Security**: Bounded by computational resources
- Based on hard mathematical problems (factoring, discrete log)

### Consensus and Information

**Impossibility Results**: Information-theoretic bounds on consensus
- FLP impossibility: No deterministic asynchronous consensus with one faulty process
- CAP theorem: Cannot simultaneously achieve consistency, availability, partition tolerance

**Communication Complexity**: Lower bounds on message complexity
- $\Omega(n)$ messages required for Byzantine agreement
- $\Omega(f)$ rounds required with $f$ faulty processes

## Interactive Tools and Calculators

### Entropy Calculator

```javascript
function calculateEntropy(probabilities) {
    let entropy = 0;
    for (let p of probabilities) {
        if (p > 0) {
            entropy -= p * Math.log2(p);
        }
    }
    return entropy;
}

function calculateMutualInformation(joint_prob) {
    / joint_prob is 2D array of P(X=i, Y=j)
    let marginal_x = joint_prob.map(row => row.reduce((a, b) => a + b));
    let marginal_y = joint_prob[0].map((_, j) => 
        joint_prob.reduce((sum, row) => sum + row[j], 0));
    
    let mi = 0;
    for (let i = 0; i < joint_prob.length; i++) {
        for (let j = 0; j < joint_prob[i].length; j++) {
            if (joint_prob[i][j] > 0) {
                mi += joint_prob[i][j] * Math.log2(
                    joint_prob[i][j] / (marginal_x[i] * marginal_y[j])
                );
            }
        }
    }
    return mi;
}

/ Example usage
const probs = [0.25, 0.25, 0.25, 0.25];  / Uniform distribution
console.log(`Entropy: ${calculateEntropy(probs)} bits`);

const joint = [[0.25, 0.25], [0.25, 0.25]];  / Independent variables
console.log(`Mutual Information: ${calculateMutualInformation(joint)} bits`);
```

### Channel Capacity Calculator

```javascript
function bscCapacity(p) {
    / Binary Symmetric Channel capacity
    if (p === 0 || p === 1) return 0;
    return 1 + p * Math.log2(p) + (1 - p) * Math.log2(1 - p);
}

function awgnCapacity(snr) {
    / AWGN channel capacity (SNR in linear scale)
    return 0.5 * Math.log2(1 + snr);
}

/ Example calculations
console.log(`BSC capacity (p=0.1): ${bscCapacity(0.1).toFixed(4)} bits`);
console.log(`AWGN capacity (SNR=10dB): ${awgnCapacity(10).toFixed(4)} bits per channel use`);
```

## Performance Analysis and Complexity

### Compression Algorithms Comparison

| Algorithm | Time Complexity | Space Complexity | Compression Ratio | Entropy Efficiency |
|-----------|----------------|------------------|-------------------|-------------------|
| Huffman | $O(n \log n)$ | $O(n)$ | 2-3x | $H \leq L < H+1$ |
| Arithmetic | $O(n)$ | $O(1)$ | 3-4x | $L \leq H+2$ |
| LZ77 | $O(n^2)$ | $O(w)$ | 2-10x | Universal |
| LZ78/LZW | $O(n)$ | $O(d)$ | 2-10x | Universal |

where $n$ is input size, $w$ is window size, $d$ is dictionary size.

### Error Correction Performance

| Code Type | Rate | $d_{min}$ | Decoding Complexity | Applications |
|-----------|------|-----------|-------------------|--------------|
| Hamming(7,4) | 4/7 | 3 | $O(n)$ | Memory systems |
| BCH(31,21) | 21/31 | 5 | $O(n^3)$ | Storage |
| RS(255,223) | 223/255 | 33 | $O(n^2)$ | CDs, DVDs |
| Turbo | ~1/2 | - | $O(n \log n)$ | Wireless |
| LDPC | Variable | - | $O(n)$ | WiFi, 5G |

## Implementation Examples

### Distributed Hash Table with Information-Theoretic Load Balancing

```python
import hashlib
import math
from typing import List, Tuple

class InformationTheoreticDHT:
    def __init__(self, nodes: List[str], hash_bits: int = 160):
        self.nodes = nodes
        self.hash_bits = hash_bits
        self.ring_size = 2 ** hash_bits
        self.node_positions = self._calculate_positions()
    
    def _calculate_positions(self) -> List[Tuple[int, str]]:
        """Calculate node positions to minimize entropy of load distribution"""
        positions = []
        for node in self.nodes:
            # Use multiple hash functions for better distribution
            for i in range(math.ceil(math.log2(len(self.nodes)))):
                hash_input = f"{node}:{i}"
                hash_val = int(hashlib.sha1(hash_input.encode()).hexdigest(), 16)
                position = hash_val % self.ring_size
                positions.append((position, node))
        
        return sorted(positions)
    
    def get_responsible_node(self, key: str) -> str:
        """Find node responsible for given key"""
        key_hash = int(hashlib.sha1(key.encode()).hexdigest(), 16) % self.ring_size
        
        # Find next node clockwise
        for position, node in self.node_positions:
            if position >= key_hash:
                return node
        
        # Wrap around to first node
        return self.node_positions[0][1]
    
    def calculate_load_entropy(self, keys: List[str]) -> float:
        """Calculate entropy of load distribution"""
        node_counts = {}
        for key in keys:
            node = self.get_responsible_node(key)
            node_counts[node] = node_counts.get(node, 0) + 1
        
        total_keys = len(keys)
        probabilities = [count / total_keys for count in node_counts.values()]
        
        entropy = 0
        for p in probabilities:
            if p > 0:
                entropy -= p * math.log2(p)
        
        return entropy

# Example usage
nodes = [f"node_{i}" for i in range(8)]
dht = InformationTheoreticDHT(nodes)

# Simulate key distribution
keys = [f"key_{i}" for i in range(1000)]
entropy = dht.calculate_load_entropy(keys)
max_entropy = math.log2(len(nodes))

print(f"Load distribution entropy: {entropy:.3f} bits")
print(f"Maximum possible entropy: {max_entropy:.3f} bits")
print(f"Load balance efficiency: {entropy/max_entropy:.1%}")
```

## Research Frontiers

### Quantum Information Theory

**Quantum Entropy**: Von Neumann entropy for quantum states
$$S(\rho) = -\text{Tr}(\rho \log \rho)$$

**Quantum Channel Capacity**: Holevo bound for classical information transmission
$$\chi = S\left(\sum_i p_i \rho_i\right) - \sum_i p_i S(\rho_i)$$

### Network Information Theory

**Multiple Access Channel**: Information-theoretic limits for multiple senders
**Broadcast Channel**: Capacity region for one sender, multiple receivers
**Interference Channel**: Mutual interference between communication pairs

### Distributed Source Coding

**Slepian-Wolf Theorem**: Separate encoding of correlated sources
$$R_X \geq H(X|Y), \quad R_Y \geq H(Y|X), \quad R_X + R_Y \geq H(X,Y)$$

## References and Further Reading

### Seminal Papers
1. Shannon, C.E. (1948). "A Mathematical Theory of Communication"
2. Hamming, R.W. (1950). "Error Detecting and Error Correcting Codes"
3. Reed, I.S. & Solomon, G. (1960). "Polynomial Codes Over Certain Finite Fields"
4. Berrou, C. et al. (1993). "Near Shannon Limit Error-Correcting Coding and Decoding: Turbo-Codes"

### Modern Textbooks
- Cover, T.M. & Thomas, J.A. "Elements of Information Theory" (2006)
- MacKay, D.J.C. "Information Theory, Inference and Learning Algorithms" (2003)
- Richardson, T.J. & Urbanke, R.L. "Modern Coding Theory" (2008)

### Applications in Distributed Systems
- Dimakis, A.G. et al. "Network Coding for Distributed Storage Systems" (2010)
- Papailiopoulos, D.S. & Dimakis, A.G. "Locally Repairable Codes" (2012)
- Shah, N.B. et al. "Information-Theoretically Secure Erasure Codes for Distributed Storage" (2012)

## Related Topics

- [Compression](compression.md) - Detailed analysis of compression algorithms
- [Probabilistic Structures](probabilistic-structures.md) - Bloom filters and sketching
- [Stochastic Processes](stochastic-processes.md) - Random processes in systems
- [Graph Theory](graph-theory.md) - Network topology and algorithms
- [Privacy Metrics](privacy-metrics.md) - Information-theoretic privacy measures
