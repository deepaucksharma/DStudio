---
title: Uber's Real-Time Location System
description: Track millions of drivers and riders globally with sub-second updates
type: case-study
difficulty: advanced
reading_time: 20 min
prerequisites: []
status: complete
last_updated: 2025-07-20
---

<!-- Navigation -->
[Home](../index.md) ‚Üí [Case Studies](index.md) ‚Üí **Uber's Real-Time Location System**

# üöó Uber's Real-Time Location System

**Challenge**: Track millions of drivers and riders globally with sub-second updates

!!! info "Sources"
    - Uber Engineering: "Scaling Real-time Infrastructure"¬π
    - QCon: "How Uber Scales Real-Time Platform"¬≤  
    - "Engineering Self-Healing Architecture"¬≥
    - H3 Geospatial Indexing‚Å¥

## üèóÔ∏è Architecture Evolution

### Phase 1: Simple Polling (2009-2011)
```text
Driver App ‚Üí API Gateway ‚Üí MySQL ‚Üí Dispatcher
```
**Problems**: Database overwhelmed, no real-time updates, synchronous polling

### Phase 2: In-Memory Grid (2011-2013)

```mermaid
graph TB
    subgraph "Client Layer"
        DA[Driver Apps]
        RA[Rider Apps]
    end
    
    subgraph "API Layer"
        LB[Load Balancer<br/>HAProxy]
        AS1[App Server 1]
        AS2[App Server 2]
        AS3[App Server N]
    end
    
    subgraph "Cache Layer"
        RC[Redis Cluster<br/>Location Cache]
    end
    
    subgraph "Persistence"
        M[MySQL<br/>Backup Storage]
    end
    
    DA --> LB
    RA --> LB
    LB --> AS1 & AS2 & AS3
    AS1 & AS2 & AS3 --> RC
    RC -.-> M
    
    style RC fill:#ff9999
```

**Key Decision**: Redis for hot data - 100x performance gain
- Latency: 500ms ‚Üí 50ms¬π

### Phase 3: Geospatial Sharding (2013-2016)

```mermaid
graph TB
    subgraph "Global Load Distribution"
        GLB[Global Load Balancer<br/>Geo-DNS]
    end
    
    subgraph "Regional Clusters"
        subgraph "North America"
            LB1[Regional LB]
            GS1[Geo Service]
            RC1[Redis Cluster]
            K1[Kafka Cluster]
        end
        
        subgraph "Europe" 
            LB2[Regional LB]
            GS2[Geo Service]
            RC2[Redis Cluster]
            K2[Kafka Cluster]
        end
        
        subgraph "Asia"
            LB3[Regional LB]
            GS3[Geo Service]
            RC3[Redis Cluster]
            K3[Kafka Cluster]
        end
    end
    
    subgraph "Global Services"
        CS[Cassandra<br/>Historical Data]
        MK[Master Kafka<br/>Cross-Region Sync]
    end
    
    GLB --> LB1 & LB2 & LB3
    RC1 & RC2 & RC3 --> CS
    K1 & K2 & K3 -.-> MK
```

**H3 Innovation**‚Å¥: Hexagonal grid system (0-15 resolution levels)
- Efficient neighbor queries, predictable sharding

### Phase 4: Event-Driven Architecture (2016-Present)

```mermaid
graph LR
    subgraph "Input Layer"
        DA[Driver App]
        RA[Rider App]
        subgraph "SDK"
            LS[Location SDK<br/>Adaptive Sampling]
            ES[Event SDK<br/>Batching]
        end
    end

    subgraph "Edge Services"
        ELB[Edge Load Balancer<br/>Envoy Proxy]
        AG[API Gateway<br/>Rate Limiting]
    end

    subgraph "Stream Processing"
        K[Kafka<br/>4M msgs/sec‚Åµ]
        subgraph "Processing"
            F[Flink<br/>Stateful Processing]
            S[Storm<br/>Real-time Analytics]
        end
    end

    subgraph "Core Services"
        LS2[Location Service<br/>H3 Grid]
        MS[Matching Service<br/>Supply/Demand]
        PS[Pricing Service<br/>Dynamic Pricing]
        ES2[ETA Service<br/>Route Calculation]
        NS[Notification Service]
    end

    subgraph "Storage Tier"
        R[Redis<br/>Live State<br/>5M ops/sec]
        C[Cassandra<br/>Trip History<br/>100TB]
        H[HDFS<br/>Analytics<br/>PB Scale]
        D[DynamoDB<br/>User Profiles]
    end

    subgraph "Observability"
        M[Metrics<br/>Prometheus]
        T[Tracing<br/>Jaeger]
        L[Logging<br/>ELK]
    end

    DA --> LS --> ELB
    RA --> ES --> ELB
    ELB --> AG --> K
    
    K --> F --> LS2 --> R
    K --> S --> MS
    
    LS2 --> ES2
    LS2 --> PS
    MS --> NS
    
    R --> C
    C --> H
    
    LS2 & MS & PS & ES2 -.-> M & T & L
```

**Patterns**: Event-driven (Kafka), Service mesh (Envoy), Circuit breakers, CQRS, Bulkheads

## üî¨ Complete Axiom Analysis

### Comprehensive Axiom Mapping Table

| Design Decision | Axiom 1: Latency | Axiom 2: Capacity | Axiom 3: Failure | Axiom 4: Concurrency | Axiom 5: Coordination | Axiom 6: Observability | Axiom 7: Human Interface | Axiom 8: Economics |
|-----------------|------------------|-------------------|------------------|----------------------|----------------------|------------------------|--------------------------|-------------------|
| **H3 Hexagonal Grid**‚Å¥ | 10ms lookup time vs 50ms for lat/lng | 40% less storage than rectangles | Cell boundaries remain stable | Parallel spatial queries | No coordinate conflicts | Clear cell ownership | Intuitive hex visualization | Reduces compute by 40% |
| **Geospatial Sharding**¬π | Data locality reduces latency | Natural partition boundaries | City-level failure isolation | Independent city operations | No cross-city coordination | Per-city metrics | City-based debugging | Infrastructure per market |
| **Event Streaming (Kafka)**¬≤ | Async processing, no blocking | Handles 4M events/sec | Replayable from any point | Multiple consumers | Ordered event delivery | Event flow tracing | Event-driven mental model | Shared infrastructure |
| **Redis for Live State**‚Å∂ | Sub-ms reads, 5ms writes | In-memory scales horizontally | Replicas for failover | Optimistic concurrency | No distributed locks | Real-time metrics | Simple key-value model | Memory cost vs disk |
| **Adaptive Sampling**‚Å∑ | Reduces network overhead | 68% less data transmitted | Graceful degradation | Per-driver rate limiting | Client-side decisions | Sampling rate metrics | Transparent to drivers | Bandwidth cost reduction |
| **Service Mesh (Envoy)**‚Å∏ | Circuit breakers prevent cascades | Request routing at edge | Automatic failover | Retry with backoff | Distributed tracing | Service dependency maps | Clear service boundaries | Reduces ops overhead |
| **CRDT Location Updates**‚Åπ | Conflict-free by design | Mergeable across partitions | Eventually consistent | Concurrent updates safe | No coordination needed | Convergence tracking | Simple last-write-wins | No consensus overhead |

### Axiom Impact Analysis

#### Axiom 1: Latency is Non-Zero
**Solution**: 35+ edge PoPs, regional DCs, multi-tier caching
**Results**¬π: P50: 45ms, P99: 200ms, Location update: 20ms

#### Axiom 2: Capacity is Finite
**Solution**: Adaptive sampling, delta encoding, smart batching
**Impact**¬≤: Writes: 1.25M/s ‚Üí 400K/s (-68%), Bandwidth: 625 MB/s ‚Üí 200 MB/s (-68%), Storage: 43.2 GB/day ‚Üí 13 GB/day (-70%)

#### Axiom 3: Failure is Inevitable
**Pillar Applied**: Control Distribution - Autonomous regional operation, self-healing, progressive degradation
**Patterns**: Circuit Breaker, Bulkhead, Graceful Degradation, Health Checks

**Resilience Mechanisms**¬≥:
```mermaid
graph TD
    subgraph "Failure Detection"
        HC[Health Checks<br/>Every 5s]
        CB[Circuit Breaker<br/>50% error rate]
        TO[Timeout<br/>1s threshold]
    end
    
    subgraph "Failure Response"
        F1[Fallback 1<br/>Last Known Location]
        F2[Fallback 2<br/>City Center Default]
        F3[Fallback 3<br/>Approximate Match]
    end
    
    subgraph "Recovery"
        R1[Auto-retry<br/>Exponential Backoff]
        R2[Traffic Shift<br/>Healthy Regions]
        R3[State Rebuild<br/>From Kafka]
    end
    
    HC & CB & TO --> F1
    F1 --> F2 --> F3
    F1 --> R1 --> R2 --> R3
```

#### Axiom 4: Concurrency Requires Coordination
**Pillar Applied**: Truth Distribution - Eventually consistent, CRDTs, event sourcing
**Patterns**: Event Sourcing, CQRS, Saga Pattern, Idempotent Operations

**Driver State Machine** (from Uber's architecture docs)¬π‚Å∞:
```mermaid
stateDiagram-v2
    [*] --> OFFLINE
    OFFLINE --> ONLINE: Go Online
    ONLINE --> DISPATCHED: Accept Ride
    DISPATCHED --> EN_ROUTE: Start Trip
    EN_ROUTE --> ARRIVED: Reach Pickup
    ARRIVED --> IN_TRIP: Start Ride
    IN_TRIP --> ONLINE: Complete Trip
    ONLINE --> OFFLINE: Go Offline
    
    note right of DISPATCHED: Atomic state transition<br/>Version vector tracking
    note right of IN_TRIP: Location updates<br/>Every 4 seconds
```

#### Axiom 5: Coordination is Hard
**Pillar Applied**: Truth Distribution - Regional autonomy, eventual consistency, gossip protocols
**Patterns**: Leader Election, Consensus, Vector Clocks, Tunable Consistency

#### Axiom 6: Observability is Required
**Pillar Applied**: Intelligence Distribution - Real-time dashboards, predictive analytics, anomaly detection
**Patterns**: Observability Stack, Distributed Tracing, SLI/SLO/SLA

**Observability Stack** (Uber's M3 platform)¬π¬π:
```yaml
Metrics (Prometheus/M3):
- 10M metrics/second
- 1-second granularity
- 30-day retention

Tracing (Jaeger):
- 1% sampling rate
- Critical path analysis
- Cross-service correlation

Logging (ELK):
- 100TB/day log volume
- Real-time search
- 7-day hot storage
```

#### Axiom 7: Human Interface Matters
**Pillar Applied**: Control Distribution - Intuitive dashboards, automated runbooks, progressive rollouts
**Patterns**: Runbook Automation, Progressive Deployment, Feature Flags

#### Axiom 8: Economics Drive Decisions
**Pillar Applied**: Intelligence Distribution - Adaptive resource allocation, spot instances, multi-cloud arbitrage
**Patterns**: Auto-scaling, Cost Optimization, Serverless

**Cost Optimization Results**¬π¬≤:
```yaml
Infrastructure Costs:
- Compute: $2.3M/month ‚Üí $1.5M (35% reduction via spot instances)
- Storage: $800K/month ‚Üí $500K (37% reduction via tiering)
- Network: $1.2M/month ‚Üí $700K (42% reduction via edge caching)
- Total Annual Savings: $15.6M
```

---

## üèõÔ∏è Architecture Alternatives

### Alternative 1: Centralized Database with Caching

```mermaid
graph TB
    subgraph "Centralized Architecture"
        C[Mobile Clients]
        LB[Load Balancer]
        AS[App Servers]
        RC[Redis Cache]
        PG[(PostgreSQL<br/>with PostGIS)]
        RR[(Read Replicas)]
    end
    
    C --> LB --> AS
    AS --> RC
    RC -.->|Cache Miss| PG
    AS --> PG
    PG -->|Replication| RR
    AS -.->|Read| RR
```

### Alternative 2: Peer-to-Peer Mesh

```mermaid
graph TB
    subgraph "P2P Architecture"
        D1[Driver 1]
        D2[Driver 2]
        D3[Driver 3]
        D4[Driver 4]
        R1[Rider 1]
        R2[Rider 2]
        
        DHT[Distributed<br/>Hash Table]
        BS[Bootstrap<br/>Servers]
    end
    
    D1 <--> D2
    D2 <--> D3
    D3 <--> D4
    D4 <--> D1
    D1 <--> D3
    
    R1 --> DHT
    R2 --> DHT
    DHT --> D1
    
    All nodes -.-> BS
```

### Alternative 3: Edge Computing with 5G

```mermaid
graph TB
    subgraph "Edge Architecture"
        MC[Mobile Clients]
        
        subgraph "5G Edge"
            MEC1[MEC Server 1]
            MEC2[MEC Server 2]
            MEC3[MEC Server 3]
        end
        
        subgraph "Regional DC"
            RS[Regional Service]
            RDB[(Regional DB)]
        end
        
        subgraph "Central DC"
            CS[Central Service]
            CDB[(Central DB)]
        end
    end
    
    MC --> MEC1
    MC --> MEC2
    MC --> MEC3
    
    MEC1 <--> MEC2
    MEC2 <--> MEC3
    MEC1 <--> MEC3
    
    MEC1 --> RS
    MEC2 --> RS
    MEC3 --> RS
    
    RS --> RDB
    RS --> CS
    CS --> CDB
```

### Alternative 4: Blockchain-Based Location

```mermaid
graph TB
    subgraph "Blockchain Architecture"
        D[Drivers]
        R[Riders]
        
        subgraph "Blockchain Network"
            N1[Node 1]
            N2[Node 2]
            N3[Node 3]
            N4[Node 4]
            BC[(Blockchain)]
        end
        
        subgraph "Off-chain"
            SC[State Channels]
            IPFS[IPFS Storage]
        end
    end
    
    D --> N1
    R --> N2
    
    N1 <--> N2
    N2 <--> N3
    N3 <--> N4
    N4 <--> N1
    
    All nodes --> BC
    
    D <--> SC
    R <--> SC
    SC --> IPFS
```

### Alternative 5: Uber's Chosen Architecture

```mermaid
graph TB
    subgraph "Uber's Architecture"
        subgraph "Client Layer"
            DA[Driver App]
            RA[Rider App]
        end
        
        subgraph "Edge PoPs"
            EP1[Edge PoP 1]
            EP2[Edge PoP 2]
        end
        
        subgraph "Stream Processing"
            K[Kafka Clusters]
            F[Flink Jobs]
        end
        
        subgraph "Services"
            LS[Location Service]
            MS[Matching Service]
            H3[H3 Index]
        end
        
        subgraph "Storage"
            R[Redis Clusters]
            C[Cassandra]
        end
    end
    
    DA --> EP1 --> K
    RA --> EP2 --> K
    K --> F
    F --> LS
    LS --> H3
    LS --> R
    R --> C
    LS --> MS
```

## üìä Architecture Trade-off Analysis

### Comprehensive Comparison Matrix

| Aspect | Centralized DB | P2P Mesh | Edge Computing | Blockchain | Uber's Choice |
|--------|----------------|----------|----------------|------------|---------------|
| **Latency** | ‚ùå 100-200ms | ‚ö†Ô∏è Variable (50-500ms) | ‚úÖ <10ms at edge | ‚ùå Seconds | ‚úÖ 45ms P50 |
| **Scalability** | ‚ùå Database bottleneck | ‚ö†Ô∏è O(log n) lookups | ‚úÖ Edge scales | ‚ùå Limited TPS | ‚úÖ Linear scaling |
| **Reliability** | ‚ùå SPOF at DB | ‚úÖ No SPOF | ‚úÖ Edge redundancy | ‚úÖ Immutable | ‚úÖ 99.97% uptime |
| **Consistency** | ‚úÖ Strong | ‚ùå Eventually consistent | ‚ö†Ô∏è Edge sync issues | ‚úÖ Consensus | ‚ö†Ô∏è Eventual |
| **Cost** | ‚úÖ Simple, predictable | ‚úÖ Minimal infrastructure | ‚ùå Edge expensive | ‚ùå High compute | ‚ö†Ô∏è Moderate |
| **Privacy** | ‚ùå Centralized data | ‚úÖ Distributed | ‚ö†Ô∏è Telco dependency | ‚ö†Ô∏è Public ledger | ‚ö†Ô∏è Centralized |
| **Complexity** | ‚úÖ Simple | ‚ùå NAT, connectivity | ‚ùå Edge orchestration | ‚ùå Very complex | ‚ö†Ô∏è Moderate |
| **Global Scale** | ‚ùå Latency issues | ‚ùå Discovery problems | ‚úÖ Local processing | ‚ùå Sync delays | ‚úÖ Multi-region |

### Decision Factors for Architecture Selection

| Factor | Weight | Centralized | P2P | Edge | Blockchain | Uber |
|--------|--------|------------|-----|------|------------|------|
| **Real-time Updates** | 30% | 2/10 | 4/10 | 9/10 | 1/10 | 8/10 |
| **Global Scale** | 25% | 3/10 | 5/10 | 8/10 | 2/10 | 9/10 |
| **Cost Efficiency** | 20% | 8/10 | 9/10 | 3/10 | 1/10 | 7/10 |
| **Reliability** | 15% | 4/10 | 7/10 | 8/10 | 9/10 | 8/10 |
| **Developer Experience** | 10% | 9/10 | 3/10 | 5/10 | 2/10 | 7/10 |
| **Total Score** | 100% | 4.7/10 | 5.8/10 | 6.9/10 | 2.4/10 | **7.9/10** |

---

## üí° Key Design Decisions

### 1. Push vs Pull Architecture
**Decision**: Hybrid - Push for driver updates, Pull for rider queries
**Rationale**: Minimize data transfer while ensuring freshness

### 2. Consistency Model
**Decision**: Eventual consistency with bounded staleness
- Location updates: Best effort
- Trip state: Strong consistency
- Billing: Exactly-once processing

### 3. Storage Architecture
**Decision**: Polyglot persistence
- Redis: Live locations (TTL: 5 minutes)
- Cassandra: Historical data (TTL: 30 days)
- S3/HDFS: Archive (indefinite)

### 4. Matching Algorithm
**Decision**: Hierarchical search with ML ranking
```text
1. Coarse filter: H3 cells within radius
2. Fine filter: Actual distance calculation
3. ML ranking: Driver behavior, traffic, history
4. Assignment: Distributed lock for atomicity
```

---

## üìä Production Metrics & Scale

Based on Uber's 2023 engineering reports¬π¬≥:

### System Scale
```yaml
Global Statistics:
- Active Cities: 10,000+
- Countries: 70+
- Monthly Active Drivers: 5.4 million
- Monthly Active Riders: 130 million
- Trips per Day: 25 million
- Location Updates: 4 million/second (peak)

Infrastructure Scale:
- Kafka Messages: 4 trillion/day
- Redis Operations: 10 million/second
- Cassandra Nodes: 10,000+
- Container Instances: 100,000+
- Microservices: 4,000+
```

### Reliability Metrics
```yaml
Availability by Region:
- North America: 99.99% (4.38 minutes downtime/month)
- Europe: 99.98% (8.76 minutes/month)
- Asia: 99.97% (13.14 minutes/month)
- Global Average: 99.98%

Performance SLOs:
- Location Update Latency P99: <200ms ‚úì
- Trip Match Time P99: <15 seconds ‚úì
- ETA Accuracy: ¬±2 minutes (85% of trips) ‚úì
```

---

## üéØ Key Innovations & Lessons

### 1. H3 Geospatial Index
**Why Hexagons**‚Å¥: Equal neighbor distance, no orientation bias, natural hierarchy
**Impact**: Computation -40%, Storage -60%

### 2. Adaptive Sampling
```python
if driver.speed < 5 mph and driver.stationary_time > 60s:
    update_frequency = 30s  # Stationary
elif driver.in_trip:
    update_frequency = 4s   # In trip
else:
    update_frequency = 10s  # Default
```
**Impact**: -68% bandwidth‚Å∑

### 3. Regional Fault Isolation
**City-as-failure-domain**¬≥: No cross-city dependencies, autonomous operation

### 4. CRDT Location Updates
Type: Last-Write-Wins Register, Merge: Max(timestamp)
Trade-off: Temporary inconsistency for convergence‚Åπ

---

## üß™ Failure Scenarios & Mitigations

### Scenario 1: Regional Data Center Failure
**Impact**: 5M users affected
**Mitigation**: Auto-failover < 30s, degraded mode with cache, progressive restoration

### Scenario 2: Kafka Cluster Partition
**Impact**: Location update delays
**Mitigation**: Multi-cluster mirroring, client buffering, auto-repartitioning

### Scenario 3: Redis Memory Exhaustion
**Impact**: Cannot store new locations
**Mitigation**: Aggressive TTL, emergency eviction, secondary storage overflow

---

## üèÜ Key Lessons

1. **Iterate**: Start simple, measure, then optimize
2. **Fail-safe**: Design for failure, test with chaos
3. **Common case**: 95% trips in 100 cities - optimize for density
4. **Explicit trade-offs**: Document CAP choices, version APIs
5. **Developer experience**: Strong typing, monitoring, self-service¬π‚Å¥

## üîó Related Resources

**Patterns**: Event-driven, Geospatial sharding, Service mesh, CQRS, Circuit breaker, Bulkhead, Edge computing

**Similar Systems**: [Google Maps](google-maps-enhanced.md), [WhatsApp](chat-system-enhanced.md)

## References

¬π [Uber Engineering: Scaling Uber's Real-time Market Platform](https://eng.uber.com/scaling-uber-real-time-market-platform/)

¬≤ [QCon 2018: How Uber Scales Their Real-Time Market Platform](https://www.infoq.com/presentations/uber-market-platform/)

¬≥ [Uber: Engineering Uber's Self-Healing Architecture](https://eng.uber.com/engineering-ubers-self-healing-architecture/)

‚Å¥ [Uber Engineering: H3 - Uber's Hexagonal Hierarchical Spatial Index](https://eng.uber.com/h3/)

‚Åµ [Uber: How We Built Uber Engineering's Highest Query per Second Service Using Go](https://eng.uber.com/go-geofence-highest-query-per-second-service/)

‚Å∂ [Uber: Scaling Uber with Redis](https://eng.uber.com/scaling-redis-at-uber/)

‚Å∑ [Uber: Optimizing Uber's Location Updates](https://eng.uber.com/uber-location-updates/)

‚Å∏ [Uber: Evolving Distributed Tracing at Uber Engineering](https://eng.uber.com/distributed-tracing/)

‚Åπ [Uber: CRDT-Based State Management](https://eng.uber.com/schemaless-part-three/)

¬π‚Å∞ [Uber: Designing Uber's Driver State Machine](https://eng.uber.com/driver-state-machine/)

¬π¬π [Uber: M3 - Uber's Open Source Large-scale Metrics Platform](https://eng.uber.com/m3/)

¬π¬≤ [Uber Engineering: Cost-Effective Compute Capacity](https://eng.uber.com/cost-effective-compute/)

¬π¬≥ [Uber Investor Report Q4 2023](https://investor.uber.com/financials/quarterly-results/)

¬π‚Å¥ [Uber Engineering: Lessons Learned from Scaling Uber to 2000 Engineers](https://eng.uber.com/scaling-engineering-team/)

## üîç Related Concepts & Deep Dives

### üìö Relevant Axioms (Part I)
- **[Axiom 1: Latency](../part1-axioms/axiom1-latency/index.md)** - Speed of light limits (150ms SF‚ÜíSingapore) drive regional architecture decisions
- **[Axiom 2: Finite Capacity](../part1-axioms/axiom2-capacity/index.md)** - H3 hexagonal grid partitioning ensures even load distribution across cells
- **[Axiom 3: Failure is Normal](../part1-axioms/axiom3-failure/index.md)** - Multi-region replication and graceful degradation handle infrastructure failures
- **[Axiom 4: Concurrency](../part1-axioms/axiom4-concurrency/index.md)** - Lock-free data structures handle millions of concurrent location updates
- **[Axiom 5: Coordination](../part1-axioms/axiom5-coordination/index.md)** - Ringpop gossip protocol coordinates without central authority
- **[Axiom 6: Observability](../part1-axioms/axiom6-observability/index.md)** - Real-time dashboards track driver density and system health per region
- **[Axiom 7: Human Interface](../part1-axioms/axiom7-human/index.md)** - Driver app design optimizes for one-handed operation while driving
- **[Axiom 8: Economics](../part1-axioms/axiom8-economics/index.md)** - Efficient matching algorithms reduce driver idle time and fuel costs

### üèõÔ∏è Related Patterns (Part III)
- **[Sharding & Partitioning](../patterns/sharding.md)** - H3 hexagonal grid provides natural geographic sharding boundaries
- **[Event-Driven Architecture](../patterns/event-driven.md)** - Location updates flow through Kafka event streams
- **[CQRS](../patterns/cqrs.md)** - Separate write path (location updates) from read path (driver queries)
- **[Circuit Breaker](../patterns/circuit-breaker.md)** - Protects dispatch service from location service failures
- **[Edge Computing](../patterns/edge-computing.md)** - Regional data centers reduce location update latency
- **[Service Mesh](../patterns/service-mesh.md)** - Envoy proxies handle service-to-service communication
- **[Load Balancing](../patterns/load-balancing.md)** - Geo-aware routing directs requests to nearest data center

### üìä Quantitative Models
- **[Little's Law](../quantitative/littles-law.md)** - Driver utilization: L = ŒªW (active drivers = arrival rate √ó trip duration)
- **[Queueing Theory](../quantitative/queueing-models.md)** - M/M/c model for driver dispatch optimization
- **[Scaling Laws](../quantitative/scaling-laws.md)** - Square root scaling: doubling drivers reduces wait time by ‚àö2
- **[CAP Theorem](../quantitative/cap-pacelc.md)** - Chooses AP: available during network partitions with eventual consistency

### üë• Human Factors Considerations
- **[On-Call Culture](../human-factors/oncall-culture.md)** - 24/7 global operations require follow-the-sun support model
- **[Incident Response](../human-factors/incident-response.md)** - Playbooks for common scenarios (region failures, GPS outages)
- **[Observability Tools](../human-factors/observability-stacks.md)** - Heat maps show driver density and demand patterns
- **[SRE Practices](../human-factors/sre-practices.md)** - Error budgets balance innovation with reliability

### üîÑ Similar Case Studies
- **[Amazon DynamoDB](amazon-dynamo.md)** - Similar challenges with global distribution and availability
- **[News Feed System](news-feed.md)** - Real-time data distribution to millions of users
- **[YouTube's Video Platform](youtube.md)** - Geographic content distribution and edge caching
- **[Consistent Hashing](consistent-hashing.md)** - Core technique used in Uber's Ringpop protocol

*"At Uber's scale, the speed of light becomes a real constraint in system design."*

**Next**: [Amazon DynamoDB ‚Üí](amazon-dynamo.md)
