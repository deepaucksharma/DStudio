# Designing Amazon S3: Object Storage at Scale

## Problem Statement

Design a highly scalable, durable, and available object storage service that can:
- Store unlimited amounts of data (objects up to 5TB)
- Provide 99.999999999% (11 9s) durability
- Support millions of requests per second
- Offer multiple storage classes for cost optimization
- Enable global access with low latency
- Provide strong consistency for all operations

## Core Architecture

### Regional Infrastructure

```mermaid
graph TB
    subgraph "Region (e.g., us-east-1)"
        subgraph "Availability Zone 1"
            S1[Storage Node 1]
            S2[Storage Node 2]
            M1[Metadata Server]
        end
        
        subgraph "Availability Zone 2"
            S3[Storage Node 3]
            S4[Storage Node 4]
            M2[Metadata Server]
        end
        
        subgraph "Availability Zone 3"
            S5[Storage Node 5]
            S6[Storage Node 6]
            M3[Metadata Server]
        end
        
        LB[Load Balancer]
        API[API Gateway]
        
        LB --> API
        API --> M1
        API --> M2
        API --> M3
    end
    
    Client[Client] --> LB
```

### Key Components

1. **Front-end Services**
   - API Gateway: REST API processing
   - Authentication: IAM integration
   - Request routing: Intelligent load distribution

2. **Metadata Layer**
   - Object metadata storage
   - Bucket configuration
   - Access control lists (ACLs)
   - Versioning information

3. **Storage Layer**
   - Distributed file system
   - Erasure coding for durability
   - Replication across AZs

## Data Durability: Achieving 11 9s

### Erasure Coding Implementation

```python
import numpy as np
from typing import List, Tuple

class ErasureCoding:
    def __init__(self, data_shards: int = 10, parity_shards: int = 4):
        """
        Reed-Solomon erasure coding for S3-like durability.
        Default: 10 data shards + 4 parity shards = 14 total
        Can tolerate loss of any 4 shards
        """
        self.data_shards = data_shards
        self.parity_shards = parity_shards
        self.total_shards = data_shards + parity_shards
        
    def encode(self, data: bytes) -> List[bytes]:
        """Split data into shards and generate parity shards"""
# Calculate shard size
        shard_size = (len(data) + self.data_shards - 1) // self.data_shards
        
# Pad data if necessary
        padded_data = data + b'\0' * (shard_size * self.data_shards - len(data))
        
# Split into data shards
        data_shards = [
            padded_data[i:i + shard_size] 
            for i in range(0, len(padded_data), shard_size)
        ]
        
# Generate parity shards (simplified)
        parity_shards = self._generate_parity(data_shards)
        
        return data_shards + parity_shards
    
    def _generate_parity(self, data_shards: List[bytes]) -> List[bytes]:
        """Generate parity shards using Galois Field arithmetic"""
# Simplified parity generation
        parity_shards = []
        
        for p in range(self.parity_shards):
            parity = bytearray(len(data_shards[0]))
            for i, shard in enumerate(data_shards):
                coefficient = pow(i + 1, p + 1, 256)  # GF(256) arithmetic
                for j, byte in enumerate(shard):
                    parity[j] ^= (byte * coefficient) % 256
            parity_shards.append(bytes(parity))
            
        return parity_shards

    def decode(self, available_shards: List[Tuple[int, bytes]]) -> bytes:
        """Reconstruct data from available shards"""
        if len(available_shards) < self.data_shards:
            raise ValueError("Insufficient shards for reconstruction")
        
# Sort by shard index
        available_shards.sort(key=lambda x: x[0])
        
# If we have all data shards, simply concatenate
        data_shard_indices = set(range(self.data_shards))
        available_indices = {idx for idx, _ in available_shards}
        
        if data_shard_indices.issubset(available_indices):
            data = b''.join(
                shard for idx, shard in available_shards 
                if idx < self.data_shards
            )
            return data.rstrip(b'\0')  # Remove padding
        
# Otherwise, reconstruct using parity
        return self._reconstruct_from_parity(available_shards)
```

### Multi-Region Replication

```python
class S3ReplicationManager:
    def __init__(self):
        self.regions = ['us-east-1', 'us-west-2', 'eu-west-1']
        self.replication_rules = {}
        
    def configure_cross_region_replication(self, 
                                         source_bucket: str,
                                         destination_bucket: str,
                                         destination_region: str,
                                         storage_class: str = 'STANDARD_IA'):
        """Configure cross-region replication for durability"""
        rule = {
            'ID': f'crr-{source_bucket}-to-{destination_bucket}',
            'Priority': 1,
            'Status': 'Enabled',
            'Filter': {'Prefix': ''},
            'Destination': {
                'Bucket': f'arn:aws:s3:::{destination_bucket}',
                'Region': destination_region,
                'StorageClass': storage_class,
                'ReplicationTime': {
                    'Status': 'Enabled',
                    'Time': {'Minutes': 15}  # RTO target
                },
                'Metrics': {
                    'Status': 'Enabled',
                    'EventThreshold': {'Minutes': 15}
                }
            }
        }
        
        self.replication_rules[source_bucket] = rule
        return rule
        
    def replicate_object(self, bucket: str, key: str, data: bytes):
        """Asynchronously replicate object to configured destinations"""
        import asyncio
        
        async def replicate_to_region(region: str, data: bytes):
# Simulate replication to different region
            await asyncio.sleep(0.1)  # Network latency
            print(f"Replicated {key} to {region}")
            return True
            
# Get replication configuration
        if bucket in self.replication_rules:
            rule = self.replication_rules[bucket]
            destination_region = rule['Destination']['Region']
            
# Perform async replication
            asyncio.create_task(
                replicate_to_region(destination_region, data)
            )
```

## Storage Classes and Lifecycle Policies

### Storage Class Characteristics

| Storage Class | Durability | Availability | Latency | Min Storage Duration | Use Case |
|--------------|------------|--------------|---------|---------------------|----------|
| STANDARD | 11 9s | 99.99% | ms | None | Frequently accessed |
| STANDARD_IA | 11 9s | 99.9% | ms | 30 days | Infrequent access |
| ONE_ZONE_IA | 11 9s | 99.5% | ms | 30 days | Non-critical, infrequent |
| GLACIER_IR | 11 9s | 99.9% | minutes | 90 days | Archive, rapid retrieval |
| GLACIER_FLEXIBLE | 11 9s | 99.99% | 1-12 hrs | 90 days | Long-term archive |
| GLACIER_DEEP | 11 9s | 99.99% | 12-48 hrs | 180 days | Compliance archives |
| INTELLIGENT_TIERING | 11 9s | 99.9% | ms | None | Unknown access patterns |

### Lifecycle Policy Engine

```python
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional

class StorageClass(Enum):
    STANDARD = "STANDARD"
    STANDARD_IA = "STANDARD_IA"
    ONE_ZONE_IA = "ONE_ZONE_IA"
    GLACIER_IR = "GLACIER_INSTANT_RETRIEVAL"
    GLACIER_FLEXIBLE = "GLACIER_FLEXIBLE_RETRIEVAL"
    GLACIER_DEEP = "GLACIER_DEEP_ARCHIVE"
    INTELLIGENT_TIERING = "INTELLIGENT_TIERING"

class LifecycleRule:
    def __init__(self, rule_id: str, prefix: str = ""):
        self.rule_id = rule_id
        self.prefix = prefix
        self.transitions = []
        self.expiration = None
        
    def add_transition(self, days: int, storage_class: StorageClass):
        """Add storage class transition after specified days"""
        self.transitions.append({
            'Days': days,
            'StorageClass': storage_class
        })
        self.transitions.sort(key=lambda x: x['Days'])
        
    def set_expiration(self, days: int):
        """Set object expiration after specified days"""
        self.expiration = {'Days': days}

class LifecycleManager:
    def __init__(self):
        self.rules: Dict[str, List[LifecycleRule]] = {}
        
    def create_standard_lifecycle(self, bucket: str) -> LifecycleRule:
        """Create a standard lifecycle policy for cost optimization"""
        rule = LifecycleRule("standard-lifecycle")
        
# Transition to IA after 30 days
        rule.add_transition(30, StorageClass.STANDARD_IA)
        
# Transition to Glacier IR after 90 days
        rule.add_transition(90, StorageClass.GLACIER_IR)
        
# Transition to Glacier Flexible after 180 days
        rule.add_transition(180, StorageClass.GLACIER_FLEXIBLE)
        
# Transition to Deep Archive after 365 days
        rule.add_transition(365, StorageClass.GLACIER_DEEP)
        
# Expire after 7 years
        rule.set_expiration(2555)
        
        if bucket not in self.rules:
            self.rules[bucket] = []
        self.rules[bucket].append(rule)
        
        return rule
        
    def evaluate_object(self, bucket: str, key: str, 
                       creation_date: datetime) -> Optional[str]:
        """Evaluate which storage class an object should be in"""
        if bucket not in self.rules:
            return None
            
        age_days = (datetime.now() - creation_date).days
        
        for rule in self.rules[bucket]:
            if key.startswith(rule.prefix):
# Check transitions
                for transition in reversed(rule.transitions):
                    if age_days >= transition['Days']:
                        return transition['StorageClass'].value
                        
# Check expiration
                if rule.expiration and age_days >= rule.expiration['Days']:
                    return "EXPIRED"
                    
        return StorageClass.STANDARD.value
```

## Multipart Upload Implementation

### Multipart Upload Manager

```python
import hashlib
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import BinaryIO, List, Tuple

class MultipartUpload:
    def __init__(self, bucket: str, key: str, upload_id: str):
        self.bucket = bucket
        self.key = key
        self.upload_id = upload_id
        self.parts = {}
        self.lock = threading.Lock()
        
    def add_part(self, part_number: int, etag: str, size: int):
        """Track uploaded part"""
        with self.lock:
            self.parts[part_number] = {
                'ETag': etag,
                'PartNumber': part_number,
                'Size': size
            }

class S3MultipartUploadManager:
    def __init__(self, part_size: int = 100 * 1024 * 1024):  # 100MB default
        self.part_size = part_size
        self.min_part_size = 5 * 1024 * 1024  # 5MB minimum
        self.max_parts = 10000
        self.uploads = {}
        
    def initiate_multipart_upload(self, bucket: str, key: str) -> str:
        """Initiate a new multipart upload"""
        import uuid
        upload_id = str(uuid.uuid4())
        
        upload = MultipartUpload(bucket, key, upload_id)
        self.uploads[upload_id] = upload
        
        return upload_id
        
    def upload_large_file(self, bucket: str, key: str, 
                         file_path: str, 
                         parallel_uploads: int = 10) -> dict:
        """Upload a large file using multipart upload"""
        import os
        
        file_size = os.path.getsize(file_path)
        
# Calculate optimal part size
        part_size = self._calculate_part_size(file_size)
        
# Initiate multipart upload
        upload_id = self.initiate_multipart_upload(bucket, key)
        
        try:
# Upload parts in parallel
            with ThreadPoolExecutor(max_workers=parallel_uploads) as executor:
                futures = []
                
                with open(file_path, 'rb') as f:
                    part_number = 1
                    while True:
                        data = f.read(part_size)
                        if not data:
                            break
                            
                        future = executor.submit(
                            self._upload_part,
                            upload_id, part_number, data
                        )
                        futures.append((part_number, future))
                        part_number += 1
                
# Wait for all uploads to complete
                for part_number, future in futures:
                    etag = future.result()
                    print(f"Uploaded part {part_number}, ETag: {etag}")
            
# Complete multipart upload
            return self.complete_multipart_upload(upload_id)
            
        except Exception as e:
# Abort on failure
            self.abort_multipart_upload(upload_id)
            raise e
            
    def _calculate_part_size(self, file_size: int) -> int:
        """Calculate optimal part size based on file size"""
# Ensure we don't exceed max parts limit
        min_required_size = file_size // self.max_parts + 1
        
# Use larger of default or minimum required
        part_size = max(self.part_size, min_required_size)
        
# Ensure minimum part size
        return max(part_size, self.min_part_size)
        
    def _upload_part(self, upload_id: str, 
                    part_number: int, data: bytes) -> str:
        """Upload a single part"""
# Calculate MD5 for integrity
        md5 = hashlib.md5(data).hexdigest()
        
# Simulate upload (in real implementation, this would upload to S3)
        etag = f'"{md5}"'
        
# Track uploaded part
        upload = self.uploads[upload_id]
        upload.add_part(part_number, etag, len(data))
        
        return etag
        
    def complete_multipart_upload(self, upload_id: str) -> dict:
        """Complete the multipart upload"""
        upload = self.uploads[upload_id]
        
# Sort parts by part number
        parts = sorted(
            upload.parts.values(), 
            key=lambda x: x['PartNumber']
        )
        
# Calculate final ETag (simplified)
        combined_etags = ''.join(
            part['ETag'].strip('"') for part in parts
        )
        final_etag = f'"{hashlib.md5(combined_etags.encode()).hexdigest()}-{len(parts)}"'
        
# Clean up
        del self.uploads[upload_id]
        
        return {
            'Location': f'https://{upload.bucket}.s3.amazonaws.com/{upload.key}',
            'Bucket': upload.bucket,
            'Key': upload.key,
            'ETag': final_etag
        }
        
    def abort_multipart_upload(self, upload_id: str):
        """Abort and clean up multipart upload"""
        if upload_id in self.uploads:
            del self.uploads[upload_id]
```

## Cost Optimization Strategies

### 1. Intelligent Tiering Implementation

```python
class IntelligentTieringOptimizer:
    def __init__(self):
        self.access_patterns = {}
        self.tier_thresholds = {
            'frequent': 1,      # Accessed in last day
            'infrequent': 30,   # Not accessed for 30 days
            'archive': 90,      # Not accessed for 90 days
            'deep_archive': 180 # Not accessed for 180 days
        }
        
    def track_access(self, bucket: str, key: str):
        """Track object access for intelligent tiering"""
        access_key = f"{bucket}/{key}"
        self.access_patterns[access_key] = datetime.now()
        
    def recommend_tier(self, bucket: str, key: str) -> str:
        """Recommend storage tier based on access patterns"""
        access_key = f"{bucket}/{key}"
        
        if access_key not in self.access_patterns:
            return 'archive'  # No access history
            
        last_access = self.access_patterns[access_key]
        days_since_access = (datetime.now() - last_access).days
        
        if days_since_access < self.tier_thresholds['frequent']:
            return 'frequent'
        elif days_since_access < self.tier_thresholds['infrequent']:
            return 'infrequent'
        elif days_since_access < self.tier_thresholds['archive']:
            return 'archive_instant'
        elif days_since_access < self.tier_thresholds['deep_archive']:
            return 'archive_flexible'
        else:
            return 'deep_archive'
```

### 2. Request Cost Optimizer

```python
class S3RequestOptimizer:
    def __init__(self):
        self.request_costs = {
            'PUT': 0.005,      # Per 1000 requests
            'GET': 0.0004,     # Per 1000 requests
            'LIST': 0.005,     # Per 1000 requests
            'DELETE': 0.0      # Free
        }
        
    def batch_operations(self, operations: List[dict]) -> List[dict]:
        """Batch operations to reduce request costs"""
        batched = []
        
# Group operations by type
        grouped = {}
        for op in operations:
            op_type = op['type']
            if op_type not in grouped:
                grouped[op_type] = []
            grouped[op_type].append(op)
            
# Batch deletions (up to 1000 per request)
        if 'DELETE' in grouped:
            for i in range(0, len(grouped['DELETE']), 1000):
                batch = grouped['DELETE'][i:i+1000]
                batched.append({
                    'type': 'DELETE_BATCH',
                    'objects': [op['key'] for op in batch]
                })
                
# Batch copy operations
        if 'COPY' in grouped:
            for op in grouped['COPY']:
# Use multipart copy for large objects
                if op.get('size', 0) > 5 * 1024 * 1024 * 1024:  # 5GB
                    batched.append({
                        'type': 'MULTIPART_COPY',
                        'source': op['source'],
                        'destination': op['destination']
                    })
                else:
                    batched.append(op)
                    
        return batched
```

### 3. Storage Analytics and Recommendations

```python
class S3StorageAnalyzer:
    def __init__(self):
        self.storage_costs = {
            'STANDARD': 0.023,           # Per GB/month
            'STANDARD_IA': 0.0125,       # Per GB/month
            'ONE_ZONE_IA': 0.01,         # Per GB/month
            'GLACIER_IR': 0.004,         # Per GB/month
            'GLACIER_FLEXIBLE': 0.0036,  # Per GB/month
            'GLACIER_DEEP': 0.00099      # Per GB/month
        }
        
    def analyze_bucket(self, bucket_inventory: List[dict]) -> dict:
        """Analyze bucket for cost optimization opportunities"""
        analysis = {
            'current_cost': 0,
            'optimized_cost': 0,
            'savings': 0,
            'recommendations': []
        }
        
        for obj in bucket_inventory:
            size_gb = obj['size'] / (1024**3)
            current_class = obj['storage_class']
            last_accessed = obj.get('last_accessed', datetime.now())
            age_days = (datetime.now() - obj['creation_date']).days
            
# Calculate current cost
            current_cost = size_gb * self.storage_costs.get(current_class, 0.023)
            analysis['current_cost'] += current_cost
            
# Recommend optimal storage class
            recommended_class = self._recommend_storage_class(
                size_gb, age_days, last_accessed
            )
            
            if recommended_class != current_class:
                optimized_cost = size_gb * self.storage_costs[recommended_class]
                savings = current_cost - optimized_cost
                
                if savings > 0:
                    analysis['optimized_cost'] += optimized_cost
                    analysis['recommendations'].append({
                        'object': obj['key'],
                        'current_class': current_class,
                        'recommended_class': recommended_class,
                        'monthly_savings': savings,
                        'reason': self._get_recommendation_reason(
                            age_days, last_accessed
                        )
                    })
                else:
                    analysis['optimized_cost'] += current_cost
            else:
                analysis['optimized_cost'] += current_cost
                
        analysis['savings'] = analysis['current_cost'] - analysis['optimized_cost']
        analysis['savings_percentage'] = (
            (analysis['savings'] / analysis['current_cost'] * 100) 
            if analysis['current_cost'] > 0 else 0
        )
        
        return analysis
        
    def _recommend_storage_class(self, size_gb: float, 
                                age_days: int, 
                                last_accessed: datetime) -> str:
        """Recommend optimal storage class based on access patterns"""
        days_since_access = (datetime.now() - last_accessed).days
        
# Large files (>128KB) with infrequent access
        if size_gb > 0.000128:  # 128KB in GB
            if days_since_access > 180:
                return 'GLACIER_DEEP'
            elif days_since_access > 90:
                return 'GLACIER_FLEXIBLE'
            elif days_since_access > 30:
                return 'STANDARD_IA'
                
        return 'STANDARD'
```

## Performance Optimization

### Request Routing and Load Balancing

```python
class S3RequestRouter:
    def __init__(self):
        self.regions = {
            'us-east-1': {'endpoints': ['s3.us-east-1.amazonaws.com'], 'weight': 100},
            'us-west-2': {'endpoints': ['s3.us-west-2.amazonaws.com'], 'weight': 100},
            'eu-west-1': {'endpoints': ['s3.eu-west-1.amazonaws.com'], 'weight': 100}
        }
        self.bucket_locations = {}
        
    def route_request(self, bucket: str, operation: str, 
                     client_region: str) -> str:
        """Route request to optimal endpoint"""
# Get bucket location
        bucket_region = self.bucket_locations.get(
            bucket, 
            self._detect_bucket_region(bucket)
        )
        
# For read operations, consider using closer region with replication
        if operation in ['GET', 'HEAD']:
            return self._find_nearest_replica(bucket, client_region)
            
# For write operations, must use primary region
        return f"https://s3.{bucket_region}.amazonaws.com"
        
    def _detect_bucket_region(self, bucket: str) -> str:
        """Detect bucket's primary region"""
# In production, this would make a HEAD request to determine region
# For now, return default
        return 'us-east-1'
        
    def _find_nearest_replica(self, bucket: str, client_region: str) -> str:
        """Find nearest replica for read operations"""
# Simple latency-based selection
        latencies = {
            ('us-east-1', 'us-east-1'): 1,
            ('us-east-1', 'us-west-2'): 50,
            ('us-east-1', 'eu-west-1'): 80,
            ('us-west-2', 'us-west-2'): 1,
            ('us-west-2', 'us-east-1'): 50,
            ('us-west-2', 'eu-west-1'): 120,
            ('eu-west-1', 'eu-west-1'): 1,
            ('eu-west-1', 'us-east-1'): 80,
            ('eu-west-1', 'us-west-2'): 120
        }
        
        best_region = min(
            self.regions.keys(),
            key=lambda r: latencies.get((client_region, r), 999)
        )
        
        return f"https://s3.{best_region}.amazonaws.com"
```

### Connection Pooling and Request Optimization

```python
import asyncio
from typing import Optional
import aiohttp

class S3ConnectionPool:
    def __init__(self, max_connections: int = 100):
        self.max_connections = max_connections
        self.sessions = {}
        
    async def get_session(self, region: str) -> aiohttp.ClientSession:
        """Get or create session for region"""
        if region not in self.sessions:
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                ttl_dns_cache=300,  # Cache DNS for 5 minutes
                enable_cleanup_closed=True
            )
            
            self.sessions[region] = aiohttp.ClientSession(
                connector=connector,
                timeout=aiohttp.ClientTimeout(total=30)
            )
            
        return self.sessions[region]
        
    async def execute_request(self, method: str, url: str, 
                            **kwargs) -> dict:
        """Execute S3 request with retry logic"""
        region = self._extract_region_from_url(url)
        session = await self.get_session(region)
        
        max_retries = 3
        backoff = 1
        
        for attempt in range(max_retries):
            try:
                async with session.request(method, url, **kwargs) as response:
                    if response.status == 503:  # Service unavailable
                        if attempt < max_retries - 1:
                            await asyncio.sleep(backoff)
                            backoff *= 2
                            continue
                            
                    response.raise_for_status()
                    return await response.json()
                    
            except aiohttp.ClientError as e:
                if attempt == max_retries - 1:
                    raise
                await asyncio.sleep(backoff)
                backoff *= 2
                
    def _extract_region_from_url(self, url: str) -> str:
        """Extract AWS region from S3 URL"""
        import re
        match = re.search(r's3\.([a-z0-9-]+)\.amazonaws\.com', url)
        return match.group(1) if match else 'us-east-1'
```

## Security and Access Control

### IAM Policy Evaluation Engine

```python
import json
from typing import Dict, List, Tuple

class S3PolicyEvaluator:
    def __init__(self):
        self.policy_cache = {}
        
    def evaluate_access(self, principal: str, bucket: str, 
                       key: str, action: str) -> Tuple[bool, str]:
        """Evaluate if principal has access to perform action"""
# Check bucket policy
        bucket_allow = self._evaluate_bucket_policy(
            principal, bucket, key, action
        )
        
# Check IAM policies
        iam_allow = self._evaluate_iam_policies(
            principal, bucket, key, action
        )
        
# Check ACLs
        acl_allow = self._evaluate_acls(
            principal, bucket, key, action
        )
        
# Access is allowed if any policy allows and none explicitly deny
        if bucket_allow[0] or iam_allow[0] or acl_allow[0]:
            return True, "Access allowed by policy"
            
        return False, "Access denied - no matching allow policy"
        
    def _evaluate_bucket_policy(self, principal: str, 
                               bucket: str, key: str, 
                               action: str) -> Tuple[bool, str]:
        """Evaluate bucket policy"""
        policy = {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Sid": "PublicReadGetObject",
                    "Effect": "Allow",
                    "Principal": "*",
                    "Action": "s3:GetObject",
                    "Resource": f"arn:aws:s3:::{bucket}/public/*"
                },
                {
                    "Sid": "DenyUnencryptedObjectUploads",
                    "Effect": "Deny",
                    "Principal": "*",
                    "Action": "s3:PutObject",
                    "Resource": f"arn:aws:s3:::{bucket}/*",
                    "Condition": {
                        "StringNotEquals": {
                            "s3:x-amz-server-side-encryption": "AES256"
                        }
                    }
                }
            ]
        }
        
        for statement in policy['Statement']:
            if self._matches_statement(statement, principal, 
                                     bucket, key, action):
                if statement['Effect'] == 'Deny':
                    return False, "Explicitly denied by bucket policy"
                elif statement['Effect'] == 'Allow':
                    return True, "Allowed by bucket policy"
                    
        return False, "No matching bucket policy"
```

## Monitoring and Observability

### S3 Metrics Collector

```python
class S3MetricsCollector:
    def __init__(self):
        self.metrics = {
            'requests': {},
            'errors': {},
            'latency': {},
            'bandwidth': {}
        }
        
    def record_request(self, operation: str, bucket: str, 
                      latency_ms: float, size_bytes: int, 
                      status_code: int):
        """Record S3 request metrics"""
        timestamp = datetime.now()
        
# Request counts
        key = f"{bucket}:{operation}"
        if key not in self.metrics['requests']:
            self.metrics['requests'][key] = 0
        self.metrics['requests'][key] += 1
        
# Error tracking
        if status_code >= 400:
            error_key = f"{bucket}:{operation}:{status_code}"
            if error_key not in self.metrics['errors']:
                self.metrics['errors'][error_key] = 0
            self.metrics['errors'][error_key] += 1
            
# Latency percentiles
        if operation not in self.metrics['latency']:
            self.metrics['latency'][operation] = []
        self.metrics['latency'][operation].append(latency_ms)
        
# Bandwidth tracking
        if operation in ['PUT', 'GET']:
            hour_key = timestamp.strftime('%Y-%m-%d:%H')
            if hour_key not in self.metrics['bandwidth']:
                self.metrics['bandwidth'][hour_key] = {
                    'ingress': 0,
                    'egress': 0
                }
                
            if operation == 'PUT':
                self.metrics['bandwidth'][hour_key]['ingress'] += size_bytes
            else:
                self.metrics['bandwidth'][hour_key]['egress'] += size_bytes
                
    def get_percentiles(self, operation: str) -> dict:
        """Calculate latency percentiles"""
        if operation not in self.metrics['latency']:
            return {}
            
        latencies = sorted(self.metrics['latency'][operation])
        n = len(latencies)
        
        return {
            'p50': latencies[int(n * 0.5)],
            'p90': latencies[int(n * 0.9)],
            'p99': latencies[int(n * 0.99)],
            'p999': latencies[int(n * 0.999)] if n > 1000 else latencies[-1]
        }
```

## Interview Deep Dive Topics

### 1. Consistency Model
- **Strong Read-After-Write Consistency**: How S3 achieved this in 2020
- **Metadata Consistency**: Distributed consensus using Paxos/Raft
- **Cross-Region Consistency**: Eventual consistency trade-offs

### 2. Scale Challenges
- **Request Rate Scaling**: Automatic partitioning based on request patterns
- **Namespace Scaling**: Distributed hash table for object metadata
- **Hot Partition Handling**: Dynamic repartitioning

### 3. Cost Engineering
- **Storage Tiering Economics**: TCO analysis for each tier
- **Data Transfer Costs**: Regional edge caches
- **Compute Optimization**: Erasure coding vs replication trade-offs

### 4. Operational Excellence
- **Deployment Strategy**: Progressive rollouts across AZs
- **Monitoring**: Real-time anomaly detection
- **Incident Response**: Automated remediation

### 5. Future Enhancements
- **ML-based Tiering**: Predictive access patterns
- **Edge Computing**: S3 on Outposts
- **Quantum-Safe Encryption**: Post-quantum cryptography

## Key Takeaways

1. **Durability First**: 11 9s achieved through erasure coding + multi-AZ replication
2. **Cost Optimization**: Intelligent tiering and lifecycle policies critical at scale
3. **Performance**: Request routing, connection pooling, and caching strategies
4. **Security**: Defense in depth with IAM, bucket policies, and encryption
5. **Operational Excellence**: Comprehensive monitoring and automated remediation

Remember: S3's success comes from relentless focus on durability, availability, and cost optimization while maintaining simplicity for users.