---
title: Work Distribution Examples
description: "Real-world examples and case studies demonstrating the concepts in practice"
type: pillar
difficulty: intermediate
reading_time: 30 min
prerequisites: []
status: complete
last_updated: 2025-07-28
---

# Work Distribution Examples

## âš¡ The One-Inch Punch

<div class="axiom-box">
<h3>ğŸ’¥ Work Truth</h3>
<p><strong>"More workers â‰  More speed. / Coordination costs / always win."</strong></p>
<p>That's why Google MapReduce changed everything: no coordination needed.</p>
</div>


## The Work Distribution Hall of Fame ğŸ†

<div class="axiom-box">
<strong>Real Systems That Got It Right (Eventually)</strong>

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  COMPANY     PROBLEM              SOLUTION         IMPACT      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Spotify     1 monolith,          Microservices    1000x       â•‘
â•‘              1800 engineers       + squads         deploys/day â•‘
â•‘                                                                â•‘
â•‘  Uber        15M rides/day        H3 spatial       <15s        â•‘
â•‘              10K cities           sharding         dispatch    â•‘
â•‘                                                                â•‘
â•‘  Discord     100M+ msgs/day       Elixir +         <100ms      â•‘
â•‘              15M concurrent       consistent hash  global      â•‘
â•‘                                                                â•‘
â•‘  Netflix     12hr encode times    MapReduce        20min       â•‘
â•‘              per movie            720 chunks       encode      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
</div>

### 1. Spotify: From Monolith to Music ğŸµ

<div class="decision-box">
<strong>The Conway's Law Masterclass</strong>

```
THE PROBLEM (2012)                    THE SOLUTION (2020)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚                    â”‚                â”‚Squadâ”‚ â”‚Squadâ”‚ â”‚Squadâ”‚
â”‚   ONE MASSIVE      â”‚                â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜
â”‚     BACKEND        â”‚       â”€â”€â”€â–º        â”‚       â”‚       â”‚
â”‚                    â”‚                â”Œâ”€â”€â”´â”€â”€â” â”Œâ”€â”€â”´â”€â”€â” â”Œâ”€â”€â”´â”€â”€â”
â”‚  100 engineers     â”‚                â”‚ Î¼S  â”‚ â”‚ Î¼S  â”‚ â”‚ Î¼S  â”‚
â”‚  1 deploy/week     â”‚                â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                
                                      1800 engineers
                                      1000+ deploys/day

KEY INSIGHTS:
â€¢ Organization structure = System architecture
â€¢ Autonomous squads = Independent services
â€¢ Event streaming (Kafka) = Loose coupling
â€¢ Service mesh = Dynamic discovery
```
</div>

<div class="truth-box">
<strong>Spotify's Architecture Evolution</strong>

```
     2012: THE MONOLITH                2020: THE CONSTELLATION
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     
     Deploy frequency:                 Deploy frequency:
     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 1/week              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1000+/day
     
     Lead time:                        Lead time:
     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Months              â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Hours
     
     Teams blocked:                    Teams blocked:
     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Always              â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Rarely
     
     Innovation:                       Innovation:
     â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Glacial             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Rapid
     
     THE SECRET SAUCE:
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ â€¢ Squad autonomy = Service ownership            â”‚
     â”‚ â€¢ Event streaming = Async by default            â”‚
     â”‚ â€¢ Service mesh = Discovery not configuration    â”‚
     â”‚ â€¢ Culture shift = "You build it, you run it"    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</div>

### 2. Uber: The H3 Hexagon Magic ğŸ—ºï¸

<div class="failure-vignette">
<strong>How to Handle 15M Rides Across 10,000 Cities</strong>

```
THE NAIVE APPROACH                    THE H3 GENIUS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•

Lat/Long boxes:                       Hexagonal grid:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                        â¬¡ â¬¡ â¬¡ â¬¡
â”‚   â”‚   â”‚   â”‚ â† Uneven              â¬¡ â¬¡ â¬¡ â¬¡ â¬¡
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤   density            â¬¡ â¬¡ â¬¡ â¬¡ â¬¡ â¬¡
â”‚â–ˆâ–ˆâ–ˆâ”‚   â”‚   â”‚                       â¬¡ â¬¡ â¬¡ â¬¡ â¬¡
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â† Hotspots            â¬¡ â¬¡ â¬¡ â¬¡
â”‚â–ˆâ–ˆâ–ˆâ”‚   â”‚   â”‚   kill you
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                        Equal area
                                     Natural neighbors
                                     16 zoom levels

WHY HEXAGONS?
â€¢ Equal distance to all neighbors
â€¢ No shared vertices (unlike squares)
â€¢ Approximates circles (optimal coverage)
â€¢ Used by bees for 100M years
```
</div>

<div class="axiom-box">
<strong>The H3 Work Distribution Algorithm</strong>

```
HOW IT WORKS:
â•â•â•â•â•â•â•â•â•â•â•â•â•

1. LOCATION â†’ H3 CELL                2. CELL â†’ SHARD
   
   lat: 37.7749                         Cell: 8928308280fffff
   lng: -122.4194                              â†“
        â†“                               hash(cell) % shards
   H3 Resolution 7                             â†“
        â†“                                  Shard 42
   Cell: 8928308280fffff

3. LOAD BALANCING                    4. WORK STEALING

   Shard 42: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (100%)       Check neighbors:
   Shard 43: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (20%)        Shard 41, 43, 44
   Shard 44: â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (30%)              â†“
                                        Steal edge cells
                                        from overloaded

PERFORMANCE:
â€¢ Dispatch: <15 seconds (P99)
â€¢ Success: 99.99% match rate
â€¢ Scale: 15M rides/day
â€¢ Coverage: 10,000+ cities
```
</div>


### 3. Discord: 100M Messages Without Breaking a Sweat ğŸ’¬

<div class="decision-box">
<strong>The BEAM VM Superpower</strong>

```
THE EVOLUTION OF SCALE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

2015: Python Monolith          2020: Elixir/Erlang Paradise
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                Millions of lightweight processes:
â”‚   PYTHON    â”‚                
â”‚  MONOLITH   â”‚ â†’ DEATH        Guild 1: Process 1
â”‚             â”‚   @ 10K        Guild 2: Process 2  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   users        ...      ...
                               Guild 1M: Process 1M

"Let's use Go!"                Each guild = isolated
"Let's use Node!"              Each process = 2KB memory
"Let's use... ERLANG?"         Crash one = others fine
       â†“
   GENIUS MOVE                 RESULT: 15M concurrent users
                                       100M+ msgs/day
                                       <100ms delivery
```
</div>

<div class="truth-box">
<strong>Discord's Guild Sharding Architecture</strong>

```
THE KEY INSIGHT: Guild = Natural Shard Boundary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Each Guild (Server) is:
â€¢ Completely independent
â€¢ Single process ownership  
â€¢ No cross-guild state
â€¢ Perfect for sharding

SHARDING STRATEGY:

     Guild ID: 12345678
          â†“
     hash(guild_id)
          â†“
   Consistent Hash Ring
          â†“
      Node: A7
          â†“
   Erlang Process #42

GENIUS MOVES:
â€¢ WebSocket per guild (not per user)
â€¢ Guild process supervises all channels
â€¢ Hot guilds get dedicated nodes
â€¢ Erlang OTP handles process crashes

SCALE ACHIEVED:
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ â€¢ 15M concurrent users             â•‘
â•‘ â€¢ 100M+ messages/day               â•‘
â•‘ â€¢ <100ms global latency            â•‘
â•‘ â€¢ 5 9's uptime                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
</div>


### 4. Google MapReduce: The Pattern That Changed Everything ğŸŒ

<div class="axiom-box">
<strong>2004: When Google Taught Us How to Think</strong>

```
THE REVELATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"What if we could process the ENTIRE web
 with just two simple functions?"

map(key, value) â†’ list(key2, value2)
reduce(key2, list(value2)) â†’ list(value3)

THAT'S IT. THAT'S THE WHOLE IDEA.

EXAMPLE: Count words in 1 billion documents
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MAP PHASE:                    REDUCE PHASE:
"hello world" â†’ (hello,1)     hello: [1,1,1] â†’ 3
                (world,1)     world: [1,1] â†’ 2
"hello again" â†’ (hello,1)     again: [1] â†’ 1
                (again,1)

THE GENIUS:
â€¢ Mappers don't talk to each other
â€¢ Reducers don't talk to each other
â€¢ Shuffle is the only coordination
â€¢ Failures? Just retry that chunk

IMPACT:
â€¢ Google: Indexed the web
â€¢ Yahoo: Built Hadoop
â€¢ Facebook: Process 500TB daily
â€¢ Everyone: "Oh, THAT'S how you do it!"
```
</div>

## Implementation Patterns That Actually Work ğŸ› ï¸

### 1. Work Stealing: The Self-Balancing Magic

<div class="decision-box">
<strong>How Work Stealing Really Works</strong>

```
THE PROBLEM:                         THE SOLUTION:
â•â•â•â•â•â•â•â•â•â•â•â•                         â•â•â•â•â•â•â•â•â•â•â•â•â•

Worker 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (busy)        Each worker has local deque:
Worker 2: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (light)      
Worker 3: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (idle)        Worker pushes/pops from BOTTOM
Worker 4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ (medium)      Thieves steal from TOP

UNBALANCED = SLOW                    WHY IT WORKS:
                                     â€¢ Cache locality (own work first)
                                     â€¢ Low contention (opposite ends)
                                     â€¢ Self-balancing (idle steals)

CODE PATTERN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class WorkStealingQueue:
    def push(self, task):
        with self.bottom_lock:
            self.deque.append(task)  # Push bottom
    
    def pop(self):  # Owner only
        with self.bottom_lock:
            return self.deque.pop()  # Pop bottom
    
    def steal(self):  # Thieves only
        with self.top_lock:
            return self.deque.popleft()  # Steal top

USED BY: Java ForkJoinPool, Cilk, TBB, Go scheduler
```
</div>

### 2. Consistent Hashing: Distributed Work Without Drama

<div class="truth-box">
<strong>The Algorithm That Powers Half the Internet</strong>

```
TRADITIONAL HASHING DISASTER:        CONSISTENT HASHING GENIUS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•         â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

3 nodes â†’ 4 nodes                    Add node D:
node = hash(key) % N                 
                                           A
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      0Â°â”‚ 
â”‚ Key  â”‚ Old â”‚ Newâ”‚                 270Â°â”€â”€â”¼â”€â”€90Â°
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   D â—â”‚   â— B
â”‚ foo  â”‚  A  â”‚  B â”‚ â† MOVED!            180Â°â”‚
â”‚ bar  â”‚  B  â”‚  C â”‚ â† MOVED!              C
â”‚ baz  â”‚  C  â”‚  D â”‚ â† MOVED!
â”‚ qux  â”‚  A  â”‚  A â”‚                 Only keys between C-D move!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 75% stay put! ğŸ˜Š
75% of data moves! ğŸ˜±

THE IMPLEMENTATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ConsistentHash:
    def __init__(self, nodes, virtual_nodes=150):
        self.ring = {}  # position -> node
        for node in nodes:
            for i in range(virtual_nodes):
                pos = hash(f"{node}:{i}")
                self.ring[pos] = node
        self.sorted_keys = sorted(self.ring.keys())
    
    def get_node(self, key):
        pos = hash(key)
        # Binary search for first node >= pos
        idx = bisect_right(self.sorted_keys, pos)
        if idx == len(self.sorted_keys):
            idx = 0  # Wrap around
        return self.ring[self.sorted_keys[idx]]

USED BY: Cassandra, DynamoDB, Memcached, Riak
```
</div>

### 3. Batch Processing: The 100x Performance Hack

<div class="axiom-box">
<strong>Why Single Items Are Performance Poison</strong>

```
NAIVE APPROACH:                      BATCH APPROACH:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                      â•â•â•â•â•â•â•â•â•â•â•â•â•â•

for item in million_items:           batch = []
    db.insert(item)                  for item in million_items:
    # 1M network calls                   batch.append(item)
    # 1M transactions                    if len(batch) >= 1000:
    # Death by latency                       db.insert_many(batch)
                                             batch = []

TIME: 1M Ã— 50ms = 14 hours          TIME: 1K Ã— 50ms = 50 seconds

THE MAGIC FORMULA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Optimal batch size = âˆš(setup_cost Ã— holding_cost)

For databases: ~1000 items
For network: ~10MB data
For APIs: ~100 requests

BACKPRESSURE PATTERN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class BatchProcessor:
    def __init__(self, batch_size=1000, max_pending=10000):
        self.semaphore = Semaphore(max_pending)
        self.batch = []
        self.timer = None
    
    async def submit(self, item):
        await self.semaphore.acquire()  # Backpressure!
        self.batch.append(item)
        
        if len(self.batch) >= self.batch_size:
            await self.flush()
        elif not self.timer:
            self.timer = asyncio.create_task(
                self._timeout_flush()
            )

GENIUS: Combines batching + backpressure + timeouts
```
</div>

### 4. Hierarchical Scheduling: How Google Runs Everything

<div class="decision-box">
<strong>The Borg Pattern: 2-Level Scheduling</strong>

```
THE GENIUS: Separate WHAT from WHERE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LEVEL 1: GLOBAL SCHEDULER            LEVEL 2: LOCAL SCHEDULERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"I need 4 CPUs, 8GB RAM"            "I have machine space here"
         â†“                                    â†“
Finds suitable cluster               Runs bin-packing algorithm
         â†“                                    â†“
Hands off to local                   Places on specific machine

WHY IT SCALES:
â€¢ Global scheduler isn't bottleneck
â€¢ Local schedulers know their resources
â€¢ Clusters can use different policies
â€¢ Failures are isolated

SCORING ALGORITHM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
score = w1 Ã— locality_score +
        w2 Ã— (1 / utilization) +
        w3 Ã— (1 / queue_length)

BIN PACKING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for machine in sorted_by_fragmentation:
    if machine.fits(job):
        return machine
return None  # Need new machine

USED BY: Google Borg, Kubernetes, Mesos, YARN
```
</div>

## Anti-Patterns: How NOT to Distribute Work ğŸš«

### 1. The Distributed Monolith: Worst of Both Worlds

<div class="failure-vignette">
<strong>When Microservices Go Wrong</strong>

```
THE ANTI-PATTERN:                    THE RIGHT WAY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Order   â”‚  â”‚Inventoryâ”‚            â”‚ Order   â”‚  â”‚Inventoryâ”‚
â”‚ Service â”‚  â”‚ Service â”‚            â”‚ Service â”‚  â”‚ Service â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚            â”‚                       â”‚            â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚            â”‚
          â†“                               â†“            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  SHARED   â”‚                   â”‚Orders DBâ”‚  â”‚ Inv DB  â”‚
    â”‚ DATABASE  â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    PROBLEMS:                       â”‚ Event Bus   â”‚
    â€¢ Can't deploy separately       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â€¢ Schema changes = chaos
    â€¢ Performance coupling          BENEFITS:
    â€¢ "Distributed" monolith        â€¢ Independent deployment
                                   â€¢ Schema freedom
                                   â€¢ Performance isolation
                                   â€¢ True microservices
```
</div>

### 2. The N+1 Query Disaster: Death by 1000 Cuts

<div class="axiom-box">
<strong>When Services Become Chatty Teenagers</strong>

```
THE HORROR STORY:                    THE FIX:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•

for post in get_posts():             posts = get_posts()
    author = get_author(post.id)     author_ids = [p.author_id for p in posts]
    likes = get_likes(post.id)       authors = get_authors_batch(author_ids)
    # 200 API calls for 100 posts    likes = get_likes_batch(post_ids)
    # Latency: 200 Ã— 50ms = 10s      # 3 API calls total
                                     # Latency: 3 Ã— 50ms = 150ms

THE PATTERN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WRONG: O(N) calls
for item in items:
    enrich(item)  # Network call!

# RIGHT: O(1) calls  
ids = [item.id for item in items]
enriched = batch_enrich(ids)  # One call!

REAL WORLD IMPACT:
â€¢ Facebook: GraphQL invented to solve this
â€¢ Netflix: Falcor created for same reason
â€¢ Twitter: Batch APIs everywhere
```
</div>

### 3. The Thundering Herd: When Everything Wakes at Once

<div class="truth-box">
<strong>The 12:00 AM Disaster</strong>

```
THE PROBLEM:                         THE SOLUTION:
â•â•â•â•â•â•â•â•â•â•â•â•                         â•â•â•â•â•â•â•â•â•â•â•â•â•

00:00:00.000 â†’                       Jittered start times:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           
â”‚ 10,000 SERVERS WAKE UP â”‚           for i, server in enumerate(servers):
â”‚      SIMULTANEOUSLY    â”‚               delay = random(0, 60) + (i * 0.1)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               schedule_at(00:00:00 + delay)
            â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                Result:
     â”‚ DATABASE DIE â”‚                00:00:00.000 â†’ Server 1
     â”‚ NETWORK DIE  â”‚                00:00:03.142 â†’ Server 2
     â”‚ EVERYTHING   â”‚                00:00:07.891 â†’ Server 3
     â”‚     DIE      â”‚                ... spreads over 60 seconds
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OTHER THUNDERING HERD SCENARIOS:
â€¢ Cache expires â†’ Everyone fetches
â€¢ Service restarts â†’ All reconnect
â€¢ Queue empty â†’ All workers wake

FIXES:
â€¢ Exponential backoff with jitter
â€¢ Request coalescing 
â€¢ Circuit breakers
â€¢ Gradual rollouts
```
</div>

## Performance Shootout: The Numbers Don't Lie ğŸ“Š

<div class="decision-box">
<strong>Sync vs Async vs Parallel: The Ultimate Showdown</strong>

```
TASK: Fetch 100 URLs
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SYNCHRONOUS (The Sloth):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for url in urls:
    fetch(url)  # 500ms each
    
Time: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 50 seconds
CPU:  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 10%
Code: Simple âœ“

THREADED (The Heavyweight):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with ThreadPool(10) as pool:
    pool.map(fetch, urls)
    
Time: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 5 seconds  
CPU:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 30%
RAM:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 40MB

ASYNC (The Ninja):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main():
    await asyncio.gather(*[fetch(url) for url in urls])
    
Time: â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.5 seconds
CPU:  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 10%  
RAM:  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 5MB

THE LESSON: I/O bound = Async wins
           CPU bound = Threads/Processes win
```
</div>


## The Work Distribution Wisdom Wall ğŸ§ 

<div class="axiom-box">
<strong>Hard-Won Lessons From the Trenches</strong>

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    THE 10 COMMANDMENTS                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                               â•‘
â•‘  1. "More workers â‰  More speed" - Amdahl's Law              â•‘
â•‘  2. "Batch or die" - Single items are poison                â•‘
â•‘  3. "Steal, don't assign" - Dynamic > Static                â•‘
â•‘  4. "Backpressure or bust" - Queues aren't infinite         â•‘
â•‘  5. "Jitter everything" - Thundering herds kill             â•‘
â•‘  6. "Hash consistently" - Or move all your data             â•‘
â•‘  7. "Events > RPC" - Loose coupling wins                    â•‘
â•‘  8. "Async for I/O" - Threads for CPU                       â•‘
â•‘  9. "Monitor or meltdown" - You can't fix what you can't seeâ•‘
â•‘ 10. "Test at scale" - Your laptop lies                      â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

THE BOTTOM LINE:
Work distribution is not about technology.
It's about physics, math, and human psychology.
Respect all three or pay the price.
```
</div>

---

<div class="page-nav" markdown>
[:material-arrow-left: Pillar 1: Work](part2-pillars/work/index) | 
[:material-arrow-up: The 5 Pillars](part2-pillars) | 
[:material-arrow-right: Work Exercises](part2-pillars/work/exercises/)
</div>