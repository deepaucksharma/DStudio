# Episode 36: Primary-Backup Replication - Master-Slave ‡§ï‡•Ä ‡§ï‡§π‡§æ‡§®‡•Ä

## Introduction: Mumbai ‡§ï‡•á Main Post Office ‡§ï‡•Ä ‡§ï‡§π‡§æ‡§®‡•Ä

‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç, ‡§Ü‡§ú ‡§π‡§Æ ‡§¨‡§æ‡§§ ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á Primary-Backup Replication ‡§ï‡•Ä - ‡§Ø‡§æ‡§®‡•Ä Master-Slave architecture ‡§ï‡•Ä‡•§ ‡§Ø‡•á concept ‡§∏‡§Æ‡§ù‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ö‡§≤‡§ø‡§è Mumbai ‡§ï‡•á GPO (General Post Office) ‡§ï‡•Ä ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§

Mumbai GPO, ‡§ú‡•ã Victoria Terminus ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§π‡•à, ‡§µ‡•ã ‡§π‡•à ‡§π‡§Æ‡§æ‡§∞‡§æ Primary node‡•§ ‡§î‡§∞ ‡§™‡•Ç‡§∞‡•á Mumbai ‡§Æ‡•á‡§Ç ‡§ú‡•ã sub-post offices ‡§π‡•à‡§Ç - Andheri, Borivali, Dadar - ‡§µ‡•ã ‡§∏‡§¨ ‡§π‡•à‡§Ç ‡§π‡§Æ‡§æ‡§∞‡•á Backup nodes‡•§ ‡§ú‡•à‡§∏‡•á GPO ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à important registry ‡§Ü‡§§‡•Ä ‡§π‡•à, ‡§§‡•ã ‡§â‡§∏‡§ï‡•Ä copy ‡§∏‡§≠‡•Ä sub-offices ‡§Æ‡•á‡§Ç ‡§≠‡•á‡§ú‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§Ö‡§ó‡§∞ ‡§ï‡§≠‡•Ä GPO ‡§Æ‡•á‡§Ç ‡§Ü‡§ó ‡§≤‡§ó ‡§ú‡§æ‡§è ‡§Ø‡§æ flood ‡§Ü ‡§ú‡§æ‡§è, ‡§§‡•ã ‡§≠‡•Ä ‡§Ü‡§™‡§ï‡§æ data safe ‡§∞‡§π‡§§‡§æ ‡§π‡•à sub-offices ‡§Æ‡•á‡§Ç‡•§

## Part 1: Theory ‡§ï‡•Ä ‡§ó‡§π‡§∞‡§æ‡§à - Primary-Backup ‡§ï‡§æ Mathematics (45 minutes)

### The Fundamental Model

Primary-Backup replication ‡§ï‡§æ basic principle ‡§π‡•à:
- ‡§è‡§ï Primary (Master) node ‡§ú‡•ã ‡§∏‡§≠‡•Ä writes handle ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- Multiple Backup (Slave) nodes ‡§ú‡•ã reads handle ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç
- Replication lag - Primary ‡§∏‡•á Backup ‡§§‡§ï data ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•á ‡§Æ‡•á‡§Ç ‡§≤‡§ó‡§®‡•á ‡§µ‡§æ‡§≤‡§æ time

‡§ú‡•à‡§∏‡•á Mumbai local trains ‡§Æ‡•á‡§Ç ‡§π‡•à Motorman (Primary) ‡§î‡§∞ Guard (Backup)‡•§ Motorman train ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à, ‡§∏‡§≠‡•Ä decisions ‡§≤‡•á‡§§‡§æ ‡§π‡•à‡•§ Guard ‡§∏‡§ø‡§∞‡•ç‡§´ observe ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ emergency ‡§Æ‡•á‡§Ç take over ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§

### Synchronous vs Asynchronous Replication

**Synchronous Replication** - ‡§ú‡•à‡§∏‡•á Dabbawala system:
- Dabba pickup ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§§‡§¨ ‡§§‡§ï wait ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ ‡§§‡§ï next person confirm ‡§® ‡§ï‡§∞‡•á
- Data loss ‡§ï‡•Ä guarantee zero
- Performance slow ‡§π‡•ã ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à
- Latency = Primary Write Time + Network Time + Backup Write Time + Acknowledgment Time

**Asynchronous Replication** - ‡§ú‡•à‡§∏‡•á WhatsApp message delivery:
- Message ‡§≠‡•á‡§ú‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ immediately "sent" tick ‡§Ü‡§§‡§æ ‡§π‡•à
- "Delivered" ‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç ‡§Ü‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ recipient ‡§ï‡•ã ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§π‡•à  
- Fast performance ‡§≤‡•á‡§ï‡§ø‡§® data loss ‡§ï‡§æ risk
- Latency = Primary Write Time only

### Mathematical Analysis

**Write Availability**:
- Synchronous: ‡§∏‡§≠‡•Ä nodes available ‡§π‡•ã‡§®‡•á ‡§ö‡§æ‡§π‡§ø‡§è
- Availability = P(Primary Up) √ó P(All Backups Up)
- ‡§Ö‡§ó‡§∞ 3 nodes ‡§π‡•à‡§Ç with 99% uptime each: 0.99 √ó 0.99 √ó 0.99 = 97%

**Read Scalability**:
- N backup nodes = N+1 times read capacity
- Linear scaling possible
- Load distribution: Each node handles 1/(N+1) reads

**Failover Time**:
- Detection Time (D) + Election Time (E) + Recovery Time (R)
- Typical values: D=10s, E=5s, R=30s = 45 seconds total downtime

### Consistency Guarantees

Primary-Backup different consistency levels provide ‡§ï‡§∞‡§§‡§æ ‡§π‡•à:

**Strong Consistency** - Synchronous replication:
- ‡§∏‡§≠‡•Ä reads latest data ‡§¶‡•á‡§ñ‡§§‡•á ‡§π‡•à‡§Ç
- Performance penalty high

**Eventual Consistency** - Asynchronous replication:
- Reads might see old data
- Replication lag determines staleness

**Read-Your-Writes Consistency**:
- Client ‡§Ö‡§™‡§®‡•á writes ‡§π‡§Æ‡•á‡§∂‡§æ ‡§¶‡•á‡§ñ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à
- Session stickiness ‡§Ø‡§æ version tracking ‡§∏‡•á achieve ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç

## Part 2: Production ‡§ï‡•Ä ‡§ï‡§π‡§æ‡§®‡§ø‡§Ø‡§æ‡§Ç - Real Implementations (60 minutes)

### MySQL Replication - The Classic Implementation

MySQL ‡§ï‡§æ replication system ‡§¨‡§π‡•Å‡§§ mature ‡§π‡•à, ‡§ú‡•à‡§∏‡•á Mumbai ‡§ï‡§æ railway system:

**Binary Log Based Replication**:
- Primary ‡§∏‡§≠‡•Ä changes binary log ‡§Æ‡•á‡§Ç record ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- Slaves binary log read ‡§ï‡§∞‡§ï‡•á replay ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç
- Position tracking ‡§∏‡•á consistency maintain ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç

**Production Setup at Flipkart**:
```
Primary (Delhi DC)
    |
    +-- Slave 1 (Mumbai DC) - Asynchronous
    +-- Slave 2 (Bangalore DC) - Asynchronous  
    +-- Slave 3 (Chennai DC) - Semi-synchronous
```

Flipkart ‡§ï‡•Ä Black Friday sale ‡§Æ‡•á‡§Ç:
- 10 million orders/day
- Primary handles all writes
- Slaves handle 90% reads
- Failover time: < 30 seconds

**Challenges faced**:
- Replication lag during peak: up to 5 seconds
- Binary log size: 500GB/day
- Network bandwidth: 10Gbps dedicated for replication

### PostgreSQL Replication Evolution

PostgreSQL ‡§®‡•á replication ‡§ï‡•ã evolve ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à over the years:

**Streaming Replication**:
- WAL (Write-Ahead Log) streaming
- Byte-by-byte replication
- Near-zero lag possible

**Logical Replication** (PostgreSQL 10+):
- Table-level granularity
- Cross-version replication
- Selective replication

**Production at Zomato**:
```
Primary (Master) - Mumbai
    |
    +-- Hot Standby 1 - Delhi (Synchronous)
    +-- Hot Standby 2 - Bangalore (Asynchronous)
    +-- Read Replicas (5) - Various cities (Asynchronous)
```

Zomato ‡§ï‡•á numbers:
- 100 million users
- 1.5 million restaurants
- Peak orders: 50,000/minute
- Read/Write ratio: 100:1

**Optimization techniques**:
- Connection pooling with PgBouncer
- Query routing based on staleness tolerance
- Automatic failover using Patroni

### Redis Replication - Speed ‡§ï‡•Ä ‡§Æ‡§ø‡§∏‡§æ‡§≤

Redis ‡§ï‡§æ replication lightning fast ‡§π‡•à, ‡§ú‡•à‡§∏‡•á Mumbai ‡§ï‡•Ä ‡§§‡•á‡§ú‡§º local trains:

**Redis Replication Architecture**:
- Memory-to-memory replication
- Asynchronous by default
- Partial resynchronization support

**Production at Ola Cabs**:
```
Redis Master (Primary Region)
    |
    +-- Slave 1 (Same DC) - Backup
    +-- Slave 2 (Different DC) - DR
    +-- Slave 3 (Different DC) - Read scaling
```

Ola ‡§ï‡•á real-time tracking system ‡§Æ‡•á‡§Ç:
- 1 million drivers active
- Location updates: 100,000/second
- Cache hit ratio: 95%
- Replication lag: < 100ms

**Sentinel for High Availability**:
- Automatic failover
- Master election
- Configuration updates
- Split-brain prevention

### MongoDB Replica Sets

MongoDB ‡§ï‡§æ replica set architecture sophisticated ‡§π‡•à:

**Replica Set Components**:
- Primary: Handles all writes
- Secondary: Replicate from primary
- Arbiter: Voting only, no data

**Production at Paytm**:
```
Replica Set (3 nodes):
- Primary: Delhi (Priority 100)
- Secondary 1: Mumbai (Priority 50)
- Secondary 2: Bangalore (Priority 30)
```

Paytm ‡§ï‡•á transaction system ‡§Æ‡•á‡§Ç:
- 500 million registered users
- 20 million transactions/day
- Write concern: w=majority
- Read preference: primaryPreferred

**Interesting optimizations**:
- Hidden members for backup
- Delayed members for recovery
- Priority 0 members for analytics

### Cloud Provider Solutions

**AWS RDS Multi-AZ**:
- Automatic failover
- Synchronous replication
- Zero data loss
- 60-120 seconds failover time

**Azure SQL Database**:
- Active geo-replication
- Up to 4 readable secondaries
- Automatic backups
- Point-in-time restore

**Google Cloud SQL**:
- High Availability configuration
- Regional replicas
- Cross-region replicas
- Automatic failover

## Part 3: Implementation Details - ‡§ï‡•à‡§∏‡•á ‡§¨‡§®‡§æ‡§è‡§Ç ‡§Ö‡§™‡§®‡§æ System (30 minutes)

### Building Your Own Primary-Backup System

‡§Ö‡§ó‡§∞ ‡§Ü‡§™ ‡§Ö‡§™‡§®‡§æ primary-backup system ‡§¨‡§®‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§Ø‡§π‡§æ‡§Å ‡§π‡•à‡§Ç key components:

**1. Change Detection**:
- Trigger-based
- Log-based (CDC - Change Data Capture)
- Query-based (polling)

**2. Data Transfer**:
- Batch transfer
- Streaming transfer
- Compressed transfer

**3. Conflict Resolution**:
- Last-write-wins
- Version vectors
- Application-specific logic

### Monitoring ‡§î‡§∞ Operations

**Key Metrics to Monitor**:
```
1. Replication Lag:
   - Current lag in seconds
   - Lag trend over time
   - Alert if lag > threshold

2. Throughput:
   - Writes/second on primary
   - Replication bytes/second
   - Network utilization

3. Errors:
   - Connection failures
   - Conflict count
   - Failed transactions
```

**Production Alerting at MakeMyTrip**:
- Lag > 1 second: Warning
- Lag > 5 seconds: Critical
- Connection lost: Page on-call
- Disk usage > 80%: Auto-scale

### Failure Scenarios ‡§î‡§∞ Recovery

**Common Failure Modes**:

1. **Primary Failure**:
   - Detection via heartbeat
   - Slave promotion
   - Client redirection
   - Old primary fencing

2. **Network Partition**:
   - Split-brain prevention
   - Quorum-based decisions
   - Witness nodes

3. **Cascading Failures**:
   - Circuit breakers
   - Backpressure
   - Graceful degradation

**Recovery Procedures**:
```
1. Detect failure (10-30 seconds)
2. Verify failure (5-10 seconds)
3. Select new primary (5-10 seconds)
4. Promote slave (10-30 seconds)
5. Redirect clients (5-10 seconds)
Total: 35-90 seconds typically
```

### Performance Optimization

**Techniques for Better Performance**:

1. **Parallel Replication**:
   - Multiple replication threads
   - Partition-based parallelism
   - Schema-based parallelism

2. **Compression**:
   - Network compression
   - Binary log compression
   - Storage compression

3. **Batching**:
   - Group commit
   - Batch acknowledgments
   - Pipeline operations

**Real Numbers from BookMyShow**:
- Without optimization: 5 second lag
- With parallel replication: 1 second lag
- With compression: 60% bandwidth saved
- With batching: 3x throughput increase

## Part 4: Advanced Topics ‡§î‡§∞ Future (30 minutes)

### Multi-Master Replication

Next evolution ‡§π‡•à Multi-Master, ‡§ú‡§π‡§æ‡§Å multiple primaries ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç:

**Advantages**:
- No single point of failure
- Geographic distribution
- Write scaling

**Challenges**:
- Conflict resolution
- Complex failure scenarios
- Higher complexity

**Production Example - Uber**:
- Multi-region active-active
- Conflict-free data types
- Eventually consistent
- Regional isolation

### Cross-Region Replication

Global companies ‡§ï‡•á ‡§≤‡§ø‡§è cross-region replication critical ‡§π‡•à:

**Netflix's Approach**:
- Primary in US-East
- Replicas in 20+ regions
- < 1 second replication lag
- Regional failover capability

**Challenges**:
- Network latency (100-300ms)
- Bandwidth costs ($$$)
- Regulatory compliance
- Data sovereignty

### Disaster Recovery Strategies

**RPO ‡§î‡§∞ RTO**:
- RPO (Recovery Point Objective): ‡§ï‡§ø‡§§‡§®‡§æ data loss acceptable ‡§π‡•à
- RTO (Recovery Time Objective): ‡§ï‡§ø‡§§‡§®‡•Ä ‡§¶‡•á‡§∞ ‡§Æ‡•á‡§Ç recover ‡§ï‡§∞‡§®‡§æ ‡§π‡•à

**Different Strategies**:
1. **Hot Standby**: Always ready (RTO: minutes)
2. **Warm Standby**: Partially ready (RTO: hours)
3. **Cold Standby**: Backup only (RTO: days)

**Cost vs RTO/RPO Trade-off**:
- Hot Standby: 2x cost, minimal RTO/RPO
- Warm Standby: 1.5x cost, moderate RTO/RPO
- Cold Standby: 1.1x cost, high RTO/RPO

### Machine Learning ‡§î‡§∞ Automation

**Predictive Failover**:
- ML models predict failures
- Proactive failover
- Reduced downtime

**Auto-tuning**:
- Dynamic buffer sizing
- Adaptive batch sizes
- Smart routing

**Example from Flipkart**:
- ML model predicts 85% failures
- 50% reduction in downtime
- 30% performance improvement

### Future Trends

**1. Serverless Replication**:
- No infrastructure management
- Automatic scaling
- Pay per use

**2. Edge Replication**:
- CDN integration
- IoT scenarios
- 5G networks

**3. Quantum Safe Replication**:
- Post-quantum cryptography
- Quantum key distribution
- Future-proof security

## Part 5: Practical Guidelines ‡§î‡§∞ Best Practices (15 minutes)

### When to Use Primary-Backup

**Perfect for**:
- Read-heavy workloads (90% reads)
- Simple consistency requirements
- Existing applications
- Limited budget

**Not suitable for**:
- Write-heavy workloads
- Multi-region writes
- Zero downtime requirements
- Complex consistency needs

### Design Decisions

**Key Questions to Ask**:
1. Synchronous ‡§Ø‡§æ Asynchronous?
2. How many replicas?
3. Geographic distribution?
4. Failover automation level?
5. Consistency requirements?

### Common Mistakes ‡§î‡§∞ Solutions

**Mistake 1**: Not monitoring replication lag
**Solution**: Set up comprehensive monitoring

**Mistake 2**: No failover testing
**Solution**: Regular disaster recovery drills

**Mistake 3**: Ignoring network partitions
**Solution**: Implement proper fencing

**Mistake 4**: Over-engineering
**Solution**: Start simple, evolve gradually

### Migration Strategy

**From Single Node to Primary-Backup**:
1. Set up replica with existing data
2. Start replication
3. Verify data consistency
4. Test failover process
5. Update application configuration
6. Monitor closely

### Cost Optimization

**Tips for Reducing Costs**:
- Use spot instances for read replicas
- Compress replication traffic
- Optimize network routes
- Right-size instances
- Use reserved instances

**Example Savings at Swiggy**:
- Spot instances: 70% cost reduction
- Compression: 40% bandwidth saving
- Right-sizing: 30% compute saving
- Total: 50% cost optimization

## Conclusion: Mumbai ‡§ï‡•Ä ‡§§‡§∞‡§π Resilient

Primary-Backup replication ‡§è‡§ï proven pattern ‡§π‡•à ‡§ú‡•ã Mumbai ‡§ï‡•Ä ‡§§‡§∞‡§π resilient ‡§π‡•à‡•§ ‡§ú‡•à‡§∏‡•á Mumbai local trains ‡§∞‡•Å‡§ï‡§§‡•Ä ‡§®‡§π‡•Ä‡§Ç, ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§Ü‡§™‡§ï‡§æ system ‡§≠‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§∞‡•Å‡§ï‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§

**Key Takeaways**:
1. Start with simple primary-backup
2. Monitor everything
3. Test failures regularly
4. Optimize based on workload
5. Evolve as you grow

**Remember**: Perfect replication strategy doesn't exist‡•§ ‡§Ü‡§™‡§ï‡•á use case ‡§ï‡•á ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§∏‡•á choose ‡§ï‡§∞‡•á‡§Ç‡•§

‡§ú‡•à‡§∏‡•á Mumbai ‡§Æ‡•á‡§Ç ‡§π‡§∞ area ‡§ï‡§æ ‡§Ö‡§™‡§®‡§æ character ‡§π‡•à - Bandra ‡§ï‡§æ cool vibe, Dadar ‡§ï‡§æ busy market, Marine Drive ‡§ï‡§æ peace - ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§π‡§∞ replication strategy ‡§ï‡•Ä ‡§Ö‡§™‡§®‡•Ä strength ‡§π‡•à‡•§ Choose wisely!

**Action Items**:
1. Evaluate your current setup
2. Identify replication requirements
3. Start with pilot project
4. Measure and optimize
5. Scale gradually

‡§Ö‡§ó‡§≤‡•Ä episode ‡§Æ‡•á‡§Ç ‡§π‡§Æ ‡§¨‡§æ‡§§ ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á Chain Replication ‡§ï‡•Ä - ‡§è‡§ï ‡§î‡§∞ interesting pattern ‡§ú‡•ã ‡§¨‡§π‡•Å‡§§ powerful ‡§π‡•à specific use cases ‡§ï‡•á ‡§≤‡§ø‡§è‡•§

‡§§‡§¨ ‡§§‡§ï ‡§ï‡•á ‡§≤‡§ø‡§è, Happy Replicating! üöÇ

---

*‡§Ø‡•á episode Distributed Systems Engineering Podcast ‡§ï‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ó‡§∞ ‡§Ü‡§™‡§ï‡•ã ‡§™‡§∏‡§Ç‡§¶ ‡§Ü‡§Ø‡§æ ‡§§‡•ã share ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ subscribe ‡§ï‡§∞‡•á‡§Ç‡•§*